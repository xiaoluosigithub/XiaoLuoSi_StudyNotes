#### ![](https://i-blog.csdnimg.cn/blog_migrate/9641664c516c3d9de3a26ade4d4ceae0.gif) 

![](https://i-blog.csdnimg.cn/blog_migrate/f0c3a8db06862e212db222bde7a6ed54.jpeg)

## å‰è¨€ 

ä¸Šä¸€ç¯‡æˆ‘ä»¬ä¸€èµ·å­¦ä¹ äº†YOLOv5çš„ç½‘ç»œæ¨¡å‹ä¹‹ä¸€yolo.pyï¼Œå®ƒè¿™æ˜¯YOLOçš„ç‰¹å®šæ¨¡å—ï¼Œè€Œä»Šå¤©è¦å­¦ä¹ å¦ä¸€ä¸ªå’Œç½‘ç»œæ­å»ºæœ‰å…³çš„æ–‡ä»¶â€”â€”common.pyï¼Œè¿™ä¸ªæ–‡ä»¶å­˜æ”¾ç€YOLOv5ç½‘ç»œæ­å»ºå¸¸è§çš„é€šç”¨æ¨¡å—ã€‚å¦‚æœæˆ‘ä»¬éœ€è¦ä¿®æ”¹æŸä¸€æ¨¡å—ï¼Œé‚£ä¹ˆå°±éœ€è¦ä¿®æ”¹è¿™ä¸ªæ–‡ä»¶ä¸­å¯¹åº”æ¨¡å—çš„å®šä¹‰ã€‚

å­¦è¿™ç¯‡çš„åŒæ—¶ï¼Œæ­é…[ã€YOLOç³»åˆ—ã€‘YOLOv5è¶…è¯¦ç»†è§£è¯»ï¼ˆç½‘ç»œè¯¦è§£ï¼‰][YOLO_YOLOv5]è¿™ç¯‡ç®—æ³•è¯¦è§£æ•ˆæœæ›´å¥½å™¢~

common.pyæ–‡ä»¶ä½ç½®åœ¨./models/common.py

![](https://i-blog.csdnimg.cn/blog_migrate/35f034771047fad9cc13f708216fed30.png)

æ–‡ç« ä»£ç é€è¡Œæ‰‹æ‰“æ³¨é‡Šï¼Œæ¯ä¸ªæ¨¡å—éƒ½æœ‰å¯¹åº”è®²è§£ï¼Œä¸€æ–‡å¸®ä½ æ¢³ç†æ•´ä¸ªä»£ç é€»è¾‘ï¼

å‹æƒ…æç¤ºï¼šå…¨æ–‡5ä¸‡å¤šå­—ï¼Œå¯ä»¥å…ˆç‚¹![](https://i-blog.csdnimg.cn/blog_migrate/ea5f7225888a49f6a6827b9ae71e856f.gif)å†æ…¢æ…¢çœ‹å“¦~

æºç ä¸‹è½½åœ°å€ï¼š[mirrors / ultralytics / yolov5 Â· GitCode][mirrors _ ultralytics _ yolov5 _ GitCode]

![](https://i-blog.csdnimg.cn/blog_migrate/4cb78898ea57a142a8ca74a0be342898.gif)

![](https://i-blog.csdnimg.cn/blog_migrate/ac3c5d6bfbcbf982e8e9e3632d7f20d1.gif) ğŸ€æœ¬äºº[YOLOv5æºç ][YOLOv5]è¯¦è§£ç³»åˆ—ï¼š

[YOLOv5æºç é€è¡Œè¶…è¯¦ç»†æ³¨é‡Šä¸è§£è¯»ï¼ˆ1ï¼‰â€”â€”é¡¹ç›®ç›®å½•ç»“æ„è§£æ][YOLOv5_1]

[YOLOv5æºç é€è¡Œè¶…è¯¦ç»†æ³¨é‡Šä¸è§£è¯»ï¼ˆ2ï¼‰â€”â€”æ¨ç†éƒ¨åˆ†detect.py][YOLOv5_2_detect.py]

[YOLOv5æºç é€è¡Œè¶…è¯¦ç»†æ³¨é‡Šä¸è§£è¯»ï¼ˆ3ï¼‰â€”â€”è®­ç»ƒéƒ¨åˆ†train.py][YOLOv5_3_train.py]

[YOLOv5æºç é€è¡Œè¶…è¯¦ç»†æ³¨é‡Šä¸è§£è¯»ï¼ˆ4ï¼‰â€”â€”éªŒè¯éƒ¨åˆ†valï¼ˆtestï¼‰.py][YOLOv5_4_val_test_.py]

[YOLOv5æºç é€è¡Œè¶…è¯¦ç»†æ³¨é‡Šä¸è§£è¯»ï¼ˆ5ï¼‰â€”â€”é…ç½®æ–‡ä»¶yolov5s.yaml][YOLOv5_5_yolov5s.yaml]

[YOLOv5æºç é€è¡Œè¶…è¯¦ç»†æ³¨é‡Šä¸è§£è¯»ï¼ˆ6ï¼‰â€”â€”ç½‘ç»œç»“æ„ï¼ˆ1ï¼‰yolo.py][YOLOv5_6_1_yolo.py]

## ç›®å½• 

[å‰è¨€ ][Link 1]

[ç›®å½•][Link 2]

[ğŸš€ä¸€ã€ å¯¼åŒ…å’ŒåŸºæœ¬é…ç½® ][Link 3]

[1.1 å¯¼å…¥å®‰è£…å¥½çš„pythonåº“][1.1 _python]

[1.2 åŠ è½½è‡ªå®šä¹‰æ¨¡å—][1.2]

[ğŸš€äºŒã€ åŸºç¡€ç»„ä»¶][Link 4]

[2.1 autopad][]

[2.2 Conv][]

[2.3 DWConv][]

[2.4 Bottleneck][]

[2.5 BottleneckCSP][]

[2.6 C3][]

[2.6.1 C3][]

[2.6.2 C3SPP(C3)][2.6.2 C3SPP_C3]

[2.6.3 C3Ghost(C3)][2.6.3 C3Ghost_C3]

[2.7 SPP][]

[2.7.1 SPP][]

[2.7.2 SPPF][]

[2.8 Focus][]

[2.9 Contract][]

[2.10 Expand][]

[2.11 Concat][]

[ğŸš€ä¸‰ã€æ³¨æ„åŠ›æ¨¡å— ][Link 5]

[3.1 TransformerLayer][]

[3.2 TransformerBlock][]

[ğŸš€å››ã€å¹»è±¡æ¨¡å—][Link 6]

[4.1 GhostConv][]

[4.2 GhostBottleneck][]

[ğŸš€äº”ã€æ¨¡å‹æ‰©å±•æ¨¡å—][Link 7]

[5.1 C3TR(C3)][5.1 C3TR_C3]

[5.2 AutoShape][]

[5.3 Detections][]

[5.4 Classify][]

[ ğŸš€å…­ã€common.pyå…¨éƒ¨æ³¨é‡Š][_common.py]

![](https://i-blog.csdnimg.cn/blog_migrate/d35d9edbb9675b824e2c1dabd7a35e91.gif)

## ğŸš€ä¸€ã€ å¯¼åŒ…å’ŒåŸºæœ¬é…ç½® 

### 1.1 å¯¼å…¥å®‰è£…å¥½çš„pythonåº“ 

```java
'''======================1.å¯¼å…¥å®‰è£…å¥½çš„pythonåº“====================='''
import json  # ç”¨äºjsonå’ŒPythonæ•°æ®ä¹‹é—´çš„ç›¸äº’è½¬æ¢
import math  # æ•°å­¦å‡½æ•°æ¨¡å—
import platform  # è·å–æ“ä½œç³»ç»Ÿçš„ä¿¡æ¯
import warnings  # è­¦å‘Šç¨‹åºå‘˜å…³äºè¯­è¨€æˆ–åº“åŠŸèƒ½çš„å˜åŒ–çš„æ–¹æ³•
from copy import copy  # æ•°æ®æ‹·è´æ¨¡å— åˆ†æµ…æ‹·è´å’Œæ·±æ‹·è´
from pathlib import Path  # Pathå°†strè½¬æ¢ä¸ºPathå¯¹è±¡ ä½¿å­—ç¬¦ä¸²è·¯å¾„æ˜“äºæ“ä½œçš„æ¨¡å—

import cv2  # è°ƒç”¨OpenCVçš„cvåº“
import numpy as np  # numpyæ•°ç»„æ“ä½œæ¨¡å—
import pandas as pd  # pandaæ•°ç»„æ“ä½œæ¨¡å—
import requests  # Pythonçš„HTTPå®¢æˆ·ç«¯åº“
import torch  # pytorchæ·±åº¦å­¦ä¹ æ¡†æ¶
import torch.nn as nn  # ä¸“é—¨ä¸ºç¥ç»ç½‘ç»œè®¾è®¡çš„æ¨¡å—åŒ–æ¥å£
from PIL import Image  # å›¾åƒåŸºç¡€æ“ä½œæ¨¡å—
from torch.cuda import amp  # æ··åˆç²¾åº¦è®­ç»ƒæ¨¡å—
```

é¦–å…ˆï¼Œå¯¼å…¥ä¸€ä¸‹å¸¸ç”¨çš„pythonåº“ï¼š

 *  jsonï¼š å®ç°å­—å…¸åˆ—è¡¨å’ŒJSONå­—ç¬¦ä¸²ä¹‹é—´çš„ç›¸äº’è§£æ
 *  math:  æ•°å­¦å‡½æ•°æ¨¡å—
 *  platform: è·å–æ“ä½œç³»ç»Ÿçš„ä¿¡æ¯
 *  warnings: è­¦å‘Šç¨‹åºå‘˜å…³äºè¯­è¨€æˆ–åº“åŠŸèƒ½çš„å˜åŒ–çš„æ–¹æ³• 
 *  copy: æ•°æ®æ‹·è´æ¨¡å— åˆ†æµ…æ‹·è´å’Œæ·±æ‹·è´
 *  pathlibï¼š è¿™ä¸ªåº“æä¾›äº†ä¸€ç§é¢å‘å¯¹è±¡çš„æ–¹å¼æ¥ä¸æ–‡ä»¶ç³»ç»Ÿäº¤äº’ï¼Œå¯ä»¥è®©ä»£ç æ›´ç®€æ´ã€æ›´æ˜“è¯»

ç„¶åå†å¯¼å…¥ä¸€äº› pytorchåº“ï¼š

 *  cv2:  è°ƒç”¨OpenCVçš„cvåº“
 *  numpyï¼š ç§‘å­¦è®¡ç®—åº“ï¼Œæä¾›äº†çŸ©é˜µï¼Œçº¿æ€§ä»£æ•°ï¼Œå‚…ç«‹å¶å˜æ¢ç­‰ç­‰çš„è§£å†³æ–¹æ¡ˆï¼Œæœ€å¸¸ç”¨çš„æ˜¯å®ƒçš„Nç»´æ•°ç»„å¯¹è±¡
 *  pandas: pandaæ•°ç»„æ“ä½œæ¨¡å—
 *  requests:  Pythonçš„HTTPå®¢æˆ·ç«¯åº“
 *  torchï¼š è¿™æ˜¯ä¸»è¦çš„Pytorchåº“ã€‚å®ƒæä¾›äº†æ„å»ºã€è®­ç»ƒå’Œè¯„ä¼°ç¥ç»ç½‘ç»œçš„å·¥å…·
 *  torch.nnï¼š torchä¸‹åŒ…å«ç”¨äºæ­å»ºç¥ç»ç½‘ç»œçš„moduleså’Œå¯ç”¨äºç»§æ‰¿çš„ç±»çš„ä¸€ä¸ªå­åŒ…
 *  PIL: å›¾åƒåŸºç¡€æ“ä½œæ¨¡å—
 *  torch.cudaï¼š è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒ â€”â€” èŠ‚çœæ˜¾å­˜å¹¶åŠ å¿«æ¨ç†é€Ÿåº¦

### 1.2 åŠ è½½è‡ªå®šä¹‰æ¨¡å— 

```java
'''===================2.åŠ è½½è‡ªå®šä¹‰æ¨¡å—============================'''
from utils.datasets import exif_transpose, letterbox  # åŠ è½½æ•°æ®é›†çš„å‡½æ•°
from utils.general import (LOGGER, check_requirements, check_suffix, colorstr, increment_path, make_divisible,
                           non_max_suppression, scale_coords, xywh2xyxy, xyxy2xywh)  # å®šä¹‰äº†ä¸€äº›å¸¸ç”¨çš„å·¥å…·å‡½æ•°
from utils.plots import Annotator, colors, plot_one_box  # å®šä¹‰äº†Annotatorç±»ï¼Œå¯ä»¥åœ¨å›¾åƒä¸Šç»˜åˆ¶çŸ©å½¢æ¡†å’Œæ ‡æ³¨ä¿¡æ¯
from utils.torch_utils import time_sync  # å®šä¹‰äº†ä¸€äº›ä¸PyTorchæœ‰å…³çš„å·¥å…·å‡½æ•°
```

è¿™äº›éƒ½æ˜¯ç”¨æˆ·è‡ªå®šä¹‰çš„åº“ï¼Œç”±äºä¸Šä¸€æ­¥å·²ç»æŠŠè·¯å¾„åŠ è½½ä¸Šäº†ï¼Œæ‰€ä»¥ç°åœ¨å¯ä»¥å¯¼å…¥ï¼Œè¿™ä¸ªé¡ºåºä¸å¯ä»¥è°ƒæ¢ã€‚å…·ä½“æ¥è¯´ï¼Œä»£ç ä»å¦‚ä¸‹å‡ ä¸ªæ–‡ä»¶ä¸­å¯¼å…¥äº†éƒ¨åˆ†å‡½æ•°å’Œç±»ï¼š

 *  utils.datasetsï¼š åŠ è½½æ•°æ®é›†çš„å‡½æ•°
 *  utils.generalï¼š å®šä¹‰äº†ä¸€äº›å¸¸ç”¨çš„å·¥å…·å‡½æ•°ï¼Œæ¯”å¦‚æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ã€æ£€æŸ¥å›¾åƒå¤§å°æ˜¯å¦ç¬¦åˆè¦æ±‚ã€æ‰“å°å‘½ä»¤è¡Œå‚æ•°ç­‰ç­‰
 *  utils.plotsï¼š å®šä¹‰äº†Annotatorç±»ï¼Œå¯ä»¥åœ¨å›¾åƒä¸Šç»˜åˆ¶çŸ©å½¢æ¡†å’Œæ ‡æ³¨ä¿¡æ¯
 *  utils.torch\_utilsï¼š å®šä¹‰äº†ä¸€äº›ä¸PyTorchæœ‰å…³çš„å·¥å…·å‡½æ•°ï¼Œæ¯”å¦‚é€‰æ‹©è®¾å¤‡ã€åŒæ­¥æ—¶é—´ç­‰é€šè¿‡å¯¼å…¥è¿™äº›æ¨¡å—ï¼Œå¯ä»¥æ›´æ–¹ä¾¿åœ°è¿›è¡Œç›®æ ‡æ£€æµ‹çš„ç›¸å…³ä»»åŠ¡ï¼Œå¹¶ä¸”å‡å°‘äº†ä»£ç çš„å¤æ‚åº¦å’Œå†—ä½™

## ğŸš€äºŒã€ åŸºç¡€ç»„ä»¶ 

### 2.1 autopad 

```java
'''===========1.autopadï¼šæ ¹æ®è¾“å…¥çš„å·ç§¯æ ¸è®¡ç®—è¯¥å·ç§¯æ¨¡å—æ‰€éœ€çš„padå€¼================'''
# ä¸ºsameå·ç§¯æˆ–è€…sameæ± åŒ–è‡ªåŠ¨æ‰©å……
# é€šè¿‡å·ç§¯æ ¸çš„å¤§å°æ¥è®¡ç®—éœ€è¦çš„paddingä¸ºå¤šå°‘æ‰èƒ½æŠŠtensorè¡¥æˆåŸæ¥çš„å½¢çŠ¶
def autopad(k, p=None):  # kernel, padding
    # å¦‚æœpæ˜¯none åˆ™è¿›è¡Œä¸‹ä¸€æ­¥
    if p is None:
        # å¦‚æœkæ˜¯int åˆ™è¿›è¡Œk//2 è‹¥ä¸æ˜¯åˆ™è¿›è¡Œx//2
        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad
    return p
```

autopadä¸»è¦ä½œç”¨æ˜¯æ ¹æ®è¾“å…¥çš„å·ç§¯æ ¸è®¡ç®—éœ€è¦çš„paddingä¸ºå¤šå°‘æ‰èƒ½æŠŠtensorè¡¥æˆåŸæ¥çš„å½¢çŠ¶

å‚æ•°ï¼š

 *  k:  å·ç§¯æ ¸çš„kernel\_size
 *  p:  è®¡ç®—çš„éœ€è¦padå€¼ï¼ˆ0å¡«å……ï¼‰

è¿™é‡Œé¦–å…ˆæ˜¯åˆ¤æ–­æ˜¯å¦æœ‰på€¼ï¼š

 *  å¦‚æœæœ‰æ—¢å®šçš„ p ï¼Œåˆ™ç›´æ¥ return pï¼Œè‡ªåŠ¨è®¡ç®—æ‰€éœ€è¦çš„padå€¼
 *  å¦‚æœæ— è®¾å®šçš„ pï¼Œåˆ™ return ä½¿å›¾åƒåœ¨å·ç§¯æ“ä½œåå°ºå¯¸ä¸å˜çš„ p

### 2.2 Conv 

```java
'''===========2.Convï¼šæ ‡å‡†å·ç§¯ ç”±Conv + BN + activateç»„æˆ================'''
class Conv(nn.Module):
    # Standard convolution
    # initåˆå§‹åŒ–æ„é€ å‡½æ•°
    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups
        
        super().__init__()
        # å·ç§¯å±‚
        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False)
        # å½’ä¸€åŒ–å±‚
        self.bn = nn.BatchNorm2d(c2)
        # æ¿€æ´»å‡½æ•°
        self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity())

    # æ­£å‘è®¡ç®—ï¼Œç½‘ç»œæ‰§è¡Œçš„é¡ºåºæ˜¯æ ¹æ®forwardå‡½æ•°æ¥å†³å®šçš„
    def forward(self, x):
        # convå·ç§¯ -> bn -> actæ¿€æ´»
        return self.act(self.bn(self.conv(x)))

    # æ­£å‘èåˆè®¡ç®—
    def forward_fuse(self, x):
        # è¿™é‡Œåªæœ‰å·ç§¯å’Œæ¿€æ´»
        return self.act(self.conv(x))
```

Convæ˜¯æ ‡å‡†å·ç§¯å±‚å‡½æ•°ï¼Œæ˜¯æ•´ä¸ªç½‘ç»œä¸­æœ€æ ¸å¿ƒçš„æ¨¡å—ï¼Œç”±å·ç§¯å±‚ + BNå±‚ + æ¿€æ´»å‡½æ•° ç»„æˆã€‚

ä¸»è¦ä½œç”¨æ˜¯å®ç°äº†å°†è¾“å…¥ç‰¹å¾ç»è¿‡å·ç§¯å±‚ï¼Œæ¿€æ´»å‡½æ•°ï¼Œå½’ä¸€åŒ–å±‚ï¼Œå¾—åˆ°è¾“å‡ºå±‚ã€‚åŒæ—¶å¯ä»¥æŒ‡å®šæ˜¯å¦ä½¿ç”¨å½’ä¸€åŒ–å±‚ã€‚

å…·ä½“ç»“æ„å¦‚ä¸‹å›¾ï¼š

![](https://i-blog.csdnimg.cn/blog_migrate/354621ee91e4a1967e4b3b313feb529f.png)

å‚æ•°ï¼š

 *  c1:  è¾“å…¥çš„channelå€¼
 *  c2: è¾“å‡ºçš„channelå€¼
 *  k: å·ç§¯çš„kernel\_sizeï¼Œk=1
 *  s:  å·ç§¯çš„strideï¼Œs=1
 *  p:  å·ç§¯çš„padding ï¼Œä¸€èˆ¬æ˜¯None ï¼Œå¯ä»¥é€šè¿‡autopadè‡ªè¡Œè®¡ç®—éœ€è¦padçš„paddingæ•°
 *  autopadï¼ˆk,pï¼‰ï¼š æ­¤å¤„æ¢æˆè‡ªåŠ¨å¡«å……
 *  gï¼š g=1è¡¨ç¤ºä»è¾“å…¥é€šé“åˆ°è¾“å‡ºé€šé“çš„é˜»å¡è¿æ¥æ•°ä¸º1
 *  actï¼š æ¿€æ´»å‡½æ•°ç±»å‹ï¼ŒTrueå°±æ˜¯SiLU()/Swishï¼ŒFalseå°±æ˜¯ä¸ä½¿ç”¨æ¿€æ´»å‡½æ•°ï¼Œç±»å‹æ˜¯nn.Moduleå°±ä½¿ç”¨ä¼ è¿›æ¥çš„æ¿€æ´»å‡½æ•°ç±»å‹

æ³¨æ„ï¼Œè¿™ä¸ªç±»ä¸­è¿˜æœ‰ä¸€ä¸ªç‰¹æ®Šå‡½æ•° fuseforward ï¼Œè¿™æ˜¯ä¸€ä¸ªå‰å‘åŠ é€Ÿæ¨ç†æ¨¡å—ï¼Œåœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡èåˆconv + bnå±‚ï¼Œè¾¾åˆ°åŠ é€Ÿæ¨ç†çš„ä½œç”¨ï¼Œä¸€èˆ¬ç”¨äºæµ‹è¯•æˆ–éªŒè¯é˜¶æ®µã€‚

> nn.Conv2då‡½æ•°åŸºæœ¬å‚æ•°ï¼š
> 
> ```java
> nn.Conv2d(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')
> ```
> 
>  *  in\_channel:  è¾“å…¥æ•°æ®çš„é€šé“æ•°ï¼Œä¾‹RGBå›¾ç‰‡é€šé“æ•°ä¸º3ã€‚
>  *  out\_channel:  è¾“å‡ºæ•°æ®çš„é€šé“æ•°ï¼Œè¿™ä¸ªæ ¹æ®æ¨¡å‹è°ƒæ•´ã€‚
>  *  kennel\_size:  å·ç§¯æ ¸å¤§å°ï¼Œå¯ä»¥æ˜¯intï¼Œæˆ–tupleï¼›kennel\_size=2,æ„å‘³ç€å·ç§¯å¤§å°(2,2)ï¼Œkennel\_size=ï¼ˆ2,3ï¼‰ï¼Œæ„å‘³ç€å·ç§¯å¤§å°ï¼ˆ2ï¼Œ3ï¼‰å³éæ­£æ–¹å½¢å·ç§¯ã€‚
>  *  strideï¼š æ­¥é•¿ï¼Œé»˜è®¤ä¸º1ï¼Œä¸kennel\_sizeç±»ä¼¼ï¼Œstride=2,æ„å‘³ç€æ­¥é•¿ä¸Šä¸‹å·¦å³æ‰«æçš†ä¸º2ï¼Œstride=ï¼ˆ2,3ï¼‰ï¼Œå·¦å³æ‰«ææ­¥é•¿ä¸º2ï¼Œä¸Šä¸‹ä¸º3ã€‚
>  *  paddingï¼š é›¶å¡«å……ã€‚
>  *  groupsï¼š ä»è¾“å…¥é€šé“åˆ°è¾“å‡ºé€šé“çš„é˜»å¡è¿æ¥æ•°ã€‚
>  *  biasï¼š å¦‚æœä¸ºâ€œTrueâ€œï¼Œåˆ™å‘è¾“å‡ºæ·»åŠ å¯å­¦ä¹ çš„åç½®ã€‚

### 2.3 DWConv 

```java
'''===========3.DWConvï¼šæ·±åº¦å¯åˆ†ç¦»å·ç§¯================'''
class DWConv(Conv):
    # Depth-wise convolution class
    def __init__(self, c1, c2, k=1, s=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups
        super().__init__(c1, c2, k, s, g=math.gcd(c1, c2), act=act)
```

DWConvæ˜¯GCONVçš„æç«¯æƒ…å†µï¼Œæ·±åº¦åˆ†ç¦»(DepthWise)å·ç§¯å±‚

åˆ†ç»„æ•°é‡ç­‰äºè¾“å…¥é€šé“æ•°é‡ï¼Œå³æ¯ä¸ªé€šé“ä½œä¸ºä¸€ä¸ªå°ç»„åˆ†åˆ«è¿›è¡Œå·ç§¯ï¼Œç»“æœè”ç»“ä½œä¸ºè¾“å‡ºï¼ŒCin = Cout = gï¼Œæ²¡æœ‰biasé¡¹ã€‚

ä¸»è¦ä½œç”¨æ˜¯å°†é€šé“æŒ‰è¾“å…¥è¾“å‡ºçš„æœ€å¤§å…¬çº¦æ•°è¿›è¡Œåˆ‡åˆ†ï¼Œåœ¨ä¸åŒçš„é€šé“å›¾å±‚ä¸Šè¿›è¡Œç‰¹å¾å­¦ä¹ æ·±åº¦åˆ†ç¦»å·ç§¯å±‚ï¼Œä¸ç”¨æ·±å…¥ç ”ç©¶ï¼Œå› ä¸ºåœ¨yolov5ä¸­æ²¡æœ‰çœŸæ­£çš„ä½¿ç”¨~

å…·ä½“ç»“æ„å¦‚ä¸‹å›¾ï¼š

![](https://i-blog.csdnimg.cn/blog_migrate/8c8cd1026a29031de317605deb7aa830.png)

å‚æ•°

 *  c1:  è¾“å…¥çš„channelå€¼
 *  c2: è¾“å‡ºçš„channelå€¼
 *  k:  å·ç§¯çš„kernel\_sizeï¼Œk=1
 *  s: å·ç§¯çš„strideï¼Œs=1
 *  act:  æ¿€æ´»å‡½æ•°ç±»å‹ï¼ŒTrueå°±æ˜¯SiLU()/Swishï¼ŒFalseå°±æ˜¯ä¸ä½¿ç”¨æ¿€æ´»å‡½æ•°ï¼Œç±»å‹æ˜¯nn.Moduleå°±ä½¿ç”¨ä¼ è¿›æ¥çš„æ¿€æ´»å‡½æ•°ç±»å‹

### 2.4 Bottleneck 

```java
'''===========4.Bottleneckï¼šæ ‡å‡†çš„ç“¶é¢ˆå±‚ ç”±1x1conv+3x3conv+æ®‹å·®å—ç»„æˆ================'''
class Bottleneck(nn.Module):
    # Standard bottleneck
    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, shortcut, groups, expansion
      
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        # 1*1å·ç§¯å±‚
        self.cv1 = Conv(c1, c_, 1, 1)
        # 3*3å·ç§¯å±‚
        self.cv2 = Conv(c_, c2, 3, 1, g=g)
        # å¦‚æœshortcutä¸ºTrueå°±ä¼šå°†è¾“å…¥å’Œè¾“å‡ºç›¸åŠ ä¹‹åå†è¾“å‡º
        self.add = shortcut and c1 == c2

    def forward(self, x):
        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))
```

Bottleneckæ˜¯ä¸€ä¸ªæ ‡å‡†çš„ç“¶é¢ˆå±‚ï¼Œç”±ä¸€äº› 1x1convã€3x3convã€æ®‹å·®å—ç»„æˆã€‚

å…·ä½“ç»“æ„å¦‚ä¸‹å›¾ï¼š

![](https://i-blog.csdnimg.cn/blog_migrate/2020b65aa16ae6e920957252bd8eb297.png)

æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸Šå›¾çœ‹å‡ºï¼Œç½‘ç»œæ¶æ„ä¸­çš„Bottleneckæ¨¡å—åˆ†ä¸ºTrueå’ŒFalseã€‚ä¸»è¦ä½œç”¨æ˜¯å¯ä»¥æ›´åŠ æœ‰æ•ˆçš„æå–ç‰¹å¾ï¼Œæ—¢å‡å°‘äº†å‚æ•°é‡ï¼Œåˆä¼˜åŒ–äº†è®¡ç®—ï¼Œä¿æŒäº†åŸæœ‰çš„ç²¾åº¦ã€‚

é¦–å…ˆBottleneckå…ˆè¿›è¡Œ1x1å·ç§¯é™ç»´ï¼Œå†è¿›è¡Œå¸¸è§„3Ã—3å·ç§¯æ ¸çš„å·ç§¯ã€‚æœ€åé€šè¿‡æ®‹å·®ç»“æ„è¿æ¥åœ¨ä¸€èµ·ã€‚

å‚æ•°ï¼š

 *  c1ï¼š ç¬¬ä¸€ä¸ªå·ç§¯çš„è¾“å…¥channel
 *  c2ï¼š ç¬¬äºŒä¸ªå·ç§¯çš„è¾“å‡ºchannel
 *  shortcutï¼š  bool æ˜¯å¦æœ‰shortcutè¿æ¥ é»˜è®¤æ˜¯True
 *  gï¼š è¡¨ç¤ºä»è¾“å…¥é€šé“åˆ°è¾“å‡ºé€šé“çš„é˜»å¡è¿æ¥æ•°ä¸º1
 *  eï¼š expansion ratio e\*c2å°±æ˜¯ç¬¬ä¸€ä¸ªå·ç§¯çš„è¾“å‡ºchannel=ç¬¬äºŒä¸ªå·ç§¯çš„è¾“å…¥channel

æ¨¡å‹ç»“æ„ï¼š

![](https://i-blog.csdnimg.cn/blog_migrate/ac61dfece8be9ab93e6f38019f4ccec0.png)

é€šè¿‡ä¸Šé¢ç“¶é¢ˆå±‚çš„æ¨¡å‹ç»“æ„ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç“¶é¢ˆä¸»è¦ä½“ç°åœ¨é€šé“æ•°channelä¸Šé¢ã€‚

å›¾ä¸­çš„çº¢è‰²è™šçº¿æ˜¯shortcutï¼Œè¿™é‡Œä½¿ç”¨çš„shortcutæˆä¸ºidentityåˆ†æ”¯ï¼Œå¯ä»¥ç†è§£ä¸ºæ’ç­‰æ˜ å°„ï¼Œå¦ä¸€ä¸ªåˆ†æ”¯è¢«ç§°ä¸ºæ®‹å·®åˆ†æ”¯(Residualåˆ†æ”¯)ã€‚

æˆ‘ä»¬å¸¸ä½¿ç”¨çš„æ®‹å·®åˆ†æ”¯å®é™…ä¸Šæ˜¯`1x1`\+`3x3`\+`1x1`çš„ç»“æ„

### 2.5 BottleneckCSP 

```java
'''===========5.BottleneckCSPï¼šç“¶é¢ˆå±‚ ç”±å‡ ä¸ªBottleneckæ¨¡å—çš„å †å +CSPç»“æ„ç»„æˆ================'''
class BottleneckCSP(nn.Module):
    # CSP Bottleneck https://github.com/WongKinYiu/CrossStagePartialNetworks
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        # 4ä¸ª1*1å·ç§¯å±‚çš„å †å 
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = nn.Conv2d(c1, c_, 1, 1, bias=False)
        self.cv3 = nn.Conv2d(c_, c_, 1, 1, bias=False)
        self.cv4 = Conv(2 * c_, c2, 1, 1)
        # bnå±‚
        self.bn = nn.BatchNorm2d(2 * c_)  # applied to cat(cv2, cv3)
        # æ¿€æ´»å‡½æ•°
        self.act = nn.SiLU()
        # mï¼šå åŠ næ¬¡Bottleneckçš„æ“ä½œ
        # æ“ä½œç¬¦*å¯ä»¥æŠŠä¸€ä¸ªlistæ‹†å¼€æˆä¸€ä¸ªä¸ªç‹¬ç«‹çš„å…ƒç´ 
        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))

    def forward(self, x):
        # y1ç›¸å½“äºå…ˆåšä¸€æ¬¡cv1æ“ä½œç„¶åè¿›è¡Œmæ“ä½œæœ€åè¿›è¡Œcv3æ“ä½œï¼Œä¹Ÿå°±æ˜¯BCSPnæ¨¡å—ä¸­çš„ä¸Šé¢çš„åˆ†æ”¯æ“ä½œ
        # è¾“å…¥x ->Convæ¨¡å— ->nä¸ªbottleneckæ¨¡å— ->Convæ¨¡å— ->y1
        y1 = self.cv3(self.m(self.cv1(x)))
        # y2å°±æ˜¯è¿›è¡Œcv2æ“ä½œï¼Œä¹Ÿå°±æ˜¯BCSPnæ¨¡å—ä¸­çš„ä¸‹é¢çš„åˆ†æ”¯æ“ä½œï¼ˆç›´æ¥é€†è¡Œconvæ“ä½œçš„åˆ†æ”¯ï¼Œ Conv--nXBottleneck--convï¼‰
        # è¾“å…¥x -> Convæ¨¡å— -> è¾“å‡ºy2
        y2 = self.cv2(x)
        # æœ€åy1å’Œy2åšæ‹¼æ¥ï¼Œ æ¥ç€è¿›å…¥bnå±‚åšå½’ä¸€åŒ–ï¼Œ ç„¶ååšactæ¿€æ´»ï¼Œ æœ€åè¾“å‡ºcv4
        # è¾“å…¥y1,y2->æŒ‰ç…§é€šé“æ•°èåˆ ->å½’ä¸€åŒ– -> æ¿€æ´»å‡½æ•° -> Convè¾“å‡º -> è¾“å‡º
        # torch.cat(y1, y2), dim=1: è¿™é‡Œæ˜¯æŒ‡å®šåœ¨ç¬¬ä¸€ä¸ªç»´åº¦ä¸Šè¿›è¡Œåˆå¹¶ï¼Œå³åœ¨channelç»´åº¦ä¸Šåˆå¹¶
        return self.cv4(self.act(self.bn(torch.cat((y1, y2), dim=1))))
```

BottleneckCSPä¹Ÿæ˜¯ç“¶é¢ˆå±‚ï¼Œç”±Bottleneckæ¨¡å—å’ŒCSPç»“æ„ç»„æˆ

å…·ä½“ç»“æ„å¦‚ä¸‹å›¾ï¼š

![](https://i-blog.csdnimg.cn/blog_migrate/b158314ed4baf22dbd92fc4a144e0454.png)ç”±ä¸Šå›¾å¯ä»¥çœ‹å‡ºBottleneckCSPä¸­cv2å’Œcv3è°ƒç”¨çš„æ˜¯ç³»ç»Ÿçš„å·ç§¯å±‚ï¼Œä½¿ç”¨concatè¿æ¥ä¹‹åï¼ŒåŠ ä¸ŠBNå±‚å’Œæ¿€æ´»å‡½æ•°ã€‚

CSPç»“æ„ä¸»è¦æ€æƒ³æ˜¯åœ¨è¾“å…¥blockï¼ˆå¦‚Bottleneckï¼‰ä¹‹å‰ï¼Œå°†è¾“å…¥åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼Œå…¶ä¸­ä¸€éƒ¨åˆ†é€šè¿‡blockè¿›è¡Œè®¡ç®—ï¼Œå¦ä¸€éƒ¨åˆ†ç›´æ¥é€šè¿‡ä¸€ä¸ªå¸¦å·ç§¯shortcutè¿›è¡Œconcatã€‚  
ä¸»è¦ä½œç”¨æ˜¯åŠ å¼ºCNNçš„å­¦ä¹ èƒ½åŠ›ã€å‡å°‘å†…å­˜æ¶ˆè€—ï¼Œå‡å°‘è®¡ç®—ç“¶é¢ˆï¼Œç°åœ¨çš„ç½‘ç»œå¤§å¤šè®¡ç®—ä»£ä»·æ˜‚è´µï¼Œä¸åˆ©äºå·¥ä¸šçš„è½åœ°ã€‚

å‚æ•°ï¼š

 *  c1:  æ•´ä¸ªBottleneckCSPçš„è¾“å…¥channel
 *  c2:  æ•´ä¸ªBottleneckCSPçš„è¾“å‡ºchannel
 *  n: æœ‰nä¸ªBottleneck
 *  gï¼š g=1ï¼Œè¡¨ç¤ºä»è¾“å…¥é€šé“åˆ°è¾“å‡ºé€šé“çš„é˜»å¡è¿æ¥æ•°ä¸º1
 *  e: expansion ratio c2xe=ä¸­é—´å…¶ä»–æ‰€æœ‰å±‚çš„å·ç§¯æ ¸ä¸ªæ•°/ä¸­é—´æ‰€æœ‰å±‚çš„è¾“å…¥è¾“å‡ºchannelæ•°
 *  torch.cat((y1, y2), dim=1)ï¼š è¿™é‡Œæ˜¯æŒ‡å®šåœ¨ç¬¬11ä¸ªç»´åº¦ä¸Šè¿›è¡Œåˆå¹¶ï¼Œå³åœ¨channelç»´åº¦ä¸Šåˆå¹¶
 *  c\_:  bottleneckCSP ç»“æ„çš„ä¸­é—´å±‚çš„é€šé“æ•°ï¼Œç”±è†¨èƒ€ç‡eå†³å®š

æ¨¡å‹ç»“æ„ï¼š 

![](https://i-blog.csdnimg.cn/blog_migrate/c44d088559553b7a7629ade7d9c894a0.png)

CSPç“¶é¢ˆå±‚ç»“æ„åœ¨Bottleneckéƒ¨åˆ†å­˜åœ¨ä¸€ä¸ªå¯ä¿®æ”¹çš„å‚æ•°nï¼Œæ ‡è¯†ä½¿ç”¨çš„Bottleneckç»“æ„ä¸ªæ•°ã€‚è¿™ä¸€æ¡ä¹Ÿæ˜¯æˆ‘ä»¬çš„ä¸»åˆ†æ”¯ï¼Œæ˜¯å¯¹æ®‹å·®è¿›è¡Œå­¦ä¹ çš„ä¸»è¦ç»“æ„(è¿™é‡Œæ²¡æœ‰å®ç°DenseNetï¼Œå¯é€‰çš„æœ‰å·ç§¯å—ï¼Œtransformerå—ã€Ghostå—)ï¼Œå³ä¾§åˆ†æ”¯`nn.Conv2d`å®é™…ä¸Šæ˜¯shortcutåˆ†æ”¯å®ç°ä¸åŒstageçš„è¿æ¥(CSPçš„æ€æƒ³å®ç°)ã€‚

### 2.6 C3 

#### 2.6.1 C3 

```java
'''===========6.C3ï¼šå’ŒBottleneckCSPæ¨¡å—ç±»ä¼¼ï¼Œä½†æ˜¯å°‘äº†ä¸€ä¸ªConvæ¨¡å—================'''
# ===6.1 C3=== #
class C3(nn.Module):
    # CSP Bottleneck with 3 convolutions
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion

        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        # 3ä¸ª1*1å·ç§¯å±‚çš„å †å ï¼Œæ¯”BottleneckCSPå°‘ä¸€ä¸ª
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c1, c_, 1, 1)
        self.cv3 = Conv(2 * c_, c2, 1)  # act=FReLU(c2)
        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))
        # self.m = nn.Sequential(*[CrossConv(c_, c_, 3, 1, g, 1.0, shortcut) for _ in range(n)])

    def forward(self, x):
        # å°†ç¬¬ä¸€ä¸ªå·ç§¯å±‚ä¸ç¬¬äºŒä¸ªå·ç§¯å±‚çš„ç»“æœæ‹¼æ¥åœ¨ä¸€èµ·
        return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), dim=1))
```

C3æ˜¯ä¸€ç§ç®€åŒ–ç‰ˆçš„BottleneckCSPï¼Œæ¨¡å—å’ŒBottleneckCSPæ¨¡å—ç±»ä¼¼ï¼Œä½†æ˜¯å°‘äº†ä¸€ä¸ªConvæ¨¡å—ï¼Œåªæœ‰3ä¸ªå·ç§¯ï¼Œå¯ä»¥å‡å°‘å‚æ•°ï¼Œæ‰€ä»¥å–åC3ã€‚å…¶å®ç»“æ„æ˜¯ä¸€æ ·çš„ï¼Œå†™æ³•ç•¥å¾®æœ‰å·®å¼‚ã€‚

BottleneckCSPä¸­cv2å’Œcv3è°ƒç”¨çš„æ˜¯ç³»ç»Ÿçš„å·ç§¯å±‚ï¼Œä½¿ç”¨concatè¿æ¥ä¹‹ååŠ ä¸ŠBNå±‚å’Œæ¿€æ´»å‡½æ•°ï¼›C3åˆ™ç›´æ¥ä½¿ç”¨äº†ä½œè€…è‡ªå·±å®šä¹‰çš„å·ç§¯å±‚ï¼ˆconv+batchnorm+SiLUï¼‰ï¼Œè¿™é‡Œæ¿€æ´»å‡½æ•°ä¹Ÿæœ‰ä¿®æ”¹ã€‚

å…·ä½“ç»“æ„å¦‚ä¸‹å›¾ï¼š

![](https://i-blog.csdnimg.cn/blog_migrate/734de2a3d5f7d351177d29cc4ef436b4.png)

å‚æ•°ï¼š

 *  c1:  æ•´ä¸ªBottleneckCSPçš„è¾“å…¥channel
 *  c2:  æ•´ä¸ªBottleneckCSPçš„è¾“å‡ºchannel
 *  n: æœ‰nä¸ªBottleneck
 *  shortcut:bool Bottleneckä¸­æ˜¯å¦æœ‰shortcutï¼Œé»˜è®¤True
 *  gï¼š g=1ï¼Œè¡¨ç¤ºä»è¾“å…¥é€šé“åˆ°è¾“å‡ºé€šé“çš„é˜»å¡è¿æ¥æ•°ä¸º1
 *  e: expansion ratio c2xe=ä¸­é—´å…¶ä»–æ‰€æœ‰å±‚çš„å·ç§¯æ ¸ä¸ªæ•°/ä¸­é—´æ‰€æœ‰å±‚çš„è¾“å…¥è¾“å‡ºchannelæ•°

#### 2.6.2 C3SPP(C3) 

```java
# ===6.2 C3SPP(C3)ï¼šç»§æ‰¿è‡ª C3ï¼Œn ä¸ª Bottleneck æ›´æ¢ä¸º 1 ä¸ª SPP=== #
class C3SPP(C3):
    # C3 module with SPP()
    def __init__(self, c1, c2, k=(5, 9, 13), n=1, shortcut=True, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)
        self.m = SPP(c_, c_, k)
```

C3SPP(C3)ï¼šç»§æ‰¿è‡ª C3ï¼Œå°†n ä¸ª Bottleneck æ›´æ¢ä¸º 1 ä¸ª SPP

å‚æ•°å’Œä¸Šé¢ä¸€æ ·ï¼Œä¸å†ç»†è®²~

#### 2.6.3 C3Ghost(C3) 

```java
# ===6.3 C3Ghost(C3)ï¼šç»§æ‰¿è‡ª C3ï¼ŒBottleneck æ›´æ¢ä¸º GhostBottleneck=== #
class C3Ghost(C3):
    # C3 module with GhostBottleneck()
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(*(GhostBottleneck(c_, c_) for _ in range(n)))
```

C3Ghost(C3)ï¼šç»§æ‰¿è‡ª C3ï¼Œå°†Bottleneck æ›´æ¢ä¸º GhostBottleneck

å‚æ•°å’Œä¸Šé¢ä¸€æ ·ï¼Œä¸å†ç»†è®²~

### 2.7 SPP 

#### 2.7.1 SPP 

```java
'''===========7.SPPï¼šç©ºé—´é‡‘å­—å¡”æ± åŒ–æ¨¡å—================'''

# ===7.1 SPPï¼šç©ºé—´é‡‘å­—å¡”æ± åŒ–=== #
class SPP(nn.Module):
    # Spatial Pyramid Pooling (SPP) layer https://arxiv.org/abs/1406.4729
    def __init__(self, c1, c2, k=(5, 9, 13)):
       
        super().__init__()
        c_ = c1 // 2  # hidden channels
        # 1*1å·ç§¯
        self.cv1 = Conv(c1, c_, 1, 1)
        #  è¿™é‡Œ+1æ˜¯å› ä¸ºæœ‰len(k)+1ä¸ªè¾“å…¥
        self.cv2 = Conv(c_ * (len(k) + 1), c2, 1, 1)
        # må…ˆè¿›è¡Œæœ€å¤§æ± åŒ–æ“ä½œï¼Œ ç„¶åé€šè¿‡nn.ModuleListè¿›è¡Œæ„é€ ä¸€ä¸ªæ¨¡å— åœ¨æ„é€ æ—¶å¯¹æ¯ä¸€ä¸ªkéƒ½è¦è¿›è¡Œæœ€å¤§æ± åŒ–
        self.m = nn.ModuleList([nn.MaxPool2d(kernel_size=x, stride=1, padding=x // 2) for x in k])

    def forward(self, x):
        # å…ˆè¿›è¡Œcv1çš„æ“ä½œ
        x = self.cv1(x)
        # å¿½ç•¥äº†è­¦å‘Šé”™è¯¯çš„è¾“å‡º
        with warnings.catch_warnings():
            warnings.simplefilter('ignore')  # suppress torch 1.9.0 max_pool2d() warning
            # å¯¹æ¯ä¸€ä¸ªmè¿›è¡Œæœ€å¤§æ± åŒ– å’Œæ²¡æœ‰åšæ± åŒ–çš„æ¯ä¸€ä¸ªè¾“å…¥è¿›è¡Œå åŠ   ç„¶ååšæ‹¼æ¥ æœ€ååšcv2æ“ä½œ
            return self.cv2(torch.cat([x] + [m(x) for m in self.m], 1))
```

SPP æ˜¯ç©ºé—´é‡‘å­—å¡”æ± åŒ–çš„ç¼©å†™ã€‚ç”¨åœ¨éª¨å¹²ç½‘ç»œæ”¶å°¾é˜¶æ®µï¼Œç”¨äºèåˆå¤šå°ºåº¦ç‰¹å¾ã€‚

SPPæ¨¡å—æ˜¯ä½•æºæ˜ç­‰å¤§ä½¬æå‡ºæ¥çš„ï¼Œéå¸¸ç»å…¸ä»yolov3ä¸­å¼€å§‹ä½¿ç”¨åˆ°ç°åœ¨ï¼Œyoloç³»åˆ—åŸºæœ¬ä¸Šéƒ½ç”¨åˆ°äº†ã€‚è¿™ä¸ªæ¨¡å—çš„ä¸»è¦ä½œç”¨æ˜¯ä¸ºäº†å°†æ›´å¤šä¸åŒåˆ†è¾¨ç‡çš„ç‰¹å¾è¿›è¡Œèåˆï¼Œå¾—åˆ°æ›´å¤šçš„ä¿¡æ¯ã€‚

å…·ä½“ç»“æ„å¦‚ä¸‹å›¾ï¼š

![](https://i-blog.csdnimg.cn/blog_migrate/747d9eb4d7e006cc28d759a7d49902e6.png)

å‚æ•°ï¼š

 *  c1:  SPPæ¨¡å—çš„è¾“å…¥channel
 *  c2:  SPPæ¨¡å—çš„è¾“å‡ºchannel
 *  k: ä¿å­˜ç€ä¸‰ä¸ªmaxpoolçš„å·ç§¯æ ¸å¤§å° é»˜è®¤æ˜¯(5, 9, 13)

#### 2.7.2 SPPF 

```java
# ===7.2 SPPFï¼šå¿«é€Ÿç‰ˆçš„ç©ºé—´é‡‘å­—å¡”æ± åŒ–=== #
class SPPF(nn.Module):
    # Spatial Pyramid Pooling - Fast (SPPF) layer for YOLOv5 by Glenn Jocher
    def __init__(self, c1, c2, k=5):  # equivalent to SPP(k=(5, 9, 13))
        super().__init__()
        c_ = c1 // 2  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c_ * 4, c2, 1, 1)
        self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)

    def forward(self, x):
        x = self.cv1(x)
        with warnings.catch_warnings():
            warnings.simplefilter('ignore')  # suppress torch 1.9.0 max_pool2d() warning
            y1 = self.m(x)
            y2 = self.m(y1)
            return self.cv2(torch.cat([x, y1, y2, self.m(y2)], 1))
```

SPPFæ˜¯å¿«é€Ÿç‰ˆçš„ç©ºé—´é‡‘å­—å¡”æ± åŒ–

æ± åŒ–å°ºå¯¸ç­‰ä»·äºï¼š5ã€9ã€13ï¼Œå’ŒåŸæ¥ä¸€æ ·ï¼Œä½†æ˜¯è¿ç®—é‡ä»åŸæ¥çš„ ![5^{2}+9^{2}+13^{2}=275](https://latex.csdn.net/eq?5%5E%7B2%7D&plus;9%5E%7B2%7D&plus;13%5E%7B2%7D%3D275) å‡å°‘åˆ°äº† ![3 \cdot 5^{2}=75](https://latex.csdn.net/eq?3%20%5Ccdot%205%5E%7B2%7D%3D75)

ï¼ˆYOLOv5ä¸­SPPå’ŒSPPFå¯ä»¥çœ‹è¿™ç¯‡ï¼š[YOLOv5ä¸­çš„SPP/SPPFç»“æ„è¯¦è§£\_ttä¸«çš„åšå®¢-CSDNåšå®¢][YOLOv5_SPP_SPPF_tt_-CSDN]ï¼‰

### 2.8 Focus 

```java
'''===========8.Focusï¼šæŠŠå®½åº¦wå’Œé«˜åº¦hçš„ä¿¡æ¯æ•´åˆåˆ°cç©ºé—´================'''
class Focus(nn.Module):
    # Focus wh information into c-space
    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups
       
        super().__init__()
        # concatåçš„å·ç§¯ï¼ˆæœ€åçš„å·ç§¯ï¼‰
        self.conv = Conv(c1 * 4, c2, k, s, p, g, act)
        # self.contract = Contract(gain=2)

    def forward(self, x):  # x(b,c,w,h) -> y(b,4c,w/2,h/2)
        # å…ˆè¿›è¡Œåˆ‡åˆ†ï¼Œ ç„¶åè¿›è¡Œæ‹¼æ¥ï¼Œ æœ€åå†åšconvæ“ä½œ
        return self.conv(torch.cat([x[..., ::2, ::2], x[..., 1::2, ::2], x[..., ::2, 1::2], x[..., 1::2, 1::2]], 1))
        # return self.conv(self.contract(x))

    # ä»¥ä¸‹æ¨¡å—Contractï¼ŒExpand,Concatæ˜¯ç”¨æ¥å¤„ç†è¾“å…¥ç‰¹å¾çš„shapeçš„
```

Focusæ˜¯YOLOv5ä½œè€…è‡ªå·±è®¾è®¡çš„ä¸€ä¸ªæ¨¡å—ï¼Œç”¨åœ¨äº†æ¨¡å‹çš„ä¸€å¼€å§‹ï¼Œä½œç”¨æ˜¯æŠŠå®½åº¦wå’Œé«˜åº¦hçš„ä¿¡æ¯æ•´åˆåˆ°cç©ºé—´ã€‚

å…·ä½“ç»“æ„å¦‚ä¸‹å›¾ï¼š

![](https://i-blog.csdnimg.cn/blog_migrate/234dc0d63513da90dfbef8d513e68847.png)

å‚æ•°ï¼š

 *  c1: sliceåçš„channel
 *  c2:  Focusæœ€ç»ˆè¾“å‡ºçš„channel
 *  k: æœ€åå·ç§¯çš„kernelï¼Œk=1
 *  s:  æœ€åå·ç§¯çš„strideï¼Œs=1
 *  p: æœ€åå·ç§¯çš„padding
 *  gï¼š g=1ï¼Œè¡¨ç¤ºä»è¾“å…¥é€šé“åˆ°è¾“å‡ºé€šé“çš„é˜»å¡è¿æ¥æ•°ä¸º1
 *  act:  boolæ¿€æ´»å‡½æ•°ç±»å‹ é»˜è®¤True:SiLU()/Swish False:ä¸ç”¨æ¿€æ´»å‡½æ•°

ä¸»è¦æ€æƒ³ï¼š 

Focusæ¨¡å—åœ¨YOLOv5ä¸­æ˜¯å›¾ç‰‡è¿›å…¥Backboneå‰ï¼Œå¯¹å›¾ç‰‡è¿›è¡Œåˆ‡ç‰‡æ“ä½œï¼Œå…·ä½“æ“ä½œæ˜¯åœ¨ä¸€å¼ å›¾ç‰‡ä¸­æ¯éš”ä¸€ä¸ªåƒç´ æ‹¿åˆ°ä¸€ä¸ªå€¼ï¼Œç±»ä¼¼äºé‚»è¿‘ä¸‹é‡‡æ ·ï¼Œè¿™æ ·å°±æ‹¿åˆ°äº†å››å¼ å›¾ç‰‡ï¼Œå››å¼ å›¾ç‰‡äº’è¡¥ï¼Œé•¿å¾—å·®ä¸å¤šï¼Œä½†æ˜¯æ²¡æœ‰ä¿¡æ¯ä¸¢å¤±ï¼Œè¿™æ ·ä¸€æ¥ï¼Œå°†Wã€Hä¿¡æ¯å°±é›†ä¸­åˆ°äº†é€šé“ç©ºé—´ï¼Œè¾“å…¥é€šé“æ‰©å……äº†4å€ï¼Œå³æ‹¼æ¥èµ·æ¥çš„å›¾ç‰‡ç›¸å¯¹äºåŸå…ˆçš„RGBä¸‰é€šé“æ¨¡å¼å˜æˆäº†12ä¸ªé€šé“ï¼Œæœ€åå°†å¾—åˆ°çš„æ–°å›¾ç‰‡å†ç»è¿‡å·ç§¯æ“ä½œï¼Œæœ€ç»ˆå¾—åˆ°äº†æ²¡æœ‰ä¿¡æ¯ä¸¢å¤±æƒ…å†µä¸‹çš„äºŒå€ä¸‹é‡‡æ ·ç‰¹å¾å›¾ã€‚

![](https://i-blog.csdnimg.cn/blog_migrate/bc893643e26c9a2ada44c12c9c9bda8d.png)

æ­¥éª¤ï¼š

é¦–å…ˆæŠŠè¾“å…¥xåˆ†åˆ«ä»ï¼ˆ0,0ï¼‰ã€ï¼ˆ1,0ï¼‰ã€ï¼ˆ0,1ï¼‰ã€ï¼ˆ1,1ï¼‰å¼€å§‹ï¼ŒæŒ‰æ­¥é•¿ä¸º2å–å€¼ï¼Œç„¶åè¿›è¡Œä¸€æ¬¡å·ç§¯ã€‚

ç„¶åå°†è¾“å…¥ï¼ˆb,c,w,hï¼‰çš„shapeå˜æˆäº†è¾“å‡ºï¼ˆb, 4c, w/2, h/2ï¼‰ã€‚ä¹Ÿå°±æ˜¯è¯´å°†ç‰¹å¾å±‚çš„é•¿å’Œå®½éƒ½ç¼©å‡ä¸ºåŸæ¥çš„ä¸€åŠï¼Œç„¶åé€šé“æ•°å˜æˆåŸæ¥çš„4å€ï¼Œä¹Ÿå¯ä»¥ç†è§£æˆå°†ä¸€ä¸ªå›¾ç‰‡ç­‰åˆ†åˆ‡æˆ4ä¸ªï¼Œæ¥ç€å°†è¿™å››ä¸ªå°çš„ä¸Šä¸‹å †å èµ·æ¥ã€‚

æœ€åå†ç»è¿‡ä¸€ä¸ªconvè¾“å‡ºã€‚

### 2.9 Contract 

```java
'''===========9.Contractï¼šæ”¶ç¼©æ¨¡å—ï¼šè°ƒæ•´å¼ é‡çš„å¤§å°ï¼Œå°†å®½é«˜æ”¶ç¼©åˆ°é€šé“ä¸­ã€‚================'''
class Contract(nn.Module):
    # Contract width-height into channels, i.e. x(1,64,80,80) to x(1,256,40,40)


    def __init__(self, gain=2):
        super().__init__()
        self.gain = gain

    def forward(self, x):
        b, c, h, w = x.size()  # assert (h / s == 0) and (W / s == 0), 'Indivisible gain'
        s = self.gain
        # permute: æ”¹å˜tensorçš„ç»´åº¦é¡ºåº
        x = x.view(b, c, h // s, s, w // s, s)  # x(1,64,40,2,40,2)
        # .view: æ”¹å˜tensorçš„ç»´åº¦
        x = x.permute(0, 3, 5, 1, 2, 4).contiguous()  # x(1,2,2,64,40,40)
        return x.view(b, c * s * s, h // s, w // s)  # x(1,256,40,40)
```

Contractæ˜¯æ”¶ç¼©æ¨¡å—ï¼Œè°ƒæ•´å¼ é‡çš„å¤§å°ï¼Œå°†å®½é«˜æ”¶ç¼©åˆ°é€šé“ä¸­ã€‚

å°†feature mapçš„wå’Œhç»´åº¦(ç¼©å°)çš„æ•°æ®æ”¶ç¼©åˆ°channelç»´åº¦ä¸Š(æ”¾å¤§)

å¦‚ï¼šå½“ gain = 2 çš„æ—¶å€™ï¼Œ(64, 80, 80) çš„å›¾åƒ -> (256, 40, 40) çš„å›¾åƒã€‚å…¶æ“ä½œç±»ä¼¼ Focusï¼Œä½†æ›´çµæ´»ï¼Œç›¸æ¯”ä¹‹ä¸‹å°‘äº†ä¸€ä¸ªå·ç§¯ã€‚

### 2.10 Expand 

```java
'''===========10.Expandï¼šæ‰©å¼ æ¨¡å—ï¼Œå°†ç‰¹å¾å›¾åƒç´ å˜å¤§================'''
class Expand(nn.Module):
    # Expand channels into width-height, i.e. x(1,64,80,80) to x(1,16,160,160)
    def __init__(self, gain=2):
        super().__init__()
        self.gain = gain

    def forward(self, x):
        b, c, h, w = x.size()  # assert C / s ** 2 == 0, 'Indivisible gain'
        s = self.gain
        x = x.view(b, s, s, c // s ** 2, h, w)  # x(1,2,2,16,80,80)
        x = x.permute(0, 3, 4, 1, 5, 2).contiguous()  # x(1,16,80,2,80,2)
        return x.view(b, c // s ** 2, h * s, w * s)  # x(1,16,160,160)
```

Expandæ˜¯Contractçš„é€†æ“ä½œï¼Œæ‰©å¼ æ¨¡å—ï¼Œå°†ç‰¹å¾å›¾åƒç´ å˜å¤§ã€‚  
æ”¹å˜è¾“å…¥ç‰¹å¾çš„shapeï¼Œæ˜¯å°†channelç»´åº¦ï¼ˆå˜å°ï¼‰çš„æ•°æ®æ‰©å±•åˆ° W å’Œ H ç»´åº¦ï¼ˆå˜å¤§ï¼‰ã€‚

å¦‚ï¼šå½“ gain = 2 çš„æ—¶å€™ï¼Œ(1,64,80,80) çš„å›¾åƒ -> (1,16,160,160) çš„å›¾åƒã€‚

### 2.11 Concat 

```java
'''===========11.Concatï¼šè‡ªå®šä¹‰concatæ¨¡å—ï¼Œdimensionå°±æ˜¯ç»´åº¦å€¼ï¼Œè¯´æ˜æ²¿ç€å“ªä¸€ä¸ªç»´åº¦è¿›è¡Œæ‹¼æ¥================'''
# ä½œæ‹¼æ¥çš„ä¸€ä¸ªç±»
# æ‹¼æ¥å‡½æ•°ï¼Œå°†ä¸¤ä¸ªtensorè¿›è¡Œæ‹¼æ¥
class Concat(nn.Module):
    # Concatenate a list of tensors along dimension
    def __init__(self, dimension=1):
        super().__init__()
        self.d = dimension

    def forward(self, x):
        return torch.cat(x, self.d)
```

Concatæ˜¯æ‹¼æ¥å‡½æ•°ï¼Œå°†ä¸¤ä¸ªtensorè¿›è¡Œæ‹¼æ¥èµ·æ¥ã€‚

è¿™ä¸ªæ˜¯è‡ªå®šä¹‰concatæ¨¡å—ï¼Œdimensionå°±æ˜¯ç»´åº¦å€¼ï¼Œè¯´æ˜æ²¿ç€å“ªä¸€ä¸ªç»´åº¦è¿›è¡Œæ‹¼æ¥ã€‚å½“ dimension = 1 æ—¶ï¼Œå°†å¤šå¼ ç›¸åŒå°ºå¯¸çš„å›¾åƒåœ¨é€šé“ç»´åº¦ä¸Šæ‹¼æ¥ (é€šé“æ•°å¯èƒ½ä¸åŒ)

è¿™ä¸ªå‡½æ•°æ˜¯è®²è‡ªèº«æŒ‰ç…§æŸä¸ªç»´åº¦è¿›è¡Œconcatï¼Œå¸¸ç”¨æ¥åˆå¹¶å‰åä¸¤ä¸ªfeature mapï¼Œä¹Ÿå°±æ˜¯yolov5sç»“æ„å›¾ä¸­çš„Concatã€‚

## ğŸš€ä¸‰ã€æ³¨æ„åŠ›æ¨¡å— 

å…³äºtransformerè¿™ä¸ªæˆ‘è¿˜æ²¡æœ‰å­¦ä¹ ï¼Œæ‰€ä»¥è¿™ä¸€å—å†…å®¹æš‚ä¸åšè¯¦è§£ï¼Œç­‰æˆ‘åæœŸå­¦è¿‡å†æ¥å¡«è¿™ä¸ªå‘å§~

è¿™é‡Œå…ˆæ”¾ä»£ç ï¼Œå°ä¼™ä¼´ä»¬è‡ªå·±çœ‹çœ‹å§ï¼

æˆ‘æ¥å¡«å‘å•¦ï¼transformerè¯·çœ‹è¿™é‡Œâ†’[transformer\_è·¯äººè´¾'Ï‰'çš„åšå®¢-CSDNåšå®¢][transformer_-CSDN]

### 3.1 TransformerLayer 

```java
'''===========1.TransformerLayerï¼š================'''
class TransformerLayer(nn.Module):
    # Transformer layer https://arxiv.org/abs/2010.11929 (LayerNorm layers removed for better performance)
    """
        Transformer layer https://arxiv.org/abs/2010.11929 (LayerNorm layers removed for better performance)

        è¿™éƒ¨åˆ†ç›¸å½“äºåŸè®ºæ–‡ä¸­çš„å•ä¸ªEncoderéƒ¨åˆ†(åªç§»é™¤äº†ä¸¤ä¸ªNorméƒ¨åˆ†, å…¶ä»–ç»“æ„å’ŒåŸæ–‡ä¸­çš„Encodingä¸€æ¨¡ä¸€æ ·)
       """
    def __init__(self, c, num_heads):
        super().__init__()
        self.q = nn.Linear(c, c, bias=False)
        self.k = nn.Linear(c, c, bias=False)
        self.v = nn.Linear(c, c, bias=False)
        # è¾“å…¥: queryã€keyã€value
        # è¾“å‡º: 0 attn_output å³é€šè¿‡self-attentionä¹‹åï¼Œä»æ¯ä¸€ä¸ªè¯è¯­ä½ç½®è¾“å‡ºæ¥çš„attention å’Œè¾“å…¥çš„queryå®ƒä»¬å½¢çŠ¶ä¸€æ ·çš„
        #      1 attn_output_weights å³attention weights æ¯ä¸€ä¸ªå•è¯å’Œä»»æ„å¦ä¸€ä¸ªå•è¯ä¹‹é—´éƒ½ä¼šäº§ç”Ÿä¸€ä¸ªweight
        self.ma = nn.MultiheadAttention(embed_dim=c, num_heads=num_heads)
        self.fc1 = nn.Linear(c, c, bias=False)
        self.fc2 = nn.Linear(c, c, bias=False)

    def forward(self, x):
        # å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ + æ®‹å·®(è¿™é‡Œç§»é™¤äº†LayerNorm for better performance)
        x = self.ma(self.q(x), self.k(x), self.v(x))[0] + x
        # feed forward å‰é¦ˆç¥ç»ç½‘ç»œ + æ®‹å·®(è¿™é‡Œç§»é™¤äº†LayerNorm for better performance)
        x = self.fc2(self.fc1(x)) + x
        return x
```

### 3.2 TransformerBlock 

```java
'''===========2.TransformerBlockï¼š================'''
class TransformerBlock(nn.Module):
    # Vision Transformer https://arxiv.org/abs/2010.11929
    def __init__(self, c1, c2, num_heads, num_layers):
        super().__init__()
        self.conv = None
        if c1 != c2:
            self.conv = Conv(c1, c2)
        self.linear = nn.Linear(c2, c2)  # learnable position embedding
        self.tr = nn.Sequential(*(TransformerLayer(c2, num_heads) for _ in range(num_layers)))
        self.c2 = c2

    def forward(self, x):
        if self.conv is not None:
            x = self.conv(x)
        b, _, w, h = x.shape
        p = x.flatten(2).permute(2, 0, 1)
        return self.tr(p + self.linear(p)).permute(1, 2, 0).reshape(b, self.c2, w, h)
```

## ğŸš€å››ã€å¹»è±¡æ¨¡å— 

### 4.1 GhostConv 

```java
'''===========1.GhostConvï¼šå¹»è±¡å·ç§¯  è½»é‡åŒ–ç½‘ç»œå·ç§¯æ¨¡å—================'''
class GhostConv(nn.Module):
    # Ghost Convolution https://github.com/huawei-noah/ghostnet

    def __init__(self, c1, c2, k=1, s=1, g=1, act=True):  # ch_in, ch_out, kernel, stride, groups
        super().__init__()
        c_ = c2 // 2  # hidden channels
        # ç¬¬ä¸€æ­¥å·ç§¯: å°‘é‡å·ç§¯, ä¸€èˆ¬æ˜¯ä¸€åŠçš„è®¡ç®—é‡
        self.cv1 = Conv(c1, c_, k, s, None, g, act)
        # ç¬¬äºŒæ­¥å·ç§¯: cheap operations ä½¿ç”¨3x3æˆ–5x5çš„å·ç§¯, å¹¶ä¸”æ˜¯é€ä¸ªç‰¹å¾å›¾çš„è¿›è¡Œå·ç§¯ï¼ˆDepth-wise convolutional
        self.cv2 = Conv(c_, c_, 5, 1, None, c_, act)

    def forward(self, x):
        y = self.cv1(x)
        return torch.cat([y, self.cv2(y)], 1)
```

GhostConvæ˜¯å¹»è±¡å·ç§¯ï¼Œå±äºè½»é‡åŒ–ç½‘ç»œå·ç§¯æ¨¡å—

å…·ä½“ç»“æ„å¦‚ä¸‹å›¾ï¼š

![](https://i-blog.csdnimg.cn/blog_migrate/5f7594a87a05d76d85db9fe947579b1e.png)

å‚æ•°ï¼š

 *  c1: è¾“å…¥çš„channelå€¼
 *  c2:  è¾“å‡ºçš„channelå€¼
 *  k:  å·ç§¯çš„kernel\_sizeï¼Œk=1
 *  s:  å·ç§¯çš„strideï¼Œs=1
 *  g:  g=1è¡¨ç¤ºä»è¾“å…¥é€šé“åˆ°è¾“å‡ºé€šé“çš„é˜»å¡è¿æ¥æ•°ä¸º1
 *  act:  æ¿€æ´»å‡½æ•°ç±»å‹ï¼ŒTrueå°±æ˜¯SiLU()/Swishï¼ŒFalseå°±æ˜¯ä¸ä½¿ç”¨æ¿€æ´»å‡½æ•°ï¼Œç±»å‹æ˜¯nn.Moduleå°±ä½¿ç”¨ä¼ è¿›æ¥çš„æ¿€æ´»å‡½æ•°ç±»å‹

GhostConvä¸»è¦ä½œç”¨æ˜¯å¯ä»¥ä»£æ›¿ä¸€èˆ¬çš„Convï¼ŒGhostBottleneckä»£æ›¿C3ï¼Œè‡³äºåœ¨å“ªäº›ä½ç½®ä»£æ›¿ï¼Œå¯ä»¥è‡ªå·±å†³å®šã€‚å¹»è±¡æ¨¡å—è™½ç„¶ä¸èƒ½å¢åŠ mAPï¼Œä½†æ˜¯å¯ä»¥å¤§å¤§å‡å°‘æ¨¡å‹è®¡ç®—é‡ã€‚

### 4.2 GhostBottleneck 

```java
'''===========2.GhostBottleneckï¼šå¹»è±¡ç“¶é¢ˆå±‚ ================'''
class GhostBottleneck(nn.Module):
    # Ghost Bottleneck https://github.com/huawei-noah/ghostnet
    def __init__(self, c1, c2, k=3, s=1):  # ch_in, ch_out, kernel, stride
        super().__init__()
        c_ = c2 // 2
        self.conv = nn.Sequential(GhostConv(c1, c_, 1, 1),  # pw
                                  DWConv(c_, c_, k, s, act=False) if s == 2 else nn.Identity(),  # dw
                                  GhostConv(c_, c2, 1, 1, act=False))  # pw-linear
        # æ³¨æ„, æºç ä¸­å¹¶ä¸æ˜¯ç›´æ¥Identityè¿æ¥, è€Œæ˜¯å…ˆç»è¿‡ä¸€ä¸ªDWConv + Conv, å†è¿›è¡Œshortcutè¿æ¥çš„ã€‚
        self.shortcut = nn.Sequential(DWConv(c1, c1, k, s, act=False),
                                      Conv(c1, c2, 1, 1, act=False)) if s == 2 else nn.Identity()

    def forward(self, x):
        return self.conv(x) + self.shortcut(x)
```

GhostBottlenecké¡¾åæ€ä¹‰å°±æ˜¯å¹»è±¡æ¨¡å—çš„ç“¶é¢ˆå±‚ã€‚

å‚æ•°ï¼š

 *  c1: è¾“å…¥çš„channelå€¼
 *  c2: è¾“å‡ºçš„channelå€¼
 *  k: å·ç§¯çš„kernel\_sizeï¼Œk=3
 *  s: å·ç§¯çš„strideï¼Œs=1

å…·ä½“ç»“æ„å¦‚ä¸‹å›¾ï¼š

![](https://i-blog.csdnimg.cn/blog_migrate/77e2d3e2159b92e41adc2ee81f56e3db.png)

è¿™æ˜¯ä¸€ä¸ªå¯å¤ç”¨æ¨¡å—ï¼Œæˆ‘ä»¬å¯ä»¥æ”¾åˆ°ç°æœ‰çš„ç½‘ç»œä¸­æ›¿æ¢æ‰Bottleneckæ¨¡å—ï¼Œä»è€Œå‡å°‘è®¡ç®—äº†ï¼Œé™ä½æ¨¡å‹ä½“ç§¯ã€‚ç±»ä¼¼äºResNetä¸­çš„åŸºæœ¬æ®‹å·®å—ï¼Œç”±ä¸¤ä¸ªå †å çš„Ghostæ¨¡å—ç»„æˆï¼š

 *  ç¬¬ä¸€ä¸ªGhostæ¨¡å—ç”¨ä½œæ‰©å±•å±‚ï¼Œå¢åŠ äº†é€šé“æ•°ã€‚è¿™é‡Œå°†è¾“å‡ºé€šé“æ•°ä¸è¾“å…¥é€šé“æ•°ä¹‹æ¯”ç§°ä¸ºexpansion ratioã€‚ç¬¬äºŒä¸ªGhostæ¨¡å—å‡å°‘é€šé“æ•°ï¼Œä»¥ä¸shortcutè·¯å¾„åŒ¹é…ã€‚ç„¶åï¼Œä½¿ç”¨shortcutè¿æ¥è¿™ä¸¤ä¸ªGhostæ¨¡å—çš„è¾“å…¥å’Œè¾“å‡ºã€‚
 *  ç¬¬äºŒä¸ªGhost æ¨¡å—ä¸ä½¿ç”¨ReLUå…¶ä»–å±‚åœ¨æ¯å±‚ä¹‹åéƒ½åº”ç”¨äº†æ‰¹é‡å½’ä¸€åŒ–ï¼ˆBNï¼‰å’ŒReLuæ¿€æ´»å‡½æ•°ï¼ˆä¸»è¦å€Ÿé‰´äº†MobileNetV2çš„æ€æƒ³ï¼‰

Ghost Bottleneckä¸­å¯¹äºstride = 2çš„æƒ…å†µï¼Œä¸¤ä¸ªGhost moduleä¹‹é—´é€šè¿‡ä¸€ä¸ªstride = 2çš„æ·±åº¦å·ç§¯è¿›è¡Œè¿æ¥ã€‚

ï¼ˆè¿™ä¸ªå†…å®¹ä»¥åä¹Ÿä¼šç­‰å­¦ä¹ åå†è¯¦ç»†è¯´ï¼Œè¿™é‡Œå‚è€ƒï¼š[\[ç›®æ ‡æ£€æµ‹\]-cvå¸¸ç”¨æ¨¡å—ghostbottleneckåŸç†è®²è§£ä¸pytorchå®ç°\_orangezsçš„åšå®¢-CSDNåšå®¢][-cv_ghostbottleneck_pytorch_orangezs_-CSDN]ï¼‰

## ğŸš€äº”ã€æ¨¡å‹æ‰©å±•æ¨¡å— 

### 5.1 C3TR(C3) 

```java
'''===========1.C3TR(C3)ï¼šç»§æ‰¿è‡ª C3ï¼Œn ä¸ª Bottleneck æ›´æ¢ä¸º 1 ä¸ª TransformerBlock ================'''
class C3TR(C3):

    # C3 module with TransformerBlock()
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)
        self.m = TransformerBlock(c_, c_, 4, n)
```

C3TR(C3)æ˜¯ç»§æ‰¿è‡ª C3ï¼Œå°†n ä¸ª Bottleneck æ›´æ¢ä¸º 1 ä¸ª TransformerBlock  
è¿™éƒ¨åˆ†æ˜¯æ ¹æ®ä¸Šé¢çš„C3ç»“æ„æ”¹ç¼–è€Œæ¥çš„ï¼Œå°†åŸå…ˆçš„Bottleneckæ›¿æ¢ä¸ºè°ƒç”¨TransformerBlockæ¨¡å—  
å‚æ•°ï¼š

 *  c1: æ•´ä¸ªC3çš„è¾“å…¥channel
 *  c2: æ•´ä¸ªC3çš„è¾“å‡ºchannel
 *  n: æœ‰nä¸ªå­æ¨¡å—\[Bottleneck/CrossConv\]
 *  shortcut:  boolå€¼ï¼Œå­æ¨¡å—\[Bottlenec/CrossConv\]ä¸­æ˜¯å¦æœ‰shortcutï¼Œé»˜è®¤True
 *  g:  g=1è¡¨ç¤ºä»è¾“å…¥é€šé“åˆ°è¾“å‡ºé€šé“çš„é˜»å¡è¿æ¥æ•°ä¸º1
 *  e:  expansion ratioï¼Œe\*c2=ä¸­é—´å…¶å®ƒæ‰€æœ‰å±‚çš„å·ç§¯æ ¸ä¸ªæ•°=ä¸­é—´æ‰€æœ‰å±‚çš„çš„è¾“å…¥è¾“å‡ºchannel

### 5.2 AutoShape 

```java
'''===========2.AutoShapeï¼šè‡ªåŠ¨è°ƒæ•´shape,è¯¥ç±»åŸºæœ¬æœªç”¨================'''
class AutoShape(nn.Module):
    # YOLOv5 input-robust model wrapper for passing cv2/np/PIL/torch inputs. Includes preprocessing, inference and NMS
    conf = 0.25  # NMS confidence threshold
    iou = 0.45  # NMS IoU threshold
    classes = None  # (optional list) filter by class, i.e. = [0, 15, 16] for COCO persons, cats and dogs
    multi_label = False  # NMS multiple labels per box
    max_det = 1000  # maximum number of detections per image

    def __init__(self, model):
        super().__init__()
        self.model = model.eval()

    def autoshape(self):
        LOGGER.info('AutoShape already enabled, skipping... ')  # model already converted to model.autoshape()
        return self

    def _apply(self, fn):
        # Apply to(), cpu(), cuda(), half() to model tensors that are not parameters or registered buffers
        self = super()._apply(fn)
        m = self.model.model[-1]  # Detect()
        m.stride = fn(m.stride)
        m.grid = list(map(fn, m.grid))
        if isinstance(m.anchor_grid, list):
            m.anchor_grid = list(map(fn, m.anchor_grid))
        return self

    @torch.no_grad()
    def forward(self, imgs, size=640, augment=False, profile=False):
        # Inference from various sources. For height=640, width=1280, RGB images example inputs are:
        #   file:       imgs = 'data/images/zidane.jpg'  # str or PosixPath
        #   URI:             = 'https://ultralytics.com/images/zidane.jpg'
        #   OpenCV:          = cv2.imread('image.jpg')[:,:,::-1]  # HWC BGR to RGB x(640,1280,3)
        #   PIL:             = Image.open('image.jpg') or ImageGrab.grab()  # HWC x(640,1280,3)
        #   numpy:           = np.zeros((640,1280,3))  # HWC
        #   torch:           = torch.zeros(16,3,320,640)  # BCHW (scaled to size=640, 0-1 values)
        #   multiple:        = [Image.open('image1.jpg'), Image.open('image2.jpg'), ...]  # list of images

        t = [time_sync()]
        p = next(self.model.parameters())  # for device and type
        if isinstance(imgs, torch.Tensor):  # torch
            with amp.autocast(enabled=p.device.type != 'cpu'):
                return self.model(imgs.to(p.device).type_as(p), augment, profile)  # inference

        # Pre-process
        n, imgs = (len(imgs), imgs) if isinstance(imgs, list) else (1, [imgs])  # number of images, list of images
        shape0, shape1, files = [], [], []  # image and inference shapes, filenames
        for i, im in enumerate(imgs):
            f = f'image{i}'  # filename
            if isinstance(im, (str, Path)):  # filename or uri
                im, f = Image.open(requests.get(im, stream=True).raw if str(im).startswith('http') else im), im
                im = np.asarray(exif_transpose(im))
            elif isinstance(im, Image.Image):  # PIL Image
                im, f = np.asarray(exif_transpose(im)), getattr(im, 'filename', f) or f
            files.append(Path(f).with_suffix('.jpg').name)
            if im.shape[0] < 5:  # image in CHW
                im = im.transpose((1, 2, 0))  # reverse dataloader .transpose(2, 0, 1)
            im = im[..., :3] if im.ndim == 3 else np.tile(im[..., None], 3)  # enforce 3ch input
            s = im.shape[:2]  # HWC
            shape0.append(s)  # image shape
            g = (size / max(s))  # gain
            shape1.append([y * g for y in s])
            imgs[i] = im if im.data.contiguous else np.ascontiguousarray(im)  # update
        shape1 = [make_divisible(x, int(self.stride.max())) for x in np.stack(shape1, 0).max(0)]  # inference shape
        x = [letterbox(im, new_shape=shape1, auto=False)[0] for im in imgs]  # pad
        x = np.stack(x, 0) if n > 1 else x[0][None]  # stack
        x = np.ascontiguousarray(x.transpose((0, 3, 1, 2)))  # BHWC to BCHW
        x = torch.from_numpy(x).to(p.device).type_as(p) / 255  # uint8 to fp16/32
        t.append(time_sync())

        with amp.autocast(enabled=p.device.type != 'cpu'):
            # Inference
            y = self.model(x, augment, profile)[0]  # forward
            t.append(time_sync())

            # Post-process
            y = non_max_suppression(y, self.conf, iou_thres=self.iou, classes=self.classes,
                                    multi_label=self.multi_label, max_det=self.max_det)  # NMS
            for i in range(n):
                scale_coords(shape1, y[i][:, :4], shape0[i])

            t.append(time_sync())
            return Detections(imgs, y, files, t, self.names, x.shape)
```

AutoShapeæ˜¯ä¸€ä¸ªæ¨¡å‹æ‰©å±•æ¨¡å—ï¼Œç»™æ¨¡å‹å°è£…æˆåŒ…å«å‰å¤„ç†ã€æ¨ç†ã€åå¤„ç†çš„æ¨¡å—(é¢„å¤„ç† + æ¨ç† + nms)ã€‚

æ³¨æ„Autoshapeæ¨¡å—åœ¨trainä¸­ä¸ä¼šè¢«è°ƒç”¨ï¼Œå½“æ¨¡å‹è®­ç»ƒç»“æŸåï¼Œä¼šé€šè¿‡è¿™ä¸ªæ¨¡å—å¯¹å›¾ç‰‡è¿›è¡Œé‡å¡‘ï¼Œæ¥æ–¹ä¾¿æ¨¡å‹çš„é¢„æµ‹ã€‚

å› ä¸ºè¿™ä¸ªæ¨¡å—åŸºæœ¬æ²¡å•¥ç”¨ï¼Œæ‰€ä»¥ä¸åšç»†è®²ã€‚

### 5.3 Detections 

```java
'''===========3.Detectionsï¼šå¯¹æ¨ç†ç»“æœè¿›è¡Œå¤„ç†================'''
class Detections:
    # YOLOv5 detections class for inference results
    """ç”¨åœ¨AutoShapeå‡½æ•°ç»“å°¾
    detections class for YOLOv5 inference results
    """
    def __init__(self, imgs, pred, files, times=None, names=None, shape=None):
        super().__init__()
        d = pred[0].device  # device
        gn = [torch.tensor([*(im.shape[i] for i in [1, 0, 1, 0]), 1, 1], device=d) for im in imgs]  # normalizations
        # imgsï¼šåŸå›¾
        self.imgs = imgs  # list of images as numpy arrays
        # predï¼šé¢„æµ‹å€¼(xyxy, conf, cls)
        self.pred = pred  # list of tensors pred[0] = (xyxy, conf, cls)
        # namesï¼š ç±»å
        self.names = names  # class names
        # filesï¼š å›¾åƒæ–‡ä»¶å
        self.files = files  # image filenames
        # xyxyï¼šå·¦ä¸Šè§’+å³ä¸‹è§’æ ¼å¼
        self.xyxy = pred  # xyxy pixels
        # xywhï¼šä¸­å¿ƒç‚¹+å®½é•¿æ ¼å¼
        self.xywh = [xyxy2xywh(x) for x in pred]  # xywh pixels
        # xyxynï¼šxyxyæ ‡å‡†åŒ–
        self.xyxyn = [x / g for x, g in zip(self.xyxy, gn)]  # xyxy normalized
        # xywhnï¼šxywhnæ ‡å‡†åŒ–
        self.xywhn = [x / g for x, g in zip(self.xywh, gn)]  # xywh normalized
        self.n = len(self.pred)  # number of images (batch size)
        self.t = tuple((times[i + 1] - times[i]) * 1000 / self.n for i in range(3))  # timestamps (ms)
        self.s = shape  # inference BCHW shape

    def display(self, pprint=False, show=False, save=False, crop=False, render=False, save_dir=Path('')):
        crops = []
        for i, (im, pred) in enumerate(zip(self.imgs, self.pred)):
            s = f'image {i + 1}/{len(self.pred)}: {im.shape[0]}x{im.shape[1]} '  # string
            if pred.shape[0]:
                for c in pred[:, -1].unique():
                    n = (pred[:, -1] == c).sum()  # detections per class
                    s += f"{n} {self.names[int(c)]}{'s' * (n > 1)}, "  # add to string
                if show or save or render or crop:
                    annotator = Annotator(im, example=str(self.names))
                    for *box, conf, cls in reversed(pred):  # xyxy, confidence, class
                        label = f'{self.names[int(cls)]} {conf:.2f}'
                        if crop:
                            file = save_dir / 'crops' / self.names[int(cls)] / self.files[i] if save else None
                            crops.append({'box': box, 'conf': conf, 'cls': cls, 'label': label,
                                          'im': save_one_box(box, im, file=file, save=save)})
                        else:  # all others
                            annotator.box_label(box, label, color=colors(cls))
                    im = annotator.im
            else:
                s += '(no detections)'

            im = Image.fromarray(im.astype(np.uint8)) if isinstance(im, np.ndarray) else im  # from np
            if pprint:
                LOGGER.info(s.rstrip(', '))
            if show:
                im.show(self.files[i])  # show
            if save:
                f = self.files[i]
                im.save(save_dir / f)  # save
                if i == self.n - 1:
                    LOGGER.info(f"Saved {self.n} image{'s' * (self.n > 1)} to {colorstr('bold', save_dir)}")
            if render:
                self.imgs[i] = np.asarray(im)
        if crop:
            if save:
                LOGGER.info(f'Saved results to {save_dir}\n')
            return crops

    def print(self):
        self.display(pprint=True)  # print results
        LOGGER.info(f'Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {tuple(self.s)}' %
                    self.t)

    def show(self):
        self.display(show=True)  # show results

    def save(self, save_dir='runs/detect/exp'):
        save_dir = increment_path(save_dir, exist_ok=save_dir != 'runs/detect/exp', mkdir=True)  # increment save_dir
        self.display(save=True, save_dir=save_dir)  # save results

    def crop(self, save=True, save_dir='runs/detect/exp'):
        save_dir = increment_path(save_dir, exist_ok=save_dir != 'runs/detect/exp', mkdir=True) if save else None
        return self.display(crop=True, save=save, save_dir=save_dir)  # crop results

    def render(self):
        self.display(render=True)  # render results
        return self.imgs

    def pandas(self):
        # return detections as pandas DataFrames, i.e. print(results.pandas().xyxy[0])
        new = copy(self)  # return copy
        ca = 'xmin', 'ymin', 'xmax', 'ymax', 'confidence', 'class', 'name'  # xyxy columns
        cb = 'xcenter', 'ycenter', 'width', 'height', 'confidence', 'class', 'name'  # xywh columns
        for k, c in zip(['xyxy', 'xyxyn', 'xywh', 'xywhn'], [ca, ca, cb, cb]):
            a = [[x[:5] + [int(x[5]), self.names[int(x[5])]] for x in x.tolist()] for x in getattr(self, k)]  # update
            setattr(new, k, [pd.DataFrame(x, columns=c) for x in a])
        return new

    def tolist(self):
        # return a list of Detections objects, i.e. 'for result in results.tolist():'
        x = [Detections([self.imgs[i]], [self.pred[i]], self.names, self.s) for i in range(self.n)]
        for d in x:
            for k in ['imgs', 'pred', 'xyxy', 'xyxyn', 'xywh', 'xywhn']:
                setattr(d, k, getattr(d, k)[0])  # pop out of list
        return x

    def __len__(self):
        return self.n
```

Detectionsæ˜¯ä¸“é—¨é’ˆå¯¹ç›®æ ‡æ£€æµ‹çš„å°è£…ç±»ï¼Œå¯¹æ¨ç†ç»“æœè¿›è¡Œå¤„ç†ã€‚

è¿™ä¸ªæ¨¡å—å§ï¼Œä»£ç soé•¿ã€‚æ˜¯å¯¹æ¨ç†ç»“æœè¿›è¡Œä¸€äº›å¤„ç†ï¼Œç”¨çš„ä¸æ˜¯å¾ˆå¤šï¼Œæ•´ä¸ªYOLOv5åªåœ¨ä¸Šé¢çš„AutoShapeå‡½æ•°ç»“å°¾è°ƒç”¨äº†ä¸€ä¸‹ã€‚ä¸ç”¨ä»”ç»†ç ”ç©¶çš„ï¼ŒæŠŠyolo.pyçš„Detectæ¨¡å—äº†è§£æ¸…æ¥šæ—¢å¯~

### 5.4 Classify 

```java
'''===========4.Classifyï¼šäºŒçº§åˆ†ç±»æ¨¡å—================'''
class Classify(nn.Module):
    # Classification head, i.e. x(b,c1,20,20) to x(b,c2)
    def __init__(self, c1, c2, k=1, s=1, p=None, g=1):  # ch_in, ch_out, kernel, stride, padding, groups

        super().__init__()
        self.aap = nn.AdaptiveAvgPool2d(1)  # to x(b,c1,1,1)
        # è‡ªé€‚åº”å¹³å‡æ± åŒ–æ“ä½œ
        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g)  # to x(b,c2,1,1)
        # å±•å¹³
        self.flat = nn.Flatten()

    def forward(self, x):
        # å…ˆè‡ªé€‚åº”å¹³å‡æ± åŒ–æ“ä½œï¼Œ ç„¶åæ‹¼æ¥
        z = torch.cat([self.aap(y) for y in (x if isinstance(x, list) else [x])], 1)  # cat if list
        # å¯¹zè¿›è¡Œå±•å¹³æ“ä½œ
        return self.flat(self.conv(z))  # flatten to x(b,c2)
```

Classifyæ˜¯ä¸€ä¸ªäºŒçº§åˆ†ç±»æ¨¡å—

> ä»€ä¹ˆæ˜¯äºŒçº§åˆ†ç±»æ¨¡å—?
> 
> æ¯”å¦‚åšè½¦ç‰Œçš„è¯†åˆ«ï¼Œå…ˆè¯†åˆ«å‡ºè½¦ç‰Œï¼Œå¦‚æœæƒ³å¯¹è½¦ç‰Œä¸Šçš„å­—è¿›è¡Œè¯†åˆ«ï¼Œå°±éœ€è¦äºŒçº§åˆ†ç±»è¿›ä¸€æ­¥æ£€æµ‹ã€‚
> 
> å†æ¯”å¦‚è¦åšè¯†åˆ«äººè„¸é¢éƒ¨è¡¨æƒ…ï¼Œå…ˆè¦è¯†åˆ«å‡ºäººè„¸ï¼Œå¦‚æœæƒ³è¯†åˆ«å‡ºäººçš„é¢éƒ¨è¡¨æƒ…ï¼Œå°±éœ€è¦äºŒçº§åˆ†ç±»è¿›ä¸€æ­¥æ£€æµ‹ã€‚

## ğŸš€å…­ã€common.pyå…¨éƒ¨æ³¨é‡Š 

```java
# YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license
"""
Common modules
"""
'''===============================================ä¸€ã€å¯¼å…¥åŒ…==================================================='''
'''======================1.å¯¼å…¥å®‰è£…å¥½çš„pythonåº“====================='''
import json  # ç”¨äºjsonå’ŒPythonæ•°æ®ä¹‹é—´çš„ç›¸äº’è½¬æ¢
import math  # æ•°å­¦å‡½æ•°æ¨¡å—
import platform  # è·å–æ“ä½œç³»ç»Ÿçš„ä¿¡æ¯
import warnings  # è­¦å‘Šç¨‹åºå‘˜å…³äºè¯­è¨€æˆ–åº“åŠŸèƒ½çš„å˜åŒ–çš„æ–¹æ³•
from copy import copy  # æ•°æ®æ‹·è´æ¨¡å— åˆ†æµ…æ‹·è´å’Œæ·±æ‹·è´
from pathlib import Path  # Pathå°†strè½¬æ¢ä¸ºPathå¯¹è±¡ ä½¿å­—ç¬¦ä¸²è·¯å¾„æ˜“äºæ“ä½œçš„æ¨¡å—

import cv2  # è°ƒç”¨OpenCVçš„cvåº“
import numpy as np  # numpyæ•°ç»„æ“ä½œæ¨¡å—
import pandas as pd  # pandaæ•°ç»„æ“ä½œæ¨¡å—
import requests  # Pythonçš„HTTPå®¢æˆ·ç«¯åº“
import torch  # pytorchæ·±åº¦å­¦ä¹ æ¡†æ¶
import torch.nn as nn  # ä¸“é—¨ä¸ºç¥ç»ç½‘ç»œè®¾è®¡çš„æ¨¡å—åŒ–æ¥å£
from PIL import Image  # å›¾åƒåŸºç¡€æ“ä½œæ¨¡å—
from torch.cuda import amp  # æ··åˆç²¾åº¦è®­ç»ƒæ¨¡å—

'''===================2.åŠ è½½è‡ªå®šä¹‰æ¨¡å—============================'''
from utils.datasets import exif_transpose, letterbox  # åŠ è½½æ•°æ®é›†çš„å‡½æ•°
from utils.general import (LOGGER, check_requirements, check_suffix, colorstr, increment_path, make_divisible,
                           non_max_suppression, scale_coords, xywh2xyxy, xyxy2xywh)  # å®šä¹‰äº†ä¸€äº›å¸¸ç”¨çš„å·¥å…·å‡½æ•°
from utils.plots import Annotator, colors, plot_one_box  # å®šä¹‰äº†Annotatorç±»ï¼Œå¯ä»¥åœ¨å›¾åƒä¸Šç»˜åˆ¶çŸ©å½¢æ¡†å’Œæ ‡æ³¨ä¿¡æ¯
from utils.torch_utils import time_sync  # å®šä¹‰äº†ä¸€äº›ä¸PyTorchæœ‰å…³çš„å·¥å…·å‡½æ•°

'''===============================================äºŒã€åŸºç¡€ç»„ä»¶==================================================='''
'''===========1.autopadï¼šæ ¹æ®è¾“å…¥çš„å·ç§¯æ ¸è®¡ç®—è¯¥å·ç§¯æ¨¡å—æ‰€éœ€çš„padå€¼================'''
# ä¸ºsameå·ç§¯æˆ–è€…sameæ± åŒ–è‡ªåŠ¨æ‰©å……
# é€šè¿‡å·ç§¯æ ¸çš„å¤§å°æ¥è®¡ç®—éœ€è¦çš„paddingä¸ºå¤šå°‘æ‰èƒ½æŠŠtensorè¡¥æˆåŸæ¥çš„å½¢çŠ¶
def autopad(k, p=None):  # kernel, padding
    # Pad to 'same'
    # å¦‚æœpæ˜¯none åˆ™è¿›è¡Œä¸‹ä¸€æ­¥
    if p is None:
        # å¦‚æœkæ˜¯int åˆ™è¿›è¡Œk//2 è‹¥ä¸æ˜¯åˆ™è¿›è¡Œx//2
        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad
    return p

'''===========2.Convï¼šæ ‡å‡†å·ç§¯ ç”±Conv + BN + activateç»„æˆ================'''
class Conv(nn.Module):
    # Standard convolution
    # initåˆå§‹åŒ–æ„é€ å‡½æ•°
    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups
        """åœ¨Focusã€Bottleneckã€BottleneckCSPã€C3ã€SPPã€DWConvã€TransformerBlocç­‰æ¨¡å—ä¸­è°ƒç”¨
                Standard convolution  conv+BN+act
                :params c1: è¾“å…¥çš„channelå€¼
                :params c2: è¾“å‡ºçš„channelå€¼
                :params k: å·ç§¯çš„kernel_size
                :params s: å·ç§¯çš„stride
                :params p: å·ç§¯çš„padding  ä¸€èˆ¬æ˜¯None  å¯ä»¥é€šè¿‡autopadè‡ªè¡Œè®¡ç®—éœ€è¦padçš„paddingæ•°
                :params g: å·ç§¯çš„groupsæ•°  =1å°±æ˜¯æ™®é€šçš„å·ç§¯  >1å°±æ˜¯æ·±åº¦å¯åˆ†ç¦»å·ç§¯
                :params act: æ¿€æ´»å‡½æ•°ç±»å‹   Trueå°±æ˜¯SiLU()/Swish   Falseå°±æ˜¯ä¸ä½¿ç”¨æ¿€æ´»å‡½æ•°
                             ç±»å‹æ˜¯nn.Moduleå°±ä½¿ç”¨ä¼ è¿›æ¥çš„æ¿€æ´»å‡½æ•°ç±»å‹
        """
        super().__init__()
        # å·ç§¯å±‚
        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False)
        # å½’ä¸€åŒ–å±‚
        self.bn = nn.BatchNorm2d(c2)
        # æ¿€æ´»å‡½æ•°
        self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity())

    # æ­£å‘è®¡ç®—ï¼Œç½‘ç»œæ‰§è¡Œçš„é¡ºåºæ˜¯æ ¹æ®forwardå‡½æ•°æ¥å†³å®šçš„
    def forward(self, x):
        # convå·ç§¯ -> bn -> actæ¿€æ´»
        return self.act(self.bn(self.conv(x)))

    # æ­£å‘èåˆè®¡ç®—
    def forward_fuse(self, x):
        # è¿™é‡Œåªæœ‰å·ç§¯å’Œæ¿€æ´»
        return self.act(self.conv(x))

'''===========3.DWConvï¼šæ·±åº¦å¯åˆ†ç¦»å·ç§¯================'''
class DWConv(Conv):
    # Depth-wise convolution class
    def __init__(self, c1, c2, k=1, s=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups
        super().__init__(c1, c2, k, s, g=math.gcd(c1, c2), act=act)

'''===========4.Bottleneckï¼šæ ‡å‡†çš„ç“¶é¢ˆå±‚ ç”±1x1conv+3x3conv+æ®‹å·®å—ç»„æˆ================'''
class Bottleneck(nn.Module):
    # Standard bottleneck
    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, shortcut, groups, expansion
        """åœ¨BottleneckCSPå’Œyolo.pyçš„parse_modelä¸­è°ƒç”¨
          Standard bottleneck  Conv+Conv+shortcut
          :params c1: ç¬¬ä¸€ä¸ªå·ç§¯çš„è¾“å…¥channel
          :params c2: ç¬¬äºŒä¸ªå·ç§¯çš„è¾“å‡ºchannel
          :params shortcut: bool æ˜¯å¦æœ‰shortcutè¿æ¥ é»˜è®¤æ˜¯True
          :params g: å·ç§¯åˆ†ç»„çš„ä¸ªæ•°  =1å°±æ˜¯æ™®é€šå·ç§¯  >1å°±æ˜¯æ·±åº¦å¯åˆ†ç¦»å·ç§¯
          :params e: expansion ratio  e*c2å°±æ˜¯ç¬¬ä¸€ä¸ªå·ç§¯çš„è¾“å‡ºchannel=ç¬¬äºŒä¸ªå·ç§¯çš„è¾“å…¥channel
          """
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        # 1*1å·ç§¯å±‚
        self.cv1 = Conv(c1, c_, 1, 1)
        # 3*3å·ç§¯å±‚
        self.cv2 = Conv(c_, c2, 3, 1, g=g)
        # å¦‚æœshortcutä¸ºTrueå°±ä¼šå°†è¾“å…¥å’Œè¾“å‡ºç›¸åŠ ä¹‹åå†è¾“å‡º
        self.add = shortcut and c1 == c2

    def forward(self, x):
        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))

'''===========5.BottleneckCSPï¼šç“¶é¢ˆå±‚ ç”±å‡ ä¸ªBottleneckæ¨¡å—çš„å †å +CSPç»“æ„ç»„æˆ================'''
class BottleneckCSP(nn.Module):
    # CSP Bottleneck https://github.com/WongKinYiu/CrossStagePartialNetworks
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion
        """åœ¨C3æ¨¡å—å’Œyolo.pyçš„parse_modelæ¨¡å—è°ƒç”¨
            CSP Bottleneck https://github.com/WongKinYiu/CrossStagePartialNetworks
            :params c1: æ•´ä¸ªBottleneckCSPçš„è¾“å…¥channel
            :params c2: æ•´ä¸ªBottleneckCSPçš„è¾“å‡ºchannel
            :params n: æœ‰nä¸ªBottleneck
            :params shortcut: bool Bottleneckä¸­æ˜¯å¦æœ‰shortcutï¼Œé»˜è®¤True
            :params g: Bottleneckä¸­çš„3x3å·ç§¯ç±»å‹  =1æ™®é€šå·ç§¯  >1æ·±åº¦å¯åˆ†ç¦»å·ç§¯
            :params e: expansion ratio c2xe=ä¸­é—´å…¶ä»–æ‰€æœ‰å±‚çš„å·ç§¯æ ¸ä¸ªæ•°/ä¸­é—´æ‰€æœ‰å±‚çš„è¾“å…¥è¾“å‡ºchannelæ•°
            c_: bottleneckCSP ç»“æ„çš„ä¸­é—´å±‚çš„é€šé“æ•°ï¼Œç”±è†¨èƒ€ç‡eå†³å®š
            """
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        # 4ä¸ª1*1å·ç§¯å±‚çš„å †å 
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = nn.Conv2d(c1, c_, 1, 1, bias=False)
        self.cv3 = nn.Conv2d(c_, c_, 1, 1, bias=False)
        self.cv4 = Conv(2 * c_, c2, 1, 1)
        # bnå±‚
        self.bn = nn.BatchNorm2d(2 * c_)  # applied to cat(cv2, cv3)
        # æ¿€æ´»å‡½æ•°
        self.act = nn.SiLU()
        # mï¼šå åŠ næ¬¡Bottleneckçš„æ“ä½œ
        # æ“ä½œç¬¦*å¯ä»¥æŠŠä¸€ä¸ªlistæ‹†å¼€æˆä¸€ä¸ªä¸ªç‹¬ç«‹çš„å…ƒç´ 
        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))

    def forward(self, x):
        # y1ç›¸å½“äºå…ˆåšä¸€æ¬¡cv1æ“ä½œç„¶åè¿›è¡Œmæ“ä½œæœ€åè¿›è¡Œcv3æ“ä½œï¼Œä¹Ÿå°±æ˜¯BCSPnæ¨¡å—ä¸­çš„ä¸Šé¢çš„åˆ†æ”¯æ“ä½œ
        # è¾“å…¥x ->Convæ¨¡å— ->nä¸ªbottleneckæ¨¡å— ->Convæ¨¡å— ->y1
        y1 = self.cv3(self.m(self.cv1(x)))
        # y2å°±æ˜¯è¿›è¡Œcv2æ“ä½œï¼Œä¹Ÿå°±æ˜¯BCSPnæ¨¡å—ä¸­çš„ä¸‹é¢çš„åˆ†æ”¯æ“ä½œï¼ˆç›´æ¥é€†è¡Œconvæ“ä½œçš„åˆ†æ”¯ï¼Œ Conv--nXBottleneck--convï¼‰
        # è¾“å…¥x -> Convæ¨¡å— -> è¾“å‡ºy2
        y2 = self.cv2(x)
        # æœ€åy1å’Œy2åšæ‹¼æ¥ï¼Œ æ¥ç€è¿›å…¥bnå±‚åšå½’ä¸€åŒ–ï¼Œ ç„¶ååšactæ¿€æ´»ï¼Œ æœ€åè¾“å‡ºcv4
        # è¾“å…¥y1,y2->æŒ‰ç…§é€šé“æ•°èåˆ ->å½’ä¸€åŒ– -> æ¿€æ´»å‡½æ•° -> Convè¾“å‡º -> è¾“å‡º
        # torch.cat(y1, y2), dim=1: è¿™é‡Œæ˜¯æŒ‡å®šåœ¨ç¬¬ä¸€ä¸ªç»´åº¦ä¸Šè¿›è¡Œåˆå¹¶ï¼Œå³åœ¨channelç»´åº¦ä¸Šåˆå¹¶
        return self.cv4(self.act(self.bn(torch.cat((y1, y2), dim=1))))

'''===========6.C3ï¼šå’ŒBottleneckCSPæ¨¡å—ç±»ä¼¼ï¼Œä½†æ˜¯å°‘äº†ä¸€ä¸ªConvæ¨¡å—================'''
# ===6.1 C3=== #
class C3(nn.Module):
    # CSP Bottleneck with 3 convolutions
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion
        """åœ¨C3TRæ¨¡å—å’Œyolo.pyçš„parse_modelæ¨¡å—è°ƒç”¨
         CSP Bottleneck with 3 convolutions
         :params c1: æ•´ä¸ªBottleneckCSPçš„è¾“å…¥channel
         :params c2: æ•´ä¸ªBottleneckCSPçš„è¾“å‡ºchannel
         :params n: æœ‰nä¸ªBottleneck
         :params shortcut: bool Bottleneckä¸­æ˜¯å¦æœ‰shortcutï¼Œé»˜è®¤True
         :params g: Bottleneckä¸­çš„3x3å·ç§¯ç±»å‹  =1æ™®é€šå·ç§¯  >1æ·±åº¦å¯åˆ†ç¦»å·ç§¯
         :params e: expansion ratio c2xe=ä¸­é—´å…¶ä»–æ‰€æœ‰å±‚çš„å·ç§¯æ ¸ä¸ªæ•°/ä¸­é—´æ‰€æœ‰å±‚çš„è¾“å…¥è¾“å‡ºchannelæ•°
         """
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        # 3ä¸ª1*1å·ç§¯å±‚çš„å †å ï¼Œæ¯”BottleneckCSPå°‘ä¸€ä¸ª
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c1, c_, 1, 1)
        self.cv3 = Conv(2 * c_, c2, 1)  # act=FReLU(c2)
        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))
        # self.m = nn.Sequential(*[CrossConv(c_, c_, 3, 1, g, 1.0, shortcut) for _ in range(n)])

    def forward(self, x):
        # å°†ç¬¬ä¸€ä¸ªå·ç§¯å±‚ä¸ç¬¬äºŒä¸ªå·ç§¯å±‚çš„ç»“æœæ‹¼æ¥åœ¨ä¸€èµ·
        return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), dim=1))

# ===6.2 C3SPP(C3)ï¼šç»§æ‰¿è‡ª C3ï¼Œn ä¸ª Bottleneck æ›´æ¢ä¸º 1 ä¸ª SPP=== #
class C3SPP(C3):
    # C3 module with SPP()
    def __init__(self, c1, c2, k=(5, 9, 13), n=1, shortcut=True, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)
        self.m = SPP(c_, c_, k)

# ===6.3 C3Ghost(C3)ï¼šç»§æ‰¿è‡ª C3ï¼ŒBottleneck æ›´æ¢ä¸º GhostBottleneck=== #
class C3Ghost(C3):
    # C3 module with GhostBottleneck()
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(*(GhostBottleneck(c_, c_) for _ in range(n)))

'''===========7.SPPï¼šç©ºé—´é‡‘å­—å¡”æ± åŒ–æ¨¡å—================'''
# ç”¨åœ¨éª¨å¹²ç½‘ç»œæ”¶å°¾é˜¶æ®µï¼Œç”¨äºèåˆå¤šå°ºåº¦ç‰¹å¾ã€‚
# ===7.1 SPPï¼šç©ºé—´é‡‘å­—å¡”æ± åŒ–=== #
class SPP(nn.Module):
    # Spatial Pyramid Pooling (SPP) layer https://arxiv.org/abs/1406.4729
    def __init__(self, c1, c2, k=(5, 9, 13)):
        """åœ¨yolo.pyçš„parse_modelæ¨¡å—è°ƒç”¨
               ç©ºé—´é‡‘å­—å¡”æ± åŒ– Spatial pyramid pooling layer used in YOLOv3-SPP
               :params c1: SPPæ¨¡å—çš„è¾“å…¥channel
               :params c2: SPPæ¨¡å—çš„è¾“å‡ºchannel
               :params k: ä¿å­˜ç€ä¸‰ä¸ªmaxpoolçš„å·ç§¯æ ¸å¤§å° é»˜è®¤æ˜¯(5, 9, 13)
               """
        super().__init__()
        c_ = c1 // 2  # hidden channels
        # 1*1å·ç§¯
        self.cv1 = Conv(c1, c_, 1, 1)
        #  è¿™é‡Œ+1æ˜¯å› ä¸ºæœ‰len(k)+1ä¸ªè¾“å…¥
        self.cv2 = Conv(c_ * (len(k) + 1), c2, 1, 1)
        # må…ˆè¿›è¡Œæœ€å¤§æ± åŒ–æ“ä½œï¼Œ ç„¶åé€šè¿‡nn.ModuleListè¿›è¡Œæ„é€ ä¸€ä¸ªæ¨¡å— åœ¨æ„é€ æ—¶å¯¹æ¯ä¸€ä¸ªkéƒ½è¦è¿›è¡Œæœ€å¤§æ± åŒ–
        self.m = nn.ModuleList([nn.MaxPool2d(kernel_size=x, stride=1, padding=x // 2) for x in k])

    def forward(self, x):
        # å…ˆè¿›è¡Œcv1çš„æ“ä½œ
        x = self.cv1(x)
        # å¿½ç•¥äº†è­¦å‘Šé”™è¯¯çš„è¾“å‡º
        with warnings.catch_warnings():
            warnings.simplefilter('ignore')  # suppress torch 1.9.0 max_pool2d() warning
            # å¯¹æ¯ä¸€ä¸ªmè¿›è¡Œæœ€å¤§æ± åŒ– å’Œæ²¡æœ‰åšæ± åŒ–çš„æ¯ä¸€ä¸ªè¾“å…¥è¿›è¡Œå åŠ   ç„¶ååšæ‹¼æ¥ æœ€ååšcv2æ“ä½œ
            return self.cv2(torch.cat([x] + [m(x) for m in self.m], 1))

# ===7.2 SPPFï¼šå¿«é€Ÿç‰ˆçš„ç©ºé—´é‡‘å­—å¡”æ± åŒ–=== #
class SPPF(nn.Module):
    # Spatial Pyramid Pooling - Fast (SPPF) layer for YOLOv5 by Glenn Jocher
    def __init__(self, c1, c2, k=5):  # equivalent to SPP(k=(5, 9, 13))
        super().__init__()
        c_ = c1 // 2  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c_ * 4, c2, 1, 1)
        self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)

    def forward(self, x):
        x = self.cv1(x)
        with warnings.catch_warnings():
            warnings.simplefilter('ignore')  # suppress torch 1.9.0 max_pool2d() warning
            y1 = self.m(x)
            y2 = self.m(y1)
            return self.cv2(torch.cat([x, y1, y2, self.m(y2)], 1))


'''===========8.Focusï¼šæŠŠå®½åº¦wå’Œé«˜åº¦hçš„ä¿¡æ¯æ•´åˆåˆ°cç©ºé—´================'''
class Focus(nn.Module):
    # Focus wh information into c-space
    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups
        """åœ¨yolo.pyçš„parse_modelå‡½æ•°ä¸­è¢«è°ƒç”¨
                ç†è®ºï¼šä»é«˜åˆ†è¾¨ç‡å›¾åƒä¸­ï¼Œå‘¨æœŸæ€§çš„æŠ½å‡ºåƒç´ ç‚¹é‡æ„åˆ°ä½åˆ†è¾¨ç‡å›¾åƒä¸­ï¼Œå³å°†å›¾åƒç›¸é‚»çš„å››ä¸ªä½ç½®è¿›è¡Œå †å ï¼Œ
                    èšç„¦whç»´åº¦ä¿¡æ¯åˆ°cé€šé“ç©ºï¼Œæé«˜æ¯ä¸ªç‚¹æ„Ÿå—é‡ï¼Œå¹¶å‡å°‘åŸå§‹ä¿¡æ¯çš„ä¸¢å¤±ï¼Œè¯¥æ¨¡å—çš„è®¾è®¡ä¸»è¦æ˜¯å‡å°‘è®¡ç®—é‡åŠ å¿«é€Ÿåº¦ã€‚
                Focus wh information into c-space æŠŠå®½åº¦wå’Œé«˜åº¦hçš„ä¿¡æ¯æ•´åˆåˆ°cç©ºé—´ä¸­
                å…ˆåš4ä¸ªslice å†concat æœ€åå†åšConv
                sliceå (b,c1,w,h) -> åˆ†æˆ4ä¸ªslice æ¯ä¸ªslice(b,c1,w/2,h/2)
                concat(dim=1)å 4ä¸ªslice(b,c1,w/2,h/2)) -> (b,4c1,w/2,h/2)
                convå (b,4c1,w/2,h/2) -> (b,c2,w/2,h/2)
                :params c1: sliceåçš„channel
                :params c2: Focusæœ€ç»ˆè¾“å‡ºçš„channel
                :params k: æœ€åå·ç§¯çš„kernel
                :params s: æœ€åå·ç§¯çš„stride
                :params p: æœ€åå·ç§¯çš„padding
                :params g: æœ€åå·ç§¯çš„åˆ†ç»„æƒ…å†µ  =1æ™®é€šå·ç§¯  >1æ·±åº¦å¯åˆ†ç¦»å·ç§¯
                :params act: boolæ¿€æ´»å‡½æ•°ç±»å‹  é»˜è®¤True:SiLU()/Swish  False:ä¸ç”¨æ¿€æ´»å‡½æ•°
                """
        super().__init__()
        # concatåçš„å·ç§¯ï¼ˆæœ€åçš„å·ç§¯ï¼‰
        self.conv = Conv(c1 * 4, c2, k, s, p, g, act)
        # self.contract = Contract(gain=2)

    def forward(self, x):  # x(b,c,w,h) -> y(b,4c,w/2,h/2)
        # å…ˆè¿›è¡Œåˆ‡åˆ†ï¼Œ ç„¶åè¿›è¡Œæ‹¼æ¥ï¼Œ æœ€åå†åšconvæ“ä½œ
        return self.conv(torch.cat([x[..., ::2, ::2], x[..., 1::2, ::2], x[..., ::2, 1::2], x[..., 1::2, 1::2]], 1))
        # return self.conv(self.contract(x))

    # ä»¥ä¸‹æ¨¡å—Contractï¼ŒExpand,Concatæ˜¯ç”¨æ¥å¤„ç†è¾“å…¥ç‰¹å¾çš„shapeçš„
'''===========9.Contractï¼šæ”¶ç¼©æ¨¡å—ï¼šè°ƒæ•´å¼ é‡çš„å¤§å°ï¼Œå°†å®½é«˜æ”¶ç¼©åˆ°é€šé“ä¸­ã€‚================'''
class Contract(nn.Module):
    # Contract width-height into channels, i.e. x(1,64,80,80) to x(1,256,40,40)
    """ç”¨åœ¨yolo.pyçš„parse_modelæ¨¡å— ç”¨çš„ä¸å¤š
    æ”¹å˜è¾“å…¥ç‰¹å¾çš„shape å°†wå’Œhç»´åº¦(ç¼©å°)çš„æ•°æ®æ”¶ç¼©åˆ°channelç»´åº¦ä¸Š(æ”¾å¤§)
    Contract width-height into channels, i.e. x(1,64,80,80) to x(1,256,40,40)
    """

    def __init__(self, gain=2):
        super().__init__()
        self.gain = gain

    def forward(self, x):
        b, c, h, w = x.size()  # assert (h / s == 0) and (W / s == 0), 'Indivisible gain'
        s = self.gain
        # permute: æ”¹å˜tensorçš„ç»´åº¦é¡ºåº
        x = x.view(b, c, h // s, s, w // s, s)  # x(1,64,40,2,40,2)
        # .view: æ”¹å˜tensorçš„ç»´åº¦
        x = x.permute(0, 3, 5, 1, 2, 4).contiguous()  # x(1,2,2,64,40,40)
        return x.view(b, c * s * s, h // s, w // s)  # x(1,256,40,40)

'''===========10.Expandï¼šæ‰©å¼ æ¨¡å—ï¼Œå°†ç‰¹å¾å›¾åƒç´ å˜å¤§================'''
class Expand(nn.Module):
    # Expand channels into width-height, i.e. x(1,64,80,80) to x(1,16,160,160)
    def __init__(self, gain=2):
        super().__init__()
        self.gain = gain

    def forward(self, x):
        b, c, h, w = x.size()  # assert C / s ** 2 == 0, 'Indivisible gain'
        s = self.gain
        x = x.view(b, s, s, c // s ** 2, h, w)  # x(1,2,2,16,80,80)
        x = x.permute(0, 3, 4, 1, 5, 2).contiguous()  # x(1,16,80,2,80,2)
        return x.view(b, c // s ** 2, h * s, w * s)  # x(1,16,160,160)

'''===========11.Concatï¼šè‡ªå®šä¹‰concatæ¨¡å—ï¼Œdimensionå°±æ˜¯ç»´åº¦å€¼ï¼Œè¯´æ˜æ²¿ç€å“ªä¸€ä¸ªç»´åº¦è¿›è¡Œæ‹¼æ¥================'''
# ä½œæ‹¼æ¥çš„ä¸€ä¸ªç±»
# æ‹¼æ¥å‡½æ•°ï¼Œå°†ä¸¤ä¸ªtensorè¿›è¡Œæ‹¼æ¥
class Concat(nn.Module):
    # Concatenate a list of tensors along dimension
    def __init__(self, dimension=1):
        super().__init__()
        self.d = dimension

    def forward(self, x):
        return torch.cat(x, self.d)

'''===============================================ä¸‰ã€æ³¨æ„åŠ›æ¨¡å—==================================================='''
'''===========1.TransformerLayerï¼š================'''
class TransformerLayer(nn.Module):
    # Transformer layer https://arxiv.org/abs/2010.11929 (LayerNorm layers removed for better performance)
    """
        Transformer layer https://arxiv.org/abs/2010.11929 (LayerNorm layers removed for better performance)

        è¿™éƒ¨åˆ†ç›¸å½“äºåŸè®ºæ–‡ä¸­çš„å•ä¸ªEncoderéƒ¨åˆ†(åªç§»é™¤äº†ä¸¤ä¸ªNorméƒ¨åˆ†, å…¶ä»–ç»“æ„å’ŒåŸæ–‡ä¸­çš„Encodingä¸€æ¨¡ä¸€æ ·)
       """
    def __init__(self, c, num_heads):
        super().__init__()
        self.q = nn.Linear(c, c, bias=False)
        self.k = nn.Linear(c, c, bias=False)
        self.v = nn.Linear(c, c, bias=False)
        # è¾“å…¥: queryã€keyã€value
        # è¾“å‡º: 0 attn_output å³é€šè¿‡self-attentionä¹‹åï¼Œä»æ¯ä¸€ä¸ªè¯è¯­ä½ç½®è¾“å‡ºæ¥çš„attention å’Œè¾“å…¥çš„queryå®ƒä»¬å½¢çŠ¶ä¸€æ ·çš„
        #      1 attn_output_weights å³attention weights æ¯ä¸€ä¸ªå•è¯å’Œä»»æ„å¦ä¸€ä¸ªå•è¯ä¹‹é—´éƒ½ä¼šäº§ç”Ÿä¸€ä¸ªweight
        self.ma = nn.MultiheadAttention(embed_dim=c, num_heads=num_heads)
        self.fc1 = nn.Linear(c, c, bias=False)
        self.fc2 = nn.Linear(c, c, bias=False)

    def forward(self, x):
        # å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ + æ®‹å·®(è¿™é‡Œç§»é™¤äº†LayerNorm for better performance)
        x = self.ma(self.q(x), self.k(x), self.v(x))[0] + x
        # feed forward å‰é¦ˆç¥ç»ç½‘ç»œ + æ®‹å·®(è¿™é‡Œç§»é™¤äº†LayerNorm for better performance)
        x = self.fc2(self.fc1(x)) + x
        return x

'''===========2.TransformerBlockï¼š================'''
class TransformerBlock(nn.Module):
    # Vision Transformer https://arxiv.org/abs/2010.11929
    def __init__(self, c1, c2, num_heads, num_layers):
        super().__init__()
        self.conv = None
        if c1 != c2:
            self.conv = Conv(c1, c2)
        self.linear = nn.Linear(c2, c2)  # learnable position embedding
        self.tr = nn.Sequential(*(TransformerLayer(c2, num_heads) for _ in range(num_layers)))
        self.c2 = c2

    def forward(self, x):
        if self.conv is not None:
            x = self.conv(x)
        b, _, w, h = x.shape
        p = x.flatten(2).permute(2, 0, 1)
        return self.tr(p + self.linear(p)).permute(1, 2, 0).reshape(b, self.c2, w, h)

'''===============================================å››ã€å¹»è±¡æ¨¡å—==================================================='''
'''===========1.GhostConvï¼šå¹»è±¡å·ç§¯  è½»é‡åŒ–ç½‘ç»œå·ç§¯æ¨¡å—================'''
class GhostConv(nn.Module):
    # Ghost Convolution https://github.com/huawei-noah/ghostnet

    def __init__(self, c1, c2, k=1, s=1, g=1, act=True):  # ch_in, ch_out, kernel, stride, groups
        super().__init__()
        c_ = c2 // 2  # hidden channels
        # ç¬¬ä¸€æ­¥å·ç§¯: å°‘é‡å·ç§¯, ä¸€èˆ¬æ˜¯ä¸€åŠçš„è®¡ç®—é‡
        self.cv1 = Conv(c1, c_, k, s, None, g, act)
        # ç¬¬äºŒæ­¥å·ç§¯: cheap operations ä½¿ç”¨3x3æˆ–5x5çš„å·ç§¯, å¹¶ä¸”æ˜¯é€ä¸ªç‰¹å¾å›¾çš„è¿›è¡Œå·ç§¯ï¼ˆDepth-wise convolutional
        self.cv2 = Conv(c_, c_, 5, 1, None, c_, act)

    def forward(self, x):
        y = self.cv1(x)
        return torch.cat([y, self.cv2(y)], 1)

'''===========2.GhostBottleneckï¼šå¹»è±¡ç“¶é¢ˆå±‚ ================'''
class GhostBottleneck(nn.Module):
    # Ghost Bottleneck https://github.com/huawei-noah/ghostnet
    def __init__(self, c1, c2, k=3, s=1):  # ch_in, ch_out, kernel, stride
        super().__init__()
        c_ = c2 // 2
        self.conv = nn.Sequential(GhostConv(c1, c_, 1, 1),  # pw
                                  DWConv(c_, c_, k, s, act=False) if s == 2 else nn.Identity(),  # dw
                                  GhostConv(c_, c2, 1, 1, act=False))  # pw-linear
        # æ³¨æ„, æºç ä¸­å¹¶ä¸æ˜¯ç›´æ¥Identityè¿æ¥, è€Œæ˜¯å…ˆç»è¿‡ä¸€ä¸ªDWConv + Conv, å†è¿›è¡Œshortcutè¿æ¥çš„ã€‚
        self.shortcut = nn.Sequential(DWConv(c1, c1, k, s, act=False),
                                      Conv(c1, c2, 1, 1, act=False)) if s == 2 else nn.Identity()

    def forward(self, x):
        return self.conv(x) + self.shortcut(x)

'''===============================================äº”ã€æ¨¡å‹æ‰©å±•æ¨¡å—==================================================='''
'''===========1.C3TR(C3)ï¼šç»§æ‰¿è‡ª C3ï¼Œn ä¸ª Bottleneck æ›´æ¢ä¸º 1 ä¸ª TransformerBlock ================'''
class C3TR(C3):
    """
        è¿™éƒ¨åˆ†æ˜¯æ ¹æ®ä¸Šé¢çš„C3ç»“æ„æ”¹ç¼–è€Œæ¥çš„, å°†åŸå…ˆçš„Bottleneckæ›¿æ¢ä¸ºè°ƒç”¨TransformerBlockæ¨¡å—
        """
    # C3 module with TransformerBlock()
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        ''' åœ¨C3RTæ¨¡å—å’Œyolo.pyçš„parse_modelå‡½æ•°ä¸­è¢«è°ƒç”¨
                :params c1: æ•´ä¸ªC3çš„è¾“å…¥channel
                :params c2: æ•´ä¸ªC3çš„è¾“å‡ºchannel
                :params n: æœ‰nä¸ªå­æ¨¡å—[Bottleneck/CrossConv]
                :params shortcut: boolå€¼ï¼Œå­æ¨¡å—[Bottlenec/CrossConv]ä¸­æ˜¯å¦æœ‰shortcutï¼Œé»˜è®¤True
                :params g: å­æ¨¡å—[Bottlenec/CrossConv]ä¸­çš„3x3å·ç§¯ç±»å‹ï¼Œ=1æ™®é€šå·ç§¯ï¼Œ>1æ·±åº¦å¯åˆ†ç¦»å·ç§¯
                :params e: expansion ratioï¼Œe*c2=ä¸­é—´å…¶å®ƒæ‰€æœ‰å±‚çš„å·ç§¯æ ¸ä¸ªæ•°=ä¸­é—´æ‰€æœ‰å±‚çš„çš„è¾“å…¥è¾“å‡ºchannel
                '''
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)
        self.m = TransformerBlock(c_, c_, 4, n)

'''===========2.DetectMultiBackendï¼š ================'''
class DetectMultiBackend(nn.Module):
    # YOLOv5 MultiBackend class for python inference on various backends
    def __init__(self, weights='yolov5s.pt', device=None, dnn=True):
        # Usage:
        #   PyTorch:      weights = *.pt
        #   TorchScript:            *.torchscript.pt
        #   CoreML:                 *.mlmodel
        #   TensorFlow:             *_saved_model
        #   TensorFlow:             *.pb
        #   TensorFlow Lite:        *.tflite
        #   ONNX Runtime:           *.onnx
        #   OpenCV DNN:             *.onnx with dnn=True
        super().__init__()
        # åˆ¤æ–­weightsæ˜¯å¦ä¸ºlistï¼Œè‹¥æ˜¯å–å‡ºç¬¬ä¸€ä¸ªå€¼ä½œä¸ºä¼ å…¥è·¯å¾„
        w = str(weights[0] if isinstance(weights, list) else weights)
        suffix, suffixes = Path(w).suffix.lower(), ['.pt', '.onnx', '.tflite', '.pb', '', '.mlmodel']
        check_suffix(w, suffixes)  # check weights have acceptable suffix
        pt, onnx, tflite, pb, saved_model, coreml = (suffix == x for x in suffixes)  # backend booleans
        jit = pt and 'torchscript' in w.lower()
        stride, names = 64, [f'class{i}' for i in range(1000)]  # assign defaults

        if jit:  # TorchScript
            LOGGER.info(f'Loading {w} for TorchScript inference...')
            extra_files = {'config.txt': ''}  # model metadata
            model = torch.jit.load(w, _extra_files=extra_files)
            if extra_files['config.txt']:
                d = json.loads(extra_files['config.txt'])  # extra_files dict
                stride, names = int(d['stride']), d['names']
        elif pt:  # PyTorch
            from models.experimental import attempt_load  # scoped to avoid circular import
            model = torch.jit.load(w) if 'torchscript' in w else attempt_load(weights, map_location=device)
            stride = int(model.stride.max())  # model stride
            names = model.module.names if hasattr(model, 'module') else model.names  # get class names
        elif coreml:  # CoreML *.mlmodel
            import coremltools as ct
            model = ct.models.MLModel(w)
        elif dnn:  # ONNX OpenCV DNN
            LOGGER.info(f'Loading {w} for ONNX OpenCV DNN inference...')
            check_requirements(('opencv-python>=4.5.4',))
            net = cv2.dnn.readNetFromONNX(w)
        elif onnx:  # ONNX Runtime
            LOGGER.info(f'Loading {w} for ONNX Runtime inference...')
            check_requirements(('onnx', 'onnxruntime-gpu' if torch.has_cuda else 'onnxruntime'))
            import onnxruntime
            session = onnxruntime.InferenceSession(w, None)
        else:  # TensorFlow model (TFLite, pb, saved_model)
            import tensorflow as tf
            if pb:  # https://www.tensorflow.org/guide/migrate#a_graphpb_or_graphpbtxt
                def wrap_frozen_graph(gd, inputs, outputs):
                    x = tf.compat.v1.wrap_function(lambda: tf.compat.v1.import_graph_def(gd, name=""), [])  # wrapped
                    return x.prune(tf.nest.map_structure(x.graph.as_graph_element, inputs),
                                   tf.nest.map_structure(x.graph.as_graph_element, outputs))

                LOGGER.info(f'Loading {w} for TensorFlow *.pb inference...')
                graph_def = tf.Graph().as_graph_def()
                graph_def.ParseFromString(open(w, 'rb').read())
                frozen_func = wrap_frozen_graph(gd=graph_def, inputs="x:0", outputs="Identity:0")
            elif saved_model:
                LOGGER.info(f'Loading {w} for TensorFlow saved_model inference...')
                model = tf.keras.models.load_model(w)
            elif tflite:  # https://www.tensorflow.org/lite/guide/python#install_tensorflow_lite_for_python
                if 'edgetpu' in w.lower():
                    LOGGER.info(f'Loading {w} for TensorFlow Edge TPU inference...')
                    import tflite_runtime.interpreter as tfli
                    delegate = {'Linux': 'libedgetpu.so.1',  # install https://coral.ai/software/#edgetpu-runtime
                                'Darwin': 'libedgetpu.1.dylib',
                                'Windows': 'edgetpu.dll'}[platform.system()]
                    interpreter = tfli.Interpreter(model_path=w, experimental_delegates=[tfli.load_delegate(delegate)])
                else:
                    LOGGER.info(f'Loading {w} for TensorFlow Lite inference...')
                    interpreter = tf.lite.Interpreter(model_path=w)  # load TFLite model
                interpreter.allocate_tensors()  # allocate
                input_details = interpreter.get_input_details()  # inputs
                output_details = interpreter.get_output_details()  # outputs
        self.__dict__.update(locals())  # assign all variables to self

    def forward(self, im, augment=False, visualize=False, val=False):
        # YOLOv5 MultiBackend inference
        b, ch, h, w = im.shape  # batch, channel, height, width
        if self.pt:  # PyTorch
            y = self.model(im) if self.jit else self.model(im, augment=augment, visualize=visualize)
            return y if val else y[0]
        elif self.coreml:  # CoreML *.mlmodel
            im = im.permute(0, 2, 3, 1).cpu().numpy()  # torch BCHW to numpy BHWC shape(1,320,192,3)
            im = Image.fromarray((im[0] * 255).astype('uint8'))
            # im = im.resize((192, 320), Image.ANTIALIAS)
            y = self.model.predict({'image': im})  # coordinates are xywh normalized
            box = xywh2xyxy(y['coordinates'] * [[w, h, w, h]])  # xyxy pixels
            conf, cls = y['confidence'].max(1), y['confidence'].argmax(1).astype(np.float)
            y = np.concatenate((box, conf.reshape(-1, 1), cls.reshape(-1, 1)), 1)
        elif self.onnx:  # ONNX
            im = im.cpu().numpy()  # torch to numpy
            if self.dnn:  # ONNX OpenCV DNN
                self.net.setInput(im)
                y = self.net.forward()
            else:  # ONNX Runtime
                y = self.session.run([self.session.get_outputs()[0].name], {self.session.get_inputs()[0].name: im})[0]
        else:  # TensorFlow model (TFLite, pb, saved_model)
            im = im.permute(0, 2, 3, 1).cpu().numpy()  # torch BCHW to numpy BHWC shape(1,320,192,3)
            if self.pb:
                y = self.frozen_func(x=self.tf.constant(im)).numpy()
            elif self.saved_model:
                y = self.model(im, training=False).numpy()
            elif self.tflite:
                input, output = self.input_details[0], self.output_details[0]
                int8 = input['dtype'] == np.uint8  # is TFLite quantized uint8 model
                if int8:
                    scale, zero_point = input['quantization']
                    im = (im / scale + zero_point).astype(np.uint8)  # de-scale
                self.interpreter.set_tensor(input['index'], im)
                self.interpreter.invoke()
                y = self.interpreter.get_tensor(output['index'])
                if int8:
                    scale, zero_point = output['quantization']
                    y = (y.astype(np.float32) - zero_point) * scale  # re-scale
            y[..., 0] *= w  # x
            y[..., 1] *= h  # y
            y[..., 2] *= w  # w
            y[..., 3] *= h  # h
        y = torch.tensor(y)
        return (y, []) if val else y

'''===========3.AutoShapeï¼šè‡ªåŠ¨è°ƒæ•´shape,è¯¥ç±»åŸºæœ¬æœªç”¨================'''
class AutoShape(nn.Module):
    # YOLOv5 input-robust model wrapper for passing cv2/np/PIL/torch inputs. Includes preprocessing, inference and NMS
    conf = 0.25  # NMS confidence threshold
    iou = 0.45  # NMS IoU threshold
    classes = None  # (optional list) filter by class, i.e. = [0, 15, 16] for COCO persons, cats and dogs
    multi_label = False  # NMS multiple labels per box
    max_det = 1000  # maximum number of detections per image

    def __init__(self, model):
        super().__init__()
        self.model = model.eval()

    def autoshape(self):
        LOGGER.info('AutoShape already enabled, skipping... ')  # model already converted to model.autoshape()
        return self

    def _apply(self, fn):
        # Apply to(), cpu(), cuda(), half() to model tensors that are not parameters or registered buffers
        self = super()._apply(fn)
        m = self.model.model[-1]  # Detect()
        m.stride = fn(m.stride)
        m.grid = list(map(fn, m.grid))
        if isinstance(m.anchor_grid, list):
            m.anchor_grid = list(map(fn, m.anchor_grid))
        return self

    @torch.no_grad()
    def forward(self, imgs, size=640, augment=False, profile=False):
        # Inference from various sources. For height=640, width=1280, RGB images example inputs are:
        #   file:       imgs = 'data/images/zidane.jpg'  # str or PosixPath
        #   URI:             = 'https://ultralytics.com/images/zidane.jpg'
        #   OpenCV:          = cv2.imread('image.jpg')[:,:,::-1]  # HWC BGR to RGB x(640,1280,3)
        #   PIL:             = Image.open('image.jpg') or ImageGrab.grab()  # HWC x(640,1280,3)
        #   numpy:           = np.zeros((640,1280,3))  # HWC
        #   torch:           = torch.zeros(16,3,320,640)  # BCHW (scaled to size=640, 0-1 values)
        #   multiple:        = [Image.open('image1.jpg'), Image.open('image2.jpg'), ...]  # list of images

        t = [time_sync()]
        p = next(self.model.parameters())  # for device and type
        if isinstance(imgs, torch.Tensor):  # torch
            with amp.autocast(enabled=p.device.type != 'cpu'):
                return self.model(imgs.to(p.device).type_as(p), augment, profile)  # inference

        # Pre-process
        n, imgs = (len(imgs), imgs) if isinstance(imgs, list) else (1, [imgs])  # number of images, list of images
        shape0, shape1, files = [], [], []  # image and inference shapes, filenames
        for i, im in enumerate(imgs):
            f = f'image{i}'  # filename
            if isinstance(im, (str, Path)):  # filename or uri
                im, f = Image.open(requests.get(im, stream=True).raw if str(im).startswith('http') else im), im
                im = np.asarray(exif_transpose(im))
            elif isinstance(im, Image.Image):  # PIL Image
                im, f = np.asarray(exif_transpose(im)), getattr(im, 'filename', f) or f
            files.append(Path(f).with_suffix('.jpg').name)
            if im.shape[0] < 5:  # image in CHW
                im = im.transpose((1, 2, 0))  # reverse dataloader .transpose(2, 0, 1)
            im = im[..., :3] if im.ndim == 3 else np.tile(im[..., None], 3)  # enforce 3ch input
            s = im.shape[:2]  # HWC
            shape0.append(s)  # image shape
            g = (size / max(s))  # gain
            shape1.append([y * g for y in s])
            imgs[i] = im if im.data.contiguous else np.ascontiguousarray(im)  # update
        shape1 = [make_divisible(x, int(self.stride.max())) for x in np.stack(shape1, 0).max(0)]  # inference shape
        x = [letterbox(im, new_shape=shape1, auto=False)[0] for im in imgs]  # pad
        x = np.stack(x, 0) if n > 1 else x[0][None]  # stack
        x = np.ascontiguousarray(x.transpose((0, 3, 1, 2)))  # BHWC to BCHW
        x = torch.from_numpy(x).to(p.device).type_as(p) / 255  # uint8 to fp16/32
        t.append(time_sync())

        with amp.autocast(enabled=p.device.type != 'cpu'):
            # Inference
            y = self.model(x, augment, profile)[0]  # forward
            t.append(time_sync())

            # Post-process
            y = non_max_suppression(y, self.conf, iou_thres=self.iou, classes=self.classes,
                                    multi_label=self.multi_label, max_det=self.max_det)  # NMS
            for i in range(n):
                scale_coords(shape1, y[i][:, :4], shape0[i])

            t.append(time_sync())
            return Detections(imgs, y, files, t, self.names, x.shape)

'''===========3.Detectionsï¼šå¯¹æ¨ç†ç»“æœè¿›è¡Œå¤„ç†================'''
class Detections:
    # YOLOv5 detections class for inference results
    """ç”¨åœ¨AutoShapeå‡½æ•°ç»“å°¾
    detections class for YOLOv5 inference results
    """
    def __init__(self, imgs, pred, files, times=None, names=None, shape=None):
        super().__init__()
        d = pred[0].device  # device
        gn = [torch.tensor([*(im.shape[i] for i in [1, 0, 1, 0]), 1, 1], device=d) for im in imgs]  # normalizations
        # imgsï¼šåŸå›¾
        self.imgs = imgs  # list of images as numpy arrays
        # predï¼šé¢„æµ‹å€¼(xyxy, conf, cls)
        self.pred = pred  # list of tensors pred[0] = (xyxy, conf, cls)
        # namesï¼š ç±»å
        self.names = names  # class names
        # filesï¼š å›¾åƒæ–‡ä»¶å
        self.files = files  # image filenames
        # xyxyï¼šå·¦ä¸Šè§’+å³ä¸‹è§’æ ¼å¼
        self.xyxy = pred  # xyxy pixels
        # xywhï¼šä¸­å¿ƒç‚¹+å®½é•¿æ ¼å¼
        self.xywh = [xyxy2xywh(x) for x in pred]  # xywh pixels
        # xyxynï¼šxyxyæ ‡å‡†åŒ–
        self.xyxyn = [x / g for x, g in zip(self.xyxy, gn)]  # xyxy normalized
        # xywhnï¼šxywhnæ ‡å‡†åŒ–
        self.xywhn = [x / g for x, g in zip(self.xywh, gn)]  # xywh normalized
        self.n = len(self.pred)  # number of images (batch size)
        self.t = tuple((times[i + 1] - times[i]) * 1000 / self.n for i in range(3))  # timestamps (ms)
        self.s = shape  # inference BCHW shape

    def display(self, pprint=False, show=False, save=False, crop=False, render=False, save_dir=Path('')):
        crops = []
        for i, (im, pred) in enumerate(zip(self.imgs, self.pred)):
            s = f'image {i + 1}/{len(self.pred)}: {im.shape[0]}x{im.shape[1]} '  # string
            if pred.shape[0]:
                for c in pred[:, -1].unique():
                    n = (pred[:, -1] == c).sum()  # detections per class
                    s += f"{n} {self.names[int(c)]}{'s' * (n > 1)}, "  # add to string
                if show or save or render or crop:
                    annotator = Annotator(im, example=str(self.names))
                    for *box, conf, cls in reversed(pred):  # xyxy, confidence, class
                        label = f'{self.names[int(cls)]} {conf:.2f}'
                        if crop:
                            file = save_dir / 'crops' / self.names[int(cls)] / self.files[i] if save else None
                            crops.append({'box': box, 'conf': conf, 'cls': cls, 'label': label,
                                          'im': save_one_box(box, im, file=file, save=save)})
                        else:  # all others
                            annotator.box_label(box, label, color=colors(cls))
                    im = annotator.im
            else:
                s += '(no detections)'

            im = Image.fromarray(im.astype(np.uint8)) if isinstance(im, np.ndarray) else im  # from np
            if pprint:
                LOGGER.info(s.rstrip(', '))
            if show:
                im.show(self.files[i])  # show
            if save:
                f = self.files[i]
                im.save(save_dir / f)  # save
                if i == self.n - 1:
                    LOGGER.info(f"Saved {self.n} image{'s' * (self.n > 1)} to {colorstr('bold', save_dir)}")
            if render:
                self.imgs[i] = np.asarray(im)
        if crop:
            if save:
                LOGGER.info(f'Saved results to {save_dir}\n')
            return crops

    def print(self):
        self.display(pprint=True)  # print results
        LOGGER.info(f'Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {tuple(self.s)}' %
                    self.t)

    def show(self):
        self.display(show=True)  # show results

    def save(self, save_dir='runs/detect/exp'):
        save_dir = increment_path(save_dir, exist_ok=save_dir != 'runs/detect/exp', mkdir=True)  # increment save_dir
        self.display(save=True, save_dir=save_dir)  # save results

    def crop(self, save=True, save_dir='runs/detect/exp'):
        save_dir = increment_path(save_dir, exist_ok=save_dir != 'runs/detect/exp', mkdir=True) if save else None
        return self.display(crop=True, save=save, save_dir=save_dir)  # crop results

    def render(self):
        self.display(render=True)  # render results
        return self.imgs

    def pandas(self):
        # return detections as pandas DataFrames, i.e. print(results.pandas().xyxy[0])
        new = copy(self)  # return copy
        ca = 'xmin', 'ymin', 'xmax', 'ymax', 'confidence', 'class', 'name'  # xyxy columns
        cb = 'xcenter', 'ycenter', 'width', 'height', 'confidence', 'class', 'name'  # xywh columns
        for k, c in zip(['xyxy', 'xyxyn', 'xywh', 'xywhn'], [ca, ca, cb, cb]):
            a = [[x[:5] + [int(x[5]), self.names[int(x[5])]] for x in x.tolist()] for x in getattr(self, k)]  # update
            setattr(new, k, [pd.DataFrame(x, columns=c) for x in a])
        return new

    def tolist(self):
        # return a list of Detections objects, i.e. 'for result in results.tolist():'
        x = [Detections([self.imgs[i]], [self.pred[i]], self.names, self.s) for i in range(self.n)]
        for d in x:
            for k in ['imgs', 'pred', 'xyxy', 'xyxyn', 'xywh', 'xywhn']:
                setattr(d, k, getattr(d, k)[0])  # pop out of list
        return x

    def __len__(self):
        return self.n

'''===========5.Classifyï¼šäºŒçº§åˆ†ç±»æ¨¡å—================'''
class Classify(nn.Module):
    # Classification head, i.e. x(b,c1,20,20) to x(b,c2)
    def __init__(self, c1, c2, k=1, s=1, p=None, g=1):  # ch_in, ch_out, kernel, stride, padding, groups
        """
                è¿™æ˜¯ä¸€ä¸ªäºŒçº§åˆ†ç±»æ¨¡å—, ä»€ä¹ˆæ˜¯äºŒçº§åˆ†ç±»æ¨¡å—? æ¯”å¦‚åšè½¦ç‰Œçš„è¯†åˆ«, å…ˆè¯†åˆ«å‡ºè½¦ç‰Œ, å¦‚æœæƒ³å¯¹è½¦ç‰Œä¸Šçš„å­—è¿›è¡Œè¯†åˆ«, å°±éœ€è¦äºŒçº§åˆ†ç±»è¿›ä¸€æ­¥æ£€æµ‹.
                å¦‚æœå¯¹æ¨¡å‹è¾“å‡ºçš„åˆ†ç±»å†è¿›è¡Œåˆ†ç±», å°±å¯ä»¥ç”¨è¿™ä¸ªæ¨¡å—. ä¸è¿‡è¿™é‡Œè¿™ä¸ªç±»å†™çš„æ¯”è¾ƒç®€å•, è‹¥è¿›è¡Œå¤æ‚çš„äºŒçº§åˆ†ç±», å¯ä»¥æ ¹æ®è‡ªå·±çš„å®é™…ä»»åŠ¡å¯ä»¥æ”¹å†™, è¿™é‡Œä»£ç ä¸å”¯ä¸€.
                Classification head, i.e. x(b,c1,20,20) to x(b,c2)
                ç”¨äºç¬¬äºŒçº§åˆ†ç±»   å¯ä»¥æ ¹æ®è‡ªå·±çš„ä»»åŠ¡è‡ªå·±æ”¹å†™ï¼Œæ¯”è¾ƒç®€å•
                æ¯”å¦‚è½¦ç‰Œè¯†åˆ« æ£€æµ‹åˆ°è½¦ç‰Œä¹‹åè¿˜éœ€è¦æ£€æµ‹è½¦ç‰Œåœ¨å“ªé‡Œï¼Œå¦‚æœæ£€æµ‹åˆ°ä¾§æ‹åè¿˜æƒ³å¯¹è½¦ç‰Œä¸Šçš„å­—å†åšè¯†åˆ«çš„è¯å°±è¦è¿›è¡ŒäºŒçº§åˆ†ç±»
                """
        super().__init__()
        self.aap = nn.AdaptiveAvgPool2d(1)  # to x(b,c1,1,1)
        # è‡ªé€‚åº”å¹³å‡æ± åŒ–æ“ä½œ
        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g)  # to x(b,c2,1,1)
        # å±•å¹³
        self.flat = nn.Flatten()

    def forward(self, x):
        # å…ˆè‡ªé€‚åº”å¹³å‡æ± åŒ–æ“ä½œï¼Œ ç„¶åæ‹¼æ¥
        z = torch.cat([self.aap(y) for y in (x if isinstance(x, list) else [x])], 1)  # cat if list
        # å¯¹zè¿›è¡Œå±•å¹³æ“ä½œ
        return self.flat(self.conv(z))  # flatten to x(b,c2)
```

> æœ¬æ–‡å‚è€ƒï¼š
> 
> [ã€YOLOV5-5.x æºç è§£è¯»ã€‘common.py\_æ»¡èˆ¹æ¸…æ¢¦å‹æ˜Ÿæ²³HKçš„åšå®¢-CSDNåšå®¢][YOLOV5-5.x _common.py_HK_-CSDN]
> 
> [yolov5 ä»£ç è§£è¯» --common.py\_XiaoGShouçš„åšå®¢-CSDNåšå®¢][yolov5 _ --common.py_XiaoGShou_-CSDN]

![](https://i-blog.csdnimg.cn/blog_migrate/9171f300d63af5b21958cdbbb2e48333.gif)


[YOLO_YOLOv5]: https://blog.csdn.net/weixin_43334693/article/details/129312409?spm=1001.2014.3001.5502
[mirrors _ ultralytics _ yolov5 _ GitCode]: https://gitcode.net/mirrors/ultralytics/yolov5?utm_source=csdn_github_accelerator
[YOLOv5]: https://so.csdn.net/so/search?q=YOLOv5%E6%BA%90%E7%A0%81&spm=1001.2101.3001.7020
[YOLOv5_1]: https://blog.csdn.net/weixin_43334693/article/details/129356033?spm=1001.2014.3001.5501
[YOLOv5_2_detect.py]: https://blog.csdn.net/weixin_43334693/article/details/129349094?spm=1001.2014.3001.5501
[YOLOv5_3_train.py]: https://blog.csdn.net/weixin_43334693/article/details/129460666?spm=1001.2014.3001.5501
[YOLOv5_4_val_test_.py]: https://blog.csdn.net/weixin_43334693/article/details/129649553?spm=1001.2014.3001.5501
[YOLOv5_5_yolov5s.yaml]: https://blog.csdn.net/weixin_43334693/article/details/129697521?spm=1001.2014.3001.5501
[YOLOv5_6_1_yolo.py]: https://blog.csdn.net/weixin_43334693/article/details/129803802?spm=1001.2014.3001.5501
[Link 1]: #%C2%A0%E5%89%8D%E8%A8%80%C2%A0
[Link 2]: #%E7%9B%AE%E5%BD%95
[Link 3]: #%F0%9F%9A%80%E4%B8%80%E3%80%81%20%E5%AF%BC%E5%8C%85%E5%92%8C%E5%9F%BA%E6%9C%AC%E9%85%8D%E7%BD%AE%C2%A0
[1.1 _python]: #1.1%20%E5%AF%BC%E5%85%A5%E5%AE%89%E8%A3%85%E5%A5%BD%E7%9A%84python%E5%BA%93%C2%A0
[1.2]: #1.2%20%E5%8A%A0%E8%BD%BD%E8%87%AA%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9D%97
[Link 4]: #%F0%9F%9A%80%E4%BA%8C%E3%80%81%20%E5%9F%BA%E7%A1%80%E7%BB%84%E4%BB%B6
[2.1 autopad]: #2.1%C2%A0autopad
[2.2 Conv]: #2.2%C2%A0Conv
[2.3 DWConv]: #2.3%C2%A0DWConv
[2.4 Bottleneck]: #2.4%C2%A0Bottleneck
[2.5 BottleneckCSP]: #2.5%C2%A0BottleneckCSP
[2.6 C3]: #2.6%C2%A0C3
[2.6.1 C3]: #2.6.1%20C3
[2.6.2 C3SPP_C3]: #2.6.2%20C3SPP%28C3%29
[2.6.3 C3Ghost_C3]: #2.6.3%20C3Ghost%28C3%29
[2.7 SPP]: #2.7%C2%A0SPP
[2.7.1 SPP]: #2.7.1%20SPP
[2.7.2 SPPF]: #%C2%A02.7.2%20SPPF
[2.8 Focus]: #2.8%C2%A0Focus
[2.9 Contract]: #2.9%C2%A0Contract
[2.10 Expand]: #2.10%20Expand
[2.11 Concat]: #2.11%20Concat
[Link 5]: #%F0%9F%9A%80%E4%B8%89%E3%80%81%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9D%97%C2%A0
[3.1 TransformerLayer]: #3.1%20TransformerLayer
[3.2 TransformerBlock]: #3.2%20TransformerBlock
[Link 6]: #%F0%9F%9A%80%E5%9B%9B%E3%80%81%E5%B9%BB%E8%B1%A1%E6%A8%A1%E5%9D%97
[4.1 GhostConv]: #4.1%C2%A0GhostConv
[4.2 GhostBottleneck]: #%C2%A04.2%C2%A0GhostBottleneck
[Link 7]: #%F0%9F%9A%80%E4%BA%94%E3%80%81%E6%A8%A1%E5%9E%8B%E6%89%A9%E5%B1%95%E6%A8%A1%E5%9D%97
[5.1 C3TR_C3]: #5.1%20C3TR%28C3%29
[5.2 AutoShape]: #5.2%20AutoShape
[5.3 Detections]: #5.3%20Detections
[5.4 Classify]: #5.4%C2%A0Classify
[_common.py]: #%C2%A0%F0%9F%9A%80%E5%85%AD%E3%80%81common.py%E5%85%A8%E9%83%A8%E6%B3%A8%E9%87%8A
[YOLOv5_SPP_SPPF_tt_-CSDN]: https://blog.csdn.net/weixin_55073640/article/details/122621148?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168015601116800222867622%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168015601116800222867622&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-122621148-null-null.142%5Ev77%5Ewechat,201%5Ev4%5Eadd_ask,239%5Ev2%5Einsert_chatgpt&utm_term=SPPF&spm=1018.2226.3001.4187
[transformer_-CSDN]: https://blog.csdn.net/weixin_43334693/category_12288776.html?spm=1001.2014.3001.5482
[-cv_ghostbottleneck_pytorch_orangezs_-CSDN]: https://blog.csdn.net/ai_faker/article/details/109261824
[YOLOV5-5.x _common.py_HK_-CSDN]: https://blog.csdn.net/qq_38253797/article/details/119684388?ops_request_misc=&request_id=&biz_id=102&utm_term=yolov5%20common.py%E4%BB%A3%E7%A0%81%E8%AF%A6%E8%A7%A3&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-2-119684388.142%5Ev76%5Epc_search_v2,201%5Ev4%5Eadd_ask,239%5Ev2%5Einsert_chatgpt&spm=1018.2226.3001.4187
[yolov5 _ --common.py_XiaoGShou_-CSDN]: https://blog.csdn.net/XiaoGShou/article/details/117351971?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522162919967016780269827948%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=162919967016780269827948&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-3-117351971.first_rank_v2_pc_rank_v29&utm_term=common.py&spm=1018.2226.3001.4187