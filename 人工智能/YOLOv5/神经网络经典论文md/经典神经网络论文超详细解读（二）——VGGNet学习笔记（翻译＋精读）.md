## ![](https://i-blog.csdnimg.cn/blog_migrate/8f3ad35c49ffe28e2746d18992971984.jpeg) 

## 前言 

上一篇我们介绍了经典[神经网络][Link 1]的开山力作——AlexNet：[经典神经网络论文超详细解读（一）——AlexNet学习笔记（翻译＋精读）][AlexNet]

在文章最后提及了深度对网络结果很重要。今天我们要读的这篇VGGNet（《Very Deep Convolutional Networks For Large-Scale Image Recognition》），就是在AlexNet基础上对深度对网络性能的影响做了进一步的探索。它是ImageNet 2014年亚军，相比于AlexNet，AlexNet只有8层，而VGG有16～19层；AlexNet使用了11x11的卷积核，VGG使用了3x3卷积核和2x2的最大池化层。具体改进效果如何？让我们一起来看一下吧！

论文原文：[paper/VGG.pdf at main · shitbro6/paper · GitHub][paper_VGG.pdf at main _ shitbro6_paper _ GitHub]

## 目录 

[前言 ][Link 2]

[Abstract-摘要][Abstract-]

[一、Introduction—介绍][Introduction]

[二、ConvNet Configurations—ConvNet配置][ConvNet Configurations_ConvNet]

[2.1Architecture—结构][2.1Architecture]

[2.2Configurations—配置][2.2Configurations]

[2.3Discussion—讨论][2.3Discussion]

[三、Classification Framework—分类框架][Classification Framework]

[ 3.1Training—训练][3.1Training]

[ 3.2Testing—测试][3.2Testing]

[3.3Implementation Details—实现细节][3.3Implementation Details]

[四、Classification Experiments—分类实验 ][Classification Experiments_]

[4.1数据集：ILSVRC-2012][4.1_ILSVRC-2012]

[4.2Single Scale Evaluation—单尺度评价 ][4.2Single Scale Evaluation_]

[4.3Multi Scale Evaluation—多尺度评价][4.3Multi Scale Evaluation]

[4.4Multi-Crop Evaluation—多剪裁评价][4.4Multi-Crop Evaluation]

[4.5ConvNet Fusion—ConvNet融合][4.5ConvNet Fusion_ConvNet]

[4.6Comparision With The State of The Art—与最新技术的比较][4.6Comparision With The State of The Art]

[五、Conclusion—结论][Conclusion]

[论文十问][Link 3]

## Abstract-摘要 

### 翻译 

在这项工作中，我们研究了卷积网络深度在大规模图像识别环境下对其精度的影响。我们的主要贡献是使用具有非常小(3×3)卷积滤波器的体系结构对增加深度的网络进行了彻底的评估，这表明通过将深度推进到16-19个权重层，可以实现对现有技术配置的显著改进。这些发现是我们2014年ImageNet挑战赛提交的基础，我们的团队在局部化和分类路径上分别获得了第一和第二名。我们还表明，我们的表示法很好地推广到了其他数据集，在这些数据集上，它们获得了最先进的结果。我们已经公开了我们的两个性能最好的ConvNet模型，以促进对计算机视觉中深度视觉表示的使用的进一步研究。

### 精读 

#### 本文研究的问题 

本文研究了在大规模图像识别中，卷积网络深度对其识别精度的影响。

#### 本文主要贡献  

我们的主要贡献是使用具有非常小（3 ×3）卷积滤波器的架构对于增加了深度的网络的全面评估，这表明将通过将深度推到16-19个权重层可以实现对现有技术配置的显著改进。

## 一、Introduction—介绍 

### 翻译 

  卷积网络(ConvNets)最近在大规模图像和视频识别方面取得了巨大成功(Krizhevsky等人，2012年；Zeiler&Fergus，2013；Sermanet等人，2014；Simonyan&Zisserman，2014)，这要归功于大型公共图像库，例如ImageNet(邓等人，2009)，以及高性能计算系统，如GPU或大规模分布式集群(Dean等人，2012。特别地，ImageNet大规模视觉识别挑战(ILSVRC)(Russakovsky等人，2014)在深度视觉识别体系结构的进步中发挥了重要作用，该挑战已经作为几代大规模图像分类系统的试验台，从高维浅层特征编码(Perronnin等人，2010)(ILSVRC-2011的获胜者)到深度ConvNets(Krizhevsky等人，2012)(ILSVRC-201的获胜者)  
  随着ConvNets越来越成为计算机视觉领域的必需品，已经进行了许多尝试来改进Krizhevsky等人的原始架构。(2012)，以期达到更好的准确性。例如，向ILSVRC2013提交的表现最好的性能(Zeiler&Fergus，2013；Sermanet等人，2014)利用了较小的接受窗口大小和较小的第一卷积层步距。另一项改进涉及在整个图像和多个尺度上密集地训练和测试网络(Sermanet et al.，2014；Howard，2014)。在本文中，我们讨论了ConvNet体系结构设计的另一个重要方面-它的深度。为此，我们固定了结构的其他参数，并通过增加更多卷积层来稳步增加网络的深度，这是可行的，因为在所有层中都使用了非常小的(3×3)卷积滤波器。  
  因此，我们提出了更精确的ConvNet架构，它不仅在ILSVRC分类和局部化任务上实现了最先进的准确性，而且也适用于其他图像识别数据集，在这些数据集中，即使作为相对简单的管道的一部分使用，它们也可以获得优异的性能(例如，无需微调的线性支持向量机对深度特征进行分类)。我们已经发布了两个性能最好的模型，以方便进一步的研究。  
  论文的其余部分组织如下。在第2节中，我们描述了我们的ConvNet配置。第三节介绍了图像分类训练和评价的具体内容，并在第四节的ILSVRC分类任务中对两种配置进行了比较。第五节对本文进行了总结。为了完整性，我们还在附录A中描述和评估了我们的ILSVRC-2014对象局部化系统，并在附录B中讨论了非常深入的功能对其他数据集的概括。最后，附录C包含主要论文修订的列表。

### 精读 

#### 介绍卷积网络（ConvNets）的发展和应用 

应用方面：最近在大规模图像和视频识别方面取得了巨大成功（Overfeat）。

原因：这是由于大型公共图像存储库，如ImageNet和高性能计算系统，例如GPU或大规模分布式集群（Tensorflow）。特别是，ImageNet大规模视觉识别挑战赛（ILSVRC）（Russakovsky et al.，2014）在深度视觉识别体系结构的发展中发挥了重要作用，它是几代大型图像分类系统的试验台，从高维浅特征编码（Perronnin等人，2010年）（ILSVRC-2011的获胜者）到深度转换（Krizhevsky等人，2012年）（ILSVRC-2012的获胜者）。

#### 之前对ConvNets精度提高的尝试 

1.利用了较小的接受窗口大小和第一卷积层的较小步幅（处理更小的细节）

2.改进涉及在整个图像和多个尺度上密集地训练和测试网络（修改输入网络的数据）

#### 本文对ConvNets精度提高的改进方法和结果 

方法：在本文中，我们将讨论ConvNet架构设计的另一个重要方面——深度。为此，我们确定了体系结构的其他参数，并通过添加更多卷积层来稳步增加网络的深度，这是可行的，因为在所有层中使用了非常小的（3×3）卷积核。

结果：我们提出了更精确的ConvNet体系结构，不仅在ILSVRC分类和定位任务上达到了最先进的精度，而且还适用于其他图像识别数据集，即使仅作为特征提取器，它们也能获得优异的性能（例如，由线性支持向量机分类的深度特征，无需微调）。为了便于进一步研究，我们发布了两个性能最好的模型(VGG16和VGG19)。

## 二、ConvNet Configurations—ConvNet配置 

### 翻译 

为了在公平的环境下衡量ConvNet深度增加带来的改善，我们所有的ConvNet层配置都是根据相同的原则设计的，灵感来自Ciresan等人。(2011)；Krizhevsky等人。(2012年)。在本节中，我们首先描述我们的ConvNet配置的一般布局(第2.1节)，然后详细说明评估中使用的具体配置(第2.2节)。然后讨论我们的设计选择，并与第2.3节中的现有技术进行比较。

### 精读 

![](https://i-blog.csdnimg.cn/blog_migrate/569fe388b4e0bf23e387f232bceefc57.png)

### 2.1Architecture—结构 

### 翻译 

在训练过程中，我们的ConvNet的输入是一幅固定大小的224×224 RGB图像。我们所做的唯一预处理是从每个像素减去在训练集上计算的平均RGB值。图像通过一堆卷积(卷积)。层，其中我们使用带有非常小的接收字段的过滤器：3×3(这是捕捉左/右、上/下、中心的概念的最小尺寸)。在其中一种配置中，我们还使用1×1卷积滤波器，可以将其视为输入通道的线性变换(随后是非线性)。卷积步长固定为1像素；卷积的空间填充。层输入是这样的，即在卷积之后空间分辨率保持不变，即对于3×3卷积，填充是1个像素。层次感。空间池化是由五个最大池化层执行的，这些层紧跟在一些圆锥之后。层(不是所有的。层之后是最大池化)。最大合用是在2×2像素窗口上执行的，步长为2。  
  卷积层的堆栈(在不同的体系结构中具有不同的深度)后面是三个全连接(FC)层：前两个层各有4096个通道，第三个执行1000路ILSVRC分类，因此包含1000个通道(每个类别一个)。最后一层是softmax层。所有网络中的完全连接层的配置都是相同的。  
  所有隐藏层都配备了校正(RELU(Krizhevsky等人，2012))非线性。我们注意到，我们的所有网络(除了一个网络)都没有包含本地响应归一化(LRN)归一化(Krizhevsky等人，2012年)：如第4节所示，这种归一化不会提高ILSVRC数据集的性能，而是导致内存消耗和计算时间增加。在适用的情况下，LRN层的参数是(Krizhevsky等人，2012年)的参数。

### 精读 

深度是卷积神经网络架构的一个重要方面。为了使深度更深，作者将卷积层中卷积核尺寸都设为很小——3×3，将卷积层数量加大，使深度更深，事实证明是可行的。

#### 方法 

1.输入：一个固定大小的224\*224RGB图像

2.唯一预处理：将输入的224×224×3通道的像素值，减去平均RGB值，然后进行训练

3.卷积核：①使用最小尺寸的卷积核3×3这个尺寸也是能捕捉上下左右和中间方位的最小尺寸

②有些卷积层中还使用了1×1大小的卷积核（FC层之间的），可看作是输入通道的线性变换

> ● 为什么3个3x3的卷积核可以代替7x7的卷积？
> 
> ①得到更好的拟合效果。3个3x3的卷积，使用了3个ReLU函数非线性校正，增加了非线性表达能力，比一个ReLU的单层Layer更具有识别能力，使得分割平面更具有可分性
> 
> ②减少网络参数个数。对于C个通道的卷积核，7×7×C×C 比3×3×C×C的参数量大。

> ● 1x1卷积核的作用？
> 
> ①在不影响感受野的情况下，增加模型的非线性性
> 
> ②1x1卷积相当于线性变换，非线性激活函数起到非线性作用

4.卷积步幅 ：卷积步幅固定为1像素，3×3卷积层的填充设置为1个像素。

5.池化层：池化层采用空间池化，空间池化有五个最大池化层，他们跟在一些卷积层之后，但是也不是所有的卷积层后都跟最大池化。最大池化层使用2×2像素，步幅为2。

6.卷积层：一个卷积层（在不同的体系结构中具有不同的深度）之后是三个完全连接的（FC）层：前两个层各有4096个通道，第三个层执行1000路ILSVRC分类，因此包含1000个通道（每类一个）（1000分类对应ImageNet-1K）。最后一层是softmax层，用来分类。在所有网络中，完全连接的层的配置都是相同的。

7.隐藏层：所有隐藏层都有ReLU非线性函数，网络除了第一个其他都不包含局部响应归一化（LRN）（原因：论文中，作者指出，虽然LRN在AlexNet对最终结果起到了作用，但在VGG网络中没有效果，并且该操作会增加内存和计算，从而作者在更深的网络结构中，没有使用该操作）

![](https://i-blog.csdnimg.cn/blog_migrate/02f7e1e336694b92af6db0ea674a9f17.png)

### 2.2Configurations—配置 

### 翻译 

该篇论文中卷积网络的设置在表格1中展示出来。在接下来的环节中我们将使用它们的代词(A-E)。所有通用的设置在2.1节中叙述完，唯一的不同点就在于网络的深度：从A中的11层（8层卷积+3层全连接层）到E中的19层（16层卷积+3层全连接层）。卷积层的宽度（即通道数）是相对小的，从第一层的64，每经过一层最大池化层就扩大2倍，直到达到512。  


表一：ConvNet配置（以列展示）。配置的深度从左（A）向右（E）增加，添加了更多的层（添加的层以粗体表示)。卷积层参数表示成“conv<感受野大小>-<通道数量>”。为了简洁，未展示ReLU激活函数。

![](https://i-blog.csdnimg.cn/blog_migrate/b94532f2c808e5f9c2619754b6a82e85.png)

表二：在表2中，我们报告了每种配置的参数数量。尽管有很大的深度，但我们的网中的权重数并不比更浅、更大圆度的网中的权重数多。层宽度和感受野(144m重量(Sermanet等人，2014年))。

![](https://i-blog.csdnimg.cn/blog_migrate/a2760aea197a8773e07c8dde114afcea.png)

###  精读 

1.这个表的对比是一个对照实验组。

2.我们可以看到第一个A组有11层(8卷积层+3全连接层，只算带权重的层 所以池化层不算)。另一个A组加上LRN。D,E组就是在上文中见到的VGG16和VGG19。C和D都是16层，他俩的区别是C组使用了1×1的卷积核，而D组都是3 × 3 卷积核。

3.图中加粗的就是增加的或者修改的层。

4.这里面是有Relu函数的，作者没有标明出来。

具体对比结果我们下一节详细讨论。

### 2.3Discussion—讨论 

### 翻译 

我们的ConvNet配置与ILSVRC-2012(Krizhevsky等人，2012)和ILSVRC-2013比赛(Zeiler&Fergus，2013；Sermanet等人，2014)表现最好的参赛作品中使用的配置有很大不同。而不是在第一层conv中使用相对较大的感受野。(例如，在(Krizhevsky等人，2012)中为11×11，步长为4，或在(Zeiler&Fergus，2013；Sermanet等人，2014)中为7×7，步长为2)，我们在整个网络中使用非常小的3×3感受野，这些感受野在每个像素处与输入卷积(步长为1)。很容易看到两个3×3 conv堆叠。各层(中间无空间混合)的有效感受野为5×5，其中3层的有效感受野为7×7。那么，比方说，使用三个3×3卷积的堆叠，我们得到了什么呢？而不是单一的7×7层？首先，我们加入了三个非线性校正层而不是单个校正层，使得决策函数更具区分性。其次，我们减少了参数的数量：假设一个三层3×3卷积叠加的输入和输出都有C个通道，该叠加由3(32C2)=27C2权重参数化；同时，单个7×7卷积，层将需要72C2=49C2参数，即比27C2多81%。这可以看作是对7×7卷积施加了正规化过滤器，迫使它们通过3×3的过滤器进行分解(在其间注入非线性)。  


### 精读 

#### 对比第一个卷积层感受野大小： 

本文章中的网络配置与前面一些表现非常好的网络配置比较不一样（例如AlexNet和OverFeat），VGG在整个网络使用了非常小的感受野而不是在第一个卷积层使用相对来说比较大的卷积层（AlexNet中卷积核大小为11×11步长为4，OverFeat中卷积核大小为7×7步长为2）

> 感受野相关问题
> 
> 感受野（Receptive Field）的定义是卷积神经网络每一层输出的特征图（feature map）上的像素点在输入图片上映射的区域大小。再通俗点的解释是，特征图上的一个点跟原图上有关系的点的区域。
> 
> ![](https://i-blog.csdnimg.cn/blog_migrate/8e49c0d7fd69c1cc949a9869108f78fe.png)
> 
> 我们可以通过上图看出，在卷积神经网络 中，越深层的神经元看到的输入区域越大。上图中的卷积核大小均为3×3，步幅均为1，绿色标记的是Layer2每个神经元看到的区域，黄色标记的是Layer3看到的区域。这样我们就可以很明显的看出来，Layer2每个神经元可看到的Layer1上3×3大小的区域，Layer3每个神经元看到的是Layer2上3×3大小的区域，同时，该区域可以看到Layer1上5×5大小的区域。
> 
> 动态效果可以看下图：
> 
> ![](https://i-blog.csdnimg.cn/blog_migrate/fe725328bc1d06b74bd681c666193ef5.png)

#### 好处 

通过对卷积层作用的评估，可以很容易看出来两个3 × 3的卷积层可以起到一个5 × 5 卷积层的作用，三个3×3的卷积层可以起到一个7 × 7卷积层的作用。

#### 为什么有这样的作用？ 

1.合并了三个非线性ReLU层，不是只用一个，使得决策函数更具有判别性。

2.减少了参数数量。

## 三、Classification Framework—分类框架 

### 3.1Training—训练 

### （1）步骤和参数 

### 翻译 

ConvNet训练过程一般遵循Krizhevsky等人的做法。(2012)(除了对来自多尺度训练图像的输入作物进行采样之外，如后面所解释的)。即，通过使用带动量的小批量梯度下降(基于反向传播(LeCun等人，1989))来优化多项式Logistic回归目标来执行训练。批量设置为256，动量为0.9。训练通过权重衰减(L2惩罚倍数设置为5·10−4)和前两个完全连接的层的dropout规则化(dropout率设置为0.5)来归一化。学习速率最初设置为10−2，当验证集精度停止提高时，学习率降低到原来的1/10。总共降低了3倍的学习速率，并在37万次迭代(74个epochs)后停止学习。我们猜测，尽管与(Krizhevsky等人，2012年)相比，我们的网络有更多的参数数量和更深的网络，网络收敛所需的epochs更少由于(a)更大深度和更小Conv过滤器大小施加的隐式正则化；(b)某些层的预初始化。

### 精读 

卷积网络训练步骤大致跟随AlexNet，除了从多尺度训练图像中对输入进行采样，也就是说通过使用小批量梯度下降法优化多项式逻辑回归来进行训练。同时添加了L2正则项进行正则化，使用Dropout减少全连接层过拟合，并且应用了学习率退火，具体参数设置如下：

<table> 
 <tbody> 
  <tr> 
   <td><strong>参数名</strong></td> 
   <td><strong>大小</strong></td> 
  </tr> 
  <tr> 
   <td>batch size（批量大小）</td> 
   <td>256</td> 
  </tr> 
  <tr> 
   <td>momentum（动量）</td> 
   <td>0.9</td> 
  </tr> 
  <tr> 
   <td>weight decay（权重衰减）</td> 
   <td>0.0005</td> 
  </tr> 
  <tr> 
   <td>dropout ratio（随机失活率）</td> 
   <td>0.5</td> 
  </tr> 
  <tr> 
   <td>learning rate（学习率）</td> 
   <td>0.01</td> 
  </tr> 
  <tr> 
   <td>迭代步数</td> 
   <td>370K</td> 
  </tr> 
  <tr> 
   <td>epoches(轮数)</td> 
   <td>75</td> 
  </tr> 
 </tbody> 
</table>

> ● 学习率退火：
> 
> 当验证集的准确率停止提升的时候除以10，学习率总共下降了三次，一共经过了74个Epochs

#### 网络能更快收敛的原因？（与AlexNet相比，VGGnet有更多的参数和更大的深度） 

①较大深度和较小的卷积核所施加的隐式正则化

②某些层的预初始化

> 正则化相关问题
> 
> ● 正则化
> 
> 旨在更好实现模型泛化的补充技术，即在测试集上得到更好的表现。
> 
> ● L1和L2正则化
> 
> L1正则化会让特征变得稀疏，起到特征选择的作用；
> 
> L2正则化会让模型变得更简单，防止过拟合，而不会起到特征选择的作用。
> 
> ● 隐式正则化
> 
> 没有直接对模型进行正则化约束，但间接获取更好的泛化能力。
> 
> 1、数据标准化，平滑优化目标函数曲面； 2、数据增强，扩大数据集规模； 3、随机梯度下降算法

### （2）初始化权重  

### 翻译 

 网络权值的初始化很重要，因为由于深层网络中梯度的不稳定性，错误的初始化可能会导致学习停滞。为了绕过这个问题，我们从训练配置A(表1)开始，它足够浅，可以通过随机初始化进行训练。然后，当训练更深的体系结构时，我们用Net A的层初始化前四个卷积层和最后三个全连通层(中间层是随机初始化的)。我们没有降低预初始层的学习速率，允许它们在学习过程中改变。对于随机初始化(如果适用)，我们从具有零均值和10−2方差的正态分布中抽样权重。偏差被初始化为零。值得注意的是，在论文提交后，我们发现可以使用Glorot&Bengio(2010)的随机初始化过程来初始化权重，而不需要预先训练。

### 精读 

#### 原因 

因为深度网络中梯度的不稳定性，错误的初始化会阻碍学习。

#### 方法 

开始训练配置A，通过随机初始化进行训练，然后对更深层次网络结构训练。将前四层卷积层和后三个FC层都按照A网络的初始化配置设置，中间层接着随机初始化。没有减少预初始化层的学习率，而是让它们随着学习的进程发生改变。

#### 采样参数 

对于随机初始化，我们从一个均值为0方差为0.01的正态分布中对权重进行采样，偏置初始化为0。

#### 证明结论 

即使随机初始化所有的层，模型也能训练的很好。

#### 后期发现 

使用随机初始化程序可以在不进行预训练的情况下初始化权重。

### （3）数据增广 

### 翻译 

 为了获得固定大小的224×224 ConvNet输入图像，从重新缩放的训练图像中随机裁剪它们(每次SGD迭代每个图像一个裁剪)。为了进一步增加训练集，这些作物经历了随机水平翻转和随机RGB颜色变换(Krizhevsky等人，2012年)。下面解释训练图像的重新缩放。  
  Training image size.设S是各向同性重新缩放的训练图像的最小边，从中裁剪ConvNet输入(我们也将S称为训练比例)。当裁剪大小固定为224×224时，原则上S可以采用不小于224时的任何值：对于S=224时，裁剪将捕获整个图像统计，完全覆盖训练图像的最小边；对于S≫224，裁剪将对应于包含小对象或对象部分的图像的一小部分。  
  我们考虑了两种设置训练尺度S的方法。第一种是固定S，其对应于单尺度训练(请注意，采样作物内的图像内容仍然可以代表多尺度图像统计)。在我们的实验中，我们评估了在两个固定尺度上训练的模型：S=256(这在现有技术中已经被广泛使用(Krizhevsky等人，2012年；Zeiler&Fergus，2013；Sermanet等人，2014))和S=384。在给定ConvNet配置的情况下，我们首先使用S=256训练网络。为了加快S=384网络的训练速度，用S=256时预先训练的权值进行了初始化，并使用了较小的初始学习率10−3。  
  设置S的第二种方法是多尺度训练，其中每个训练图像通过从特定范围\[Smin，Smax\] (我们使用Smin=256和Smax=512)随机采样S来单独重新缩放。由于图像中的对象可以具有不同的大小，因此在训练期间考虑这一点是有益的。这也可以被看作是通过尺度抖动来扩大训练集，其中单个模型被训练来识别大范围尺度上的对象。出于速度的原因，我们通过微调具有相同配置的单尺度模型的所有层来训练多尺度模型，预先训练固定的S=384。

### 精读 

#### 方法 

对图片进行缩放后裁剪出224 × 224大小的图片，然后进行随机水平翻转和随机的RGB通道切换。

#### 训练尺度S 

训练用到的224 × 224的图片是从缩放后的原始图片中裁剪出来的，而缩放不仅仅可以缩小也可以放大，记图片缩放后的最短边的长度为S，也称为训练尺度(training scale)。

S的取值：当裁剪尺寸固定为224\*224时，原则上S可以取不小于224的任何值：对于S=224来说，裁剪将会捕获整个的图像统计数据，将会完整横跨训练图像的最小边。对于S ≫ 224，裁剪将会对应于图像的一小部分，包括一个小对象或者对象的一部分。

有两种设置训练尺度S的办法

1、使用固定的S（单尺度的训练），本文使用了两种大小：256(应用于AlexNet，ZFNet，Sermanet)和384。

2、使用变化的S（多尺度的训练），给定一个S变化的范围\[ Smin , Smax \](文章中使用的范围是\[ 256 , 512 \] )，使其在这个范围中随机选值来缩放图片。

### 3.2Testing—测试 

### 翻译 

在测试时，给定一个训练好的ConvNet和一个输入图像，它按以下方式分类。首先，将其各向同性地重新缩放到预定义的最小图像侧，表示为Q(我们也将其称为测试比例)。我们注意到Q不一定等于训练尺度S(正如我们将在第4节中说明的那样，对每个S使用几个Q值可以提高性能)。然后，以类似于Sermanet等人的方式在重新缩放的测试图像上密集地应用网络(Sermanet等人，2014)。也就是说，首先将完全连通的层转换为卷积层(第一FC层转换为7×7卷积。层，最后两个FC层为1×1 Conv。层)。然后，将得到的完全卷积网络应用于整个(未裁剪)图像。结果是一个类分数图，其通道数等于类数，空间分辨率可变，具体取决于输入图像的大小。最后，为了获得图像的固定大小的班级分数向量，对班级分数地图进行空间平均(总和)。我们还通过水平翻转图像来扩充测试集；对原始图像和翻转图像的软最大类后验进行平均，以获得图像的最终分数。  
由于全卷积网络应用于整个图像，因此不需要在测试时对多个作物进行采样(Krizhevsky等人，2012)，这样效率较低，因为它需要为每个作物重新计算网络。同时，正如Szegedy等人所做的那样，使用了大量的作物。(2014)，可以提高精度，因为与全卷积网络相比，它可以对输入图像进行更精细的采样。此外，由于不同的卷积边界条件，多作物评估与密集评估是互补的：当将卷积网络应用于作物时，卷积的特征地图用零填充，而在密集评估的情况下，相同作物的填充自然来自图像的相邻部分(由于卷积和空间汇集)，这大大增加了整个网络接收区域，因此捕获了更多的上下文。虽然我们认为，在实践中，多作物计算时间的增加并不能证明在精度方面的潜在收益是合理的，但为了参考，我们也使用每尺度50种作物(5×5规则网格，2个翻转)来评估我们的网络，针对3个尺度上的150个作物，这与Szegedy等人使用的4个尺度上的144个作物相当。

### 精读 

#### 方法 

将全连接层等效替换为卷积层进行测试

#### 原因 

卷积层和全连接层的唯一区别就是卷积层的神经元和输入是局部联系的，并且同一个通道（channel）内的不同神经元共享权值（weight）。卷积层和全连接层的计算实际上相同，因此可以将全连接层转换为卷积层。

改造前：假设网络是这样的 N个卷积 -> 全连接 -> 全连接。假如N个卷积之后的数据变成7x7x512 则此时展成向量变成 25088个元素的向量，通过一个4096神经元的全连接层 ，变成4096个元素的向量，再通过一个1000个神经元的全连接，最后输出1000个元素的向量。

![](https://i-blog.csdnimg.cn/blog_migrate/60a80826bd1e378580058ce30a181fca.png)

（图片来自b站up主：同济子豪兄。下同）

改造后：当得到前面的数据 7x7x512之后，不将其展开成向量，而是直接送去有 4096 个 7×7卷积核的卷积层，对比改造之前看，其实就是将改造之前的全连接层里的4096个单神经元变成了 7×7的卷积核，参数依旧是等量的。同理，后面就变成了1000个1×1卷积核的卷积层(实际上论文中是三层全连接层，改为一个7x7卷积层和两个 1×1卷积层，他这里少画了一层1×1的卷积层）

![](https://i-blog.csdnimg.cn/blog_migrate/b6ae1025cc37057fc732a363270327a9.png)

####  目的 

1.输入图像的大小不再受限制，因此可以高效地对图像作滑动窗式预测。

2.而且全连接层的计算量比较大，等效卷积层的计算量减小了，这样既达到了目的又十分高效。

### 3.3Implementation Details—实现细节 

### 翻译 

我们的实现源自公开可用的C++ Caffe工具箱(Jia，2013)(2013年12月扩展)，但包含许多重大修改，允许我们在单个系统中安装的多个GPU上执行训练和评估，以及在多个比例(如上所述)的全尺寸(未裁剪)图像上进行训练和评估。多GPU训练利用数据并行性，通过将每批训练图像分成几个GPU批次来执行，这些批次在每个GPU上并行处理。在计算GPU批次梯度之后，对它们进行平均，以获得整个批次的梯度。所有GPU之间的渐变计算是同步的，因此结果与在单个GPU上训练时完全相同。  
  虽然最近提出了更复杂的加速ConvNet训练的方法(Krizhevsky，2014)，这些方法针对网络的不同层使用模型和数据并行，但我们发现，与使用单个GPU相比，我们在概念上简单得多的方案已经在现成的4-GPU系统上提供了3.75倍的加速比。在配备了四个NVIDIA Titan Black GPU的系统上，根据架构的不同，训练一个网络需要2-3周时间。  


### 精读 

1.多个GPU上训练：实现使用的是基于C++的Caffe toolbox，并且还做了一系列的优化，可以在单个系统中安装的多个GPU上执行训练和评估，以及训练和评估多尺度的全尺寸（未裁剪）图像。

2.分为多个GPU批次：通过将每批训练图像分成几个GPU批次，在每个GPU上并行处理来进行，计算GPU批次梯度后，对它们进行平均可以获得完整批次的梯度。因此结果与在单个GPU上训练的结果是一样是的，但是可以节约很多时间。

3.花费的时间：该模型在配备有4个NVIDIA Titan Black GPU的机器上训练一个架构需要花费2-3周的时间

## 四、Classification Experiments—分类实验 

### 4.1数据集：ILSVRC-2012 

### 翻译 

数据集 在本节中，将展示我们提出的卷积网络架构在`ILSVRC-2012`数据集`(用于ILSVRC2012-2014挑战)`上获得的图像分类结果。数据集包括1000个类的图像，并分为三组：训练（130万张图像）、验证(50K张图像)和测试(100K个保留类标签的图像)。分类性能通过两个指标进行评估： `the top-1 and top-5 error`。前者是一种多类分类误差，即错误分类图像的比例；后者是`ILSVRC`中使用的主要评价标准，并且计算图像gt 不在`top-5 predicted categories`的比例。

对于大多数实验，我们使用验证集作为测试集。某些实验也在测试集上进行，并作为ILSVRC-2014竞赛的“VGG”团队参赛作品提交给官方。

### 精读 

分类：这个数据集有1000个类，包含三个部分：训练集(1.3M)，验证集(50K)，测试集(100K)，可以通过两个指标来评估准确率：top-1和 top-5误差。

> top-1和 top-5误差
> 
> top-1误差：是一个多类分类误差，表示分类错误图片的比例（模型猜的最可能的结果就是正确答案的概率）
> 
> top-5误差：是竞赛评估的主要指标，表示了真实的类别不在预测到的概率最大的五个类别中的错误率（模型猜的前五个结果里面包涵正确答案的概率）

### 4.2Single Scale Evaluation—单尺度评价（测试角度） 

### 翻译  

我们首先用章节2.2中描述的网络配置在单一尺度上评估单个ConvNet模型的性能。.测试图像大小设置如下：Q=S,固定的S，Q=0.5(Smin+Smax)，S∈\[Smin，Smax\]。结果如表3所示。首先，我们注意到使用局部响应归一化(A-LRN网络)在没有任何归一化层的改进模型A。因此，我们不在更深层次的架构中使用标准化(B-E)。

![](https://i-blog.csdnimg.cn/blog_migrate/1e29d7d65ea296272220a9106aac6506.png)

  
其次，我们观察到分类误差随着ConvNet深度的增加而减小：从A的11层减少到E的19层。值得注意的是，尽管有相同的深度，配置C(它包含三个1×1conv层)，性能比配置D差，后者使用3×3conv。贯穿整个网络的图层。这表明，虽然额外的非线性确实有帮助(C比B好),但是通过使用大的卷积核的来捕获空间上下文也很重要。(D优于C)。当深度达到19层时，我们的体系结构的错误率达到饱和，但更深的模型可能有利于更大的数据集。我们还比较了B结构，在浅层把每对3×3conv换成1个5×5conv层(它有相同的感受野。)浅层网的前1误差比B（(on a center crop）高7%，这证实了具有小滤波器的深网优于大滤波器的浅层网。  
最后，在训练时的尺度变化(S∈\[256；512\])明显比训练固定最小边的图像(S=256或S=384)，即使在测试时使用单一的尺度。这证实了通过尺度变化增强训练集确实有助于捕获多尺度图像统计数据。

### 精读 

#### 采用的方法 

训练尺度S：（1）一种方法是固定S的大小

（2）另一种方法是从一定区间内随机取S(测试集记为Q)

具体实现：测试时所用的scale固定。这里把训练scale和测试的scale分别用S和Q表示。当S为固定值时，令Q=S固定；当S为\[Smin,Smax\]浮动时，Q固定为=0.5\[Smin + Smax\]。

#### 结论 

（对比表格放在翻译里了~）

1.测试发现A组和A-LRN组的top1和top5错误率几乎持平，LRN局部响应归一化并没有带来精度提升，故在A-LRN之后的B～E类VGG网络中，都没有使用。

2.变动的S比固定的S准确率高。在训练中，采用浮动尺度效果更好，因为这有助于学习分类目标在不同尺寸下的特征。

3.卷积网络越深，损失越小，效果越好。

4.C优于B，表明增加的非线性ReLU有效。

5.D优于C，表明卷积层3×3对于捕捉空间特征有帮助。

6.E深度达到19层后达到了损失的最低点，达到饱和，更深的层次对精度没有提升，但是对于其他更大型的数据集来说，可能更深的模型效果更好。

7.B和同类型卷积核为5×5的网络进行了对比，发现其top-1错误率比B高7%，表明小尺寸卷积核效果更好。

###  4.3Multi Scale Evaluation—多尺度评价（测试角度） 

### 翻译 

在单一尺度上评估了ConvNet模型之后，我们现在评估了测试时尺度抖动的影响。它包括在一个测试图像的几个重新缩放的版本(对应于不同的Q值)上运行一个模型，然后平均得到的类后验。考虑到训练量表和测试量表之间的很大差异会导致性能下降，用固定S训练的模型在三个测试图像大小上进行评估，接近训练一个：Q=\{S−32，S，S+32\}。同时，训练时的尺度抖动允许网络在测试时应用于更广泛的尺度范围，因此使用变量S∈\[Smin；Smax\]训练的模型在更大的尺寸Q = Smin， 0.5(Smin+Smax) ，Smax范围内进行评估.

结果如表4所示，表明测试时的尺度抖动导致更好的性能（与在单一尺度上评价相同的模型相比，如表3所示）。与之前一样，最深的配置(D和E)表现最好，规模抖动比固定最小边训练更好。我们在验证集上的最佳单网络性能是24.8%/7.5%top1/top5错误（在表4中以粗体突出显示）。在测试集上，配置E达到了7.3%的前5名错误。

![](https://i-blog.csdnimg.cn/blog_migrate/678508e7f3e5d895550cb38f02b74b17.png)

### 精读 

#### 方法 

multi-scale表示测试时的scale不固定。这里当训练时的S固定时，Q取值是\{S-32,S,S+32\}这三个值，进行测试过后取平均结果。当S为\[Smin,Smax\]浮动时,Q取\{Smin,0.5(Smin+Smax),Smax\}，测试后取平均。

#### 结论 

1.同single scale一样，模型越深，效果越好

2.同深度下，浮动scale效果好于固定scale

### 4.4Multi-Crop Evaluation—多剪裁评价（测试输入角度） 

### 翻译 

在表5中，我们比较了密集的ConvNet评价和Multi-Crop评价(见第二节。3.2的细节)。我们还通过平均它们的softmax输出来评估这两种评估技术的互补性。可以看出，使用Multi-Crop的性能略优于密集评估，而且这两种方法确实是互补的，因为它们的组合性能优于每一种方法。如上所述，我们假设这是由于对卷积边界条件的不同处理。

![](https://i-blog.csdnimg.cn/blog_migrate/5f910136f5eb6515503add55e1508835.png)

### 精读  

#### dense evaluation 与multi-crop evaluation两种评估方法的区别 

①multi-crop：即对图像进行多样本的随机裁剪，然后通过网络预测每一个样本的结构，最终对所有结果平均。GoogleNet中使用了很多multi-crop的技巧，可以显著提升精度，因为有更精细的采样。

②dense ：就是在上章说的将全连接层都变成卷积网络的那种训练方式。

#### 方法 

在表5中我们比较了密集评估与多裁剪评估。我们还通过平均Softmax输出来评估两种评估技术的互补性。

#### 结果 

ConvNet评估技术对比。在所有的实验中训练尺度S是从\[256; 512\]中抽取的，3个测试尺度Q是从\{256，384，512\}中抽取的。

#### 结论 

1.使用multi-crop优于dense

2.这两种方法是互补的

3.multi-crop＋dense方法结合的效果最好

### 4.5ConvNet Fusion—ConvNet融合 

### 翻译 

到目前为止，我们评估了各个ConvNet模型的性能。在这部分的实验中，我们通过平均它们的类后验来组合几个模型的输出。由于模型的互补性，这提高了性能，并在2012年`(Krizhevsky et al., 2012) and 2013 (Zeiler & Fergus, 2013; Sermanet et al., 2014).`  
结果如表6所示。到ILSVRC提交时，我们只训练了单尺度网络，以及多尺度模型D（通过只微调全连接的层，而不是所有的层）。7个网络的集成有7.3%的ILSVRC测试误差。提交后，我们只考虑了两个表现最好的多尺度模型(配置D和E)的集合，使用密集评估将测试误差降低到7.0%，使用密集和多作物联合评估将测试误差降低到6.8%。作为参考，我们表现最好的单个模型达到了7.1%的误差(模型E，表5) ![](https://i-blog.csdnimg.cn/blog_migrate/d765be9c6abfbed69bf7d5b1603d7a7d.png)

### 精读 

#### 方法 

到目前为止，我们只计算了一个ConvNet模型的性能。在实验部分，我们通过平均几个模型的soft-max类的后验概率来结合输出。由于模型之间的互补，这种方法提高了性能。

#### 结果 

结果展示在表6中。到ILSVRC提交的时间，我们仅仅训练了一个单一尺度的网络，和一个多尺度的模型D（仅仅通过微调了全连接层而不是所有层）。

#### 结论 

1.结合多个模型，最后的结果通过softmax平均再结合判断，由于模型之间的互补性质，这会提高他们的性能。

2.融合之后网络的错误率要比单个网络的错误率低0.几个百分点。

3.使用multi-crop＋dense结合的方法会使得效果更佳。

### 4.6Comparision With The State of The Art—与最新技术的比较 

（State of the art（SOTA）——当前最佳性能/技术/算法，论文中会经常看到）

### 翻译 

最后，我们将我们的结果与表7中的最新技术水平进行了比较。在ILSVRC-2014挑战的分类任务中，我们的“VGG”团队使用7个模型的集合，以7.3%的测试误差获得了第二名。提交后，我们将使用2个模型的集合错误率降低到 6.8%。

![](https://i-blog.csdnimg.cn/blog_migrate/5fa80daeb9fd78ea7f8e98f357c1194c.png)

从表7可以看出，我们非常深的ConvNets的表现明显优于上一代的模型，在ILSVRC-2012和ILSVRC-2013的比赛中取得了最好的效果。我们的结果也与分类任务获胜者（`GoogLeNet with 6.7% error)`）具有竞争力，大大优于ILSVRC-2013获奖提交的clarfai，在使用外部训练数据达到11.2%，在没有训练数据的情况下达到11.7%。这是值得注意的，因为我们的最佳结果是通过结合两个模型——明显少于在大多数ILSVRC提交中使用的模型。在单网性能方面，我们的架构获得了最佳的结果（7.0%的测试错误），比单个GoogLeNet高出0.9%。值得注意的是，我们并没有背离LeCun等人（1989）的经典ConvNet架构，而是通过大幅增加深度而改进了它。

### 精读  

这部分不重要，作者就是想说我们的模型就是强，你们是弟弟，即使第一名的Googlenet，我们也只和他们差一点点，在雷式对比法中甚至还强些~

## 五、Conclusion—结论 

### 翻译 

在这项工作中，我们评估了非常深的卷积网络（多达19个层）的大尺度图像分类。结果表明，表示深度有利于分类精度，并且可以使用传统的ConvNet大幅增加深度 挑战数据集上实现最先进的性能。在附录中，我们还展示了我们的模型可以很好地推广到广泛的任务和数据集，匹配或优于围绕较不深的图像表示构建的更复杂的识别管道。我们的研究结果再次证实了深度在视觉表征中的重要性。

### 精读 

1.在这次的工作中，我们评估了大尺度图像分类汇总非常深的卷积神经网络的作用，他展示了模型的深度对于分类准确率的重要性。

2.同时在附录中，我们也展示了我们的模型更广范围的任务以及数据集上得到一个很好的泛化效果。

3.最后，我们的结论再一次证明了深度在视觉表征中的重要性

## 论文十问 

> Q1：论文试图解决什么问题？
> 
> 本文研究了在大规模图像识别中，卷积网络深度对其识别精度的影响。

> Q2：这是否是一个新的问题？
> 
> 不算，因为它是在Alex模型基础上增加了深度。

> Q3：这篇文章要验证一个什么科学假设？
> 
> 证明使用更小的卷积核并且增加卷积神经网络的深度，可以更有效地提升模型的性能。

> Q4：有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？
> 
> AlexNet，都属于图像识别，提高精度的问题

> Q5：论文中提到的解决方案之关键是什么？
> 
> 1.用三个3×3的卷积核代替7×7的卷积核，有的FC层还用到了1×1的卷积核。以及2×2的池化层。
> 
> 2.在更深的结构中没有用到LRN（推翻了Alex），避免了部分内存和计算的增加。
> 
> 3.使用了密集型训练方法(全连接->卷积层)，可适应各种尺寸的图片。
> 
> 4.将图像分为多个GPU批次，在多个GPU上进行训练，节省了时间。
> 
> 5.进行分类试验。

> Q6：论文中的实验是如何设计的？
> 
> 1.采用ImageNet2012数据集
> 
> 2.Single Scale Evaluation—单尺度评价
> 
> 3.Multi Scale Evaluation—多尺度评价
> 
> 4.Multi Crop Evaluation—多剪裁评价
> 
> 5.ConvNet Fusion—ConvNet融合

> Q7：用于定量评估的数据集是什么？代码有没有开源？
> 
> ImageNet2012，代码有开源

> Q8：论文中的实验及结果有没有很好地支持需要验证的科学假设？
> 
> 论文实验结果在ILSVRC-2014挑战赛的分类任务中，我们的“VGG”团队使用7个模型的融合用7.3%的测试错误率达得到了第2。提交之后，我们使用2个模型的融合将错误率减少到了6.8%。足以证明增加深度对精度识别很重要。

> Q9：这篇论文到底有什么贡献？
> 
> 我们的主要贡献是使用具有非常小（3 ×3）卷积核的架构对于增加了深度的网络的全面评估，这表明将通过将深度推到16和19个权重层可以实现对现有技术配置的显著改进。

> Q10：下一步呢？有什么工作可以继续深入？
> 
> 1.VGG在Alex基础上将卷积核都替换为了1×1及3×3的小卷积核以减少参数计算量，虽然 VGG减少了卷积层参数，但实际上其参数空间比 Alex大，其中绝大多数的参数都是来自于第一个FC，耗费更多计算资源。
> 
> 2.VGG模型比较简单，可以以它为基础进行改进。

VGG在深度学习的历史上是很有意义的，它的结构简单易懂，在当时仅次于大名鼎鼎的GoogLeNet，证明了神经网络更深表现会更好，最重要的是 VGG 向世人证明了更小的卷积核尺寸的重要性。 本篇VGG论文解读就到此为止了，欢迎大家留言交流呀~

下篇预告：没错，就是在ImageNet 2014图像分类竞赛中打败VGG，夺得第一名——大名鼎鼎的GoogLeNet！


[Link 1]: https://so.csdn.net/so/search?q=%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&spm=1001.2101.3001.7020
[AlexNet]: https://blog.csdn.net/weixin_43334693/article/details/128127653?spm=1001.2014.3001.5502
[paper_VGG.pdf at main _ shitbro6_paper _ GitHub]: https://github.com/shitbro6/paper/blob/main/VGG.pdf
[Link 2]: #%E5%89%8D%E8%A8%80%C2%A0
[Abstract-]: #Abstract-%E6%91%98%E8%A6%81
[Introduction]: #%E4%B8%80%E3%80%81Introduction%E2%80%94%E4%BB%8B%E7%BB%8D
[ConvNet Configurations_ConvNet]: #%E4%BA%8C%E3%80%81ConvNet%20Configurations%E2%80%94ConvNet%E9%85%8D%E7%BD%AE
[2.1Architecture]: #2.1Architecture%E2%80%94%E7%BB%93%E6%9E%84
[2.2Configurations]: #2.2Configurations%E2%80%94%E9%85%8D%E7%BD%AE
[2.3Discussion]: #2.3Discussion%E2%80%94%E8%AE%A8%E8%AE%BA
[Classification Framework]: #%E4%B8%89%E3%80%81Classification%20Framework%E2%80%94%E5%88%86%E7%B1%BB%E6%A1%86%E6%9E%B6
[3.1Training]: #%C2%A0%20%C2%A03.1Training%E2%80%94%E8%AE%AD%E7%BB%83
[3.2Testing]: #%C2%A03.2Testing%E2%80%94%E6%B5%8B%E8%AF%95
[3.3Implementation Details]: #3.3Implementation%20Details%E2%80%94%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82
[Classification Experiments_]: #%E5%9B%9B%E3%80%81Classification%20Experiments%E2%80%94%E5%88%86%E7%B1%BB%E5%AE%9E%E9%AA%8C%C2%A0
[4.1_ILSVRC-2012]: #%C2%A0%204.1%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%9AILSVRC-2012
[4.2Single Scale Evaluation_]: #%C2%A0%204.2Single%20Scale%20Evaluation%E2%80%94%E5%8D%95%E5%B0%BA%E5%BA%A6%E8%AF%84%E4%BB%B7%EF%BC%88%E6%B5%8B%E8%AF%95%E8%A7%92%E5%BA%A6%EF%BC%89%C2%A0
[4.3Multi Scale Evaluation]: #%C2%A0%204.3Multi%20Scale%20Evaluation%E2%80%94%E5%A4%9A%E5%B0%BA%E5%BA%A6%E8%AF%84%E4%BB%B7%EF%BC%88%E6%B5%8B%E8%AF%95%E8%A7%92%E5%BA%A6%EF%BC%89
[4.4Multi-Crop Evaluation]: #%C2%A04.4Multi-Crop%20Evaluation%E2%80%94%E5%A4%9A%E5%89%AA%E8%A3%81%E8%AF%84%E4%BB%B7%EF%BC%88%E6%B5%8B%E8%AF%95%E8%BE%93%E5%85%A5%E8%A7%92%E5%BA%A6%EF%BC%89
[4.5ConvNet Fusion_ConvNet]: #%C2%A04.5ConvNet%20Fusion%E2%80%94ConvNet%E8%9E%8D%E5%90%88
[4.6Comparision With The State of The Art]: #%C2%A04.6Comparision%20With%20The%20State%20of%20The%20Art%E2%80%94%E4%B8%8E%E6%9C%80%E6%96%B0%E6%8A%80%E6%9C%AF%E7%9A%84%E6%AF%94%E8%BE%83
[Conclusion]: #%E4%BA%94%E3%80%81Conclusion%E2%80%94%E7%BB%93%E8%AE%BA
[Link 3]: #%E8%AE%BA%E6%96%87%E5%8D%81%E9%97%AE