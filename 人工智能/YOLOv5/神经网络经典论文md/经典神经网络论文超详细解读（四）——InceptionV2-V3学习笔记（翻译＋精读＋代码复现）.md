## ![](https://i-blog.csdnimg.cn/blog_migrate/9b4413670dcfe63834bcb508f1e3448c.jpeg) 

## 前言 

上一篇我们介绍了Inception的原始版本和V1版本：[经典神经网络论文超详细解读（三）——GoogLeNet学习笔记（翻译＋精读+代码复现）][GoogLeNet]

这个结构在当时获得了第一名，备受关注。但InceptionV1是比较复杂的，于是作者潜力研究之后又推出了改进后的版本Inception V2-V3（《Rethinking the Inception Architecture for Computer Vision》），就像它的中文名字《 重新思考计算机视觉的初始架构》（大多数人认为这篇应该是InceptionV3， 将BN-Inception称为InceptionV2，不过我们今天既然读这篇，就按作者来了~）

本篇论文主要是改进了Inception模块，降低了计算量的同时，增加了模型的性能。具体怎样实现的，我们一起来看一下吧！

原文链接：[https://arxiv.org/pdf/1512.00567.pdf][https_arxiv.org_pdf_1512.00567.pdf]

前期回顾：

[经典神经网络论文超详细解读（一）——AlexNet学习笔记（翻译＋精读）][AlexNet]

[经典神经网络论文超详细解读（二）——VGGNet学习笔记（翻译＋精读）][VGGNet]

## 目录 

[前言][Link 1]

[ Abstract—摘要][Abstract]

[一、Introduction—介绍][Introduction]

[二、General Design Principles—通用设计原则][General Design Principles]

[三、Factorizing Convolutions with Large Filter Size—分解大卷积核的卷积][Factorizing Convolutions with Large Filter Size]

[3.1Factorization into smaller convolutions—分解成更小的卷积][3.1Factorization into smaller convolutions]

[3.2. Spatial Factorization into Asymmetric Convolutions—非对称分解卷积][3.2. Spatial Factorization into Asymmetric Convolutions]

[四、Utility of Auxiliary Classifiers—辅助分类器][Utility of Auxiliary Classifiers]

[五、Efficient Grid Size Reduction—高效下降特征图尺寸][Efficient Grid Size Reduction]

[六、Inception-v2][Inception-v2]

[七、Model Regularization via Label Smoothing（LSR）——通过标签平滑进行模型正则化][Model Regularization via Label Smoothing_LSR]

[八、Training Methodology—训练方法][Training Methodology]

[九、Performance on Lower Resolution Input—小分辨率输入性能][Performance on Lower Resolution Input]

[十、Experimental Results and Comparisons—实验结果和比较][Experimental Results and Comparisons]

[十一、Conclusions—结论][Conclusions]

[论文十问][Link 2]

## Abstract—摘要 

### 翻译 

对许多任务而言，卷积网络是目前最新的计算机视觉解决方案的核心。从2014年开始，深度卷积网络开始变成主流，在各种基准数据集上都取得了实质性成果。对于大多数任务而言，虽然增加的模型大小和计算成本都趋向于转化为直接的质量收益（只要提供足够的标注数据去训练），但计算效率和低参数计数仍是各种应用场景的限制因素，例如移动视觉和大数据场景。目前，我们正在探索增大网络的方法，目标是通过适当的分解卷积和积极的正则化来尽可能地有效利用增加的计算。我们在ILSVRC 2012分类挑战赛的验证集上评估了我们的方法，结果证明我们的方法超过了目前最先进的方法并取得了实质性收益：对于单一框架评估错误率为：21.2% top-1和5.6% top-5，使用的网络计算代价为每次推断需要进行50亿次乘加运算并使用不到2500万的参数。通过四个模型组合和多次评估，我们报告了3.5% top-5和17.3% top-1的错误率。

### 精读 

#### 主要内容 

背景：自2014年开始，网络变得又大又深，计算复杂度高，无法在移动场景(边缘计算)和大数据场景下使用。

目的：在加宽和加深网络的同时，我们要考虑计算的效率

引出本文主旨：探索了可分离卷积和正则化去提高计算效率

## 一、Introduction—介绍 

### 翻译 

从2012年Krizhevsky等人\[9\]赢得了ImageNet竞赛\[16\]起，他们的网络“AlexNet”已经成功了应用到了许多计算机视觉任务中，例如目标检测\[5\]，分割\[12\]，行人姿势评估\[22\]，视频分类\[8\]，目标跟踪\[23\]和超分辨率\[3\]。  
  这些成功推动了一个新研究领域，这个领域主要专注于寻找更高效运行的卷积神经网络。从2014年开始，通过利用更深更宽的网络，网络架构的质量得到了明显改善。VGGNet\[18\]和GoogLeNet\[20\]在2014 ILSVRC \[16\]分类挑战上取得了类似的高性能。一个有趣的发现是在分类性能上的收益趋向于转换成各种应用领域上的显著质量收益。这意味着深度卷积架构上的架构改进可以用来改善大多数越来越多地依赖于高质量、可学习视觉特征的其它计算机视觉任务的性能。网络质量的改善也导致了卷积网络在新领域的应用，在AlexNet特征不能与手工精心设计的解决方案竞争的情况下，例如，检测时的候选区域生成\[4\]。  
  尽管VGGNet\[18\]具有架构简洁的强有力特性，但它的成本很高：评估网络需要大量的计算。另一方面，GoogLeNet\[20\]的Inception架构也被设计为在内存和计算预算严格限制的情况下也能表现良好。例如，GoogleNet只使用了500万参数，与其前身AlexNet相比减少了12倍，AlexNet使用了6000万参数。此外，VGGNet使用了比AlexNet大约多3倍的参数。  
  Inception的计算成本也远低于VGGNet或其更高性能的后继者\[6\]。这使得可以在大数据场景中\[17\]，\[13\]，在大量数据需要以合理成本处理的情况下或在内存或计算能力固有地受限情况下，利用Inception网络变得可行，例如在移动视觉设定中。通过应用针对内存使用的专门解决方案\[2\]，\[15\]或通过计算技巧优化某些操作的执行\[10\]，可以减轻部分这些问题。但是这些方法增加了额外的复杂性。此外，这些方法也可以应用于优化Inception架构，再次扩大效率差距。  
  然而，Inception架构的复杂性使得更难以对网络进行更改。如果单纯地放大架构，大部分的计算收益可能会立即丢失。此外，\[20\]并没有提供关于导致GoogLeNet架构的各种设计决策的贡献因素的明确描述。这使得它更难以在适应新用例的同时保持其效率。例如，如果认为有必要增加一些Inception模型的能力，将滤波器组大小的数量加倍的简单变换将导致计算成本和参数数量增加4倍。这在许多实际情况下可能会被证明是禁止或不合理的，尤其是在相关收益适中的情况下。在本文中，我们从描述一些一般原则和优化思想开始，对于以有效的方式扩展卷积网络来说，这被证实是有用的。虽然我们的原则不局限于Inception类型的网络，但是在这种情况下，它们更容易观察，因为Inception类型构建块的通用结构足够灵活，可以自然地合并这些约束。这通过大量使用降维和Inception模块的并行结构来实现，这允许减轻结构变化对邻近组件的影响。但是，对于这样做需要谨慎，因为应该遵守一些指导原则来保持模型的高质量。

### 精读 

#### 主要内容 

（1）深度卷积网络性能的提升可以应用在其它计算机视觉任务中，这些任务有着共同点：它们都依赖学习到的高质量视觉特征（visual features）。

（2）卷积网络性能的提升会产生新的应用领域，比如AlexNet features 无法与手工工程相比，比如目标检测中候选框的生成。

（3）在参数上对比其他模型展现GoogLeNet的优势(参数量Alexnet：6000W，GoogLeNet：500W，VGG16：1.3E)

（4）一味的堆叠Inception模块将使得计算量爆炸，换来的精准度并不划算。

这篇论文中，先会介绍通用设计原则和一些优化的思想，然后再探索新的Inception结构

## 二、General Design Principles—通用设计原则 

### 翻译 

这里我们将介绍一些具有卷积网络的、具有各种架构选择的、基于大规模实验的设计原则。在这一点上，以下原则的效用是推测性的，另外将来的实验证据将对于评估其准确性和有效领域是必要的。然而，严重偏移这些原则往往会导致网络质量的恶化，修正检测到的这些偏差状况通常会导致改进的架构。  
1.避免表征瓶颈，尤其是在网络的前面。前馈网络可以由从输入层到分类器或回归器的非循环图表示。这为信息流定义了一个明确的方向。对于分离输入输出的任何切口，可以访问通过切口的信息量。应该避免极端压缩的瓶颈。一般来说，在达到用于着手任务的最终表示之前，表示大小应该从输入到输出缓慢减小。理论上，信息内容不能仅通过表示的维度来评估，因为它丢弃了诸如相关结构的重要因素；维度仅提供信息内容的粗略估计。  
2.更高维度的表示在网络中更容易局部处理。在卷积网络中增加每个图块的激活允许更多解耦的特征。所产生的网络将训练更快。  
3.空间聚合可以在较低维度嵌入上完成，而不会在表示能力上造成许多或任何损失。例如，在执行更多展开（例如3×3）卷积之前，可以在空间聚合之前减小输入表示的维度，没有预期的严重不利影响。我们假设，如果在空间聚合上下文中使用输出，则相邻单元之间的强相关性会导致维度缩减期间的信息损失少得多。鉴于这些信号应该易于压缩，因此尺寸减小甚至会促进更快的学习。  
4.平衡网络的宽度和深度。通过平衡每个阶段的滤波器数量和网络的深度可以达到网络的最佳性能。增加网络的宽度和深度可以有助于更高质量的网络。然而，如果两者并行增加，则可以达到恒定计算量的最佳改进。因此，计算预算应该在网络的深度和宽度之间以平衡方式进行分配。  
  虽然这些原则可能是有意义的，但并不是开箱即用的直接使用它们来提高网络质量。我们的想法是仅在不明确的情况下才明智地使用它们。

### 精读 

这一章主要是介绍了作者想到的四种设计原则，论文中说道，这几种设计原则虽然没有严格的证明或者实验加持，但你要大致上遵守，如果你背离这几个原则太多，则必然会造成较差的实验结果。

#### 原则一：要避免过度的降维和压缩特征导致特征表达瓶颈，特别是在网络的浅层。 

做法：feature map 长宽大小应随网络加深缓慢减小（不能猛减）。

原因：过度的降维或者收缩特征将造成一定程度的信息丢失（信息相关性丢失）

> 为何特别是网络的浅层？
> 
> 因为在网络的浅层丢失的原图信息还不是很多，仍然保留信息的稀疏性。如果在浅层就进行过度地压缩和降维，会对后面提取特征等工作是有负面影响的。

#### 原则二：特征越多，收敛越快 

理解：这里的特征多不是说特征图像多，而是说相互独立的特征多，特征分解的彻底。

举例：人脸特征分解成人脸、人左眼、人右眼、鼻子、嘴巴、眉毛等等独立特征会比单纯的一张大脸特征收敛的快。（赫布原理）

#### 原则三：大卷积核的卷积之前可以先降维，这个操作并不会造成太多的损失。 

原因：InceptionV1中使用的模块里有 3×3和5×5的卷积，并且在他们之前有使用1×1的卷积，这样的降维操作对于邻近单元的强相关性中损失的特征信息很少。

举例：每一层输出的feature map上每一个相邻的像素，它们的感受野是相邻且有一部分是重合的，即它们高度相关；若将它们进行1x1卷积后，特征信息是可以得到保证，因为1x1卷积后的feature map是能够实现跨通道的信息交融。

#### 原则四：均衡网络中的深度和宽度。 

 *  深度就是指层数的多少，宽度指每一层中卷积核的个数，也就是提取到的特征数。
 *  让深度和宽度并行提升，成比例的提升，能提高性能和计算效率。
 *  可以让计算量在每一层上均匀分配，VGG那第一层全连接层的数据堆积就属于不均匀分配的情况。

##  

## 三、Factorizing Convolutions with Large Filter Size—分解大卷积核的卷积 

### 翻译 

  GoogLeNet网络\[20\]的大部分初始收益来源于大量地使用降维。这可以被视为以计算有效的方式分解卷积的特例。考虑例如1×1卷积层之后接一个3×3卷积层的情况。在视觉网络中，预期相近激活的输出是高度相关的。因此，我们可以预期，它们的激活可以在聚合之前被减少，并且这应该会导致类似的富有表现力的局部表示。  
  在这里，我们将在各种设定中探索卷积分解的其它方法，特别是为了提高解决方案的计算效率。由于Inception网络是全卷积的，每个权重对应每个激活的一次乘法。因此，任何计算成本的降低会导致参数数量减少。这意味着，通过适当的分解，我们可以得到更多的解耦参数，从而加快训练。此外，我们可以使用计算和内存节省来增加我们网络的滤波器组的大小，同时保持我们在单个计算机上训练每个模型副本的能力。

### 精读  

#### 启发 

Googlenet的inception成功原因大部分得益于使用1 ×1 的卷积核，1 ×1 卷积核可以看作一个特殊的大卷积核分解过程，它损失少，大大降低计算量，增加非线性，跨通道交流。

#### 原因 

由上面的原则三，在使用大卷积核之前先用1 × 1卷积核，feature map信息不会丢失很多，因为感受野相邻的元素相关性很强，他们在卷积的过程中重合度很高，仅相差一个步长，所以feature map基本上不会丢失特征信息(元素特征相关性不变)。

#### 探究方法： 

在这篇文章中，作者探究了不同设置下的分解卷积。通过分解卷积在减少参数数量的同时可以得到更多的解耦特征，加快网络的训练。同时，节省下来的空间、计算资源可以用于进一步增加卷积核的尺寸，以达到代价基本不变下的更高性能。

### 3.1Factorization into smaller convolutions—分解成更小的卷积 

### 翻译 

 具有较大空间滤波器（例如5×5或7×7）的卷积在计算方面往往不成比例地昂贵。例如，具有n个滤波器的5×5卷积在具有m个滤波器的网格上比具有相同数量的滤波器的3×3卷积的计算量高25/9=2.78倍。当然，5×5滤波器在更前面的层可以捕获更远的单元激活之间、信号之间的依赖关系，因此滤波器几何尺寸的减小带来了很大的表现力。然而，我们可以询问5×5卷积是否可以被具有相同输入尺寸和输出深度的参数较小的多层网络所取代。如果我们放大5×5卷积的计算图，我们看到每个输出看起来像一个小的完全连接的网络，在其输入上滑过5×5的块（见图1）。由于我们正在构建视觉网络，所以通过两层的卷积结构再次利用平移不变性来代替全连接的组件似乎是很自然的：第一层是3×3卷积，第二层是在第一层的3×3输出网格之上的一个全连接层（见图1）。通过在输入激活网格上滑动这个小网络，用两层3×3卷积来替换5×5卷积（比较图4和5）。  
该设定通过相邻块之间共享权重明显减少了参数数量。为了分析预期的计算成本节省，我们将对典型的情况进行一些简单的假设：我们可以假设n=αm，也就是我们想通过常数α因子来改变激活/单元的数量。由于5×5卷积是聚合的，α通常比1略大（在GoogLeNet中大约是1.5）。用两个层替换5×5层，似乎可以通过两个步骤来实现扩展：在两个步骤中通过√α增加滤波器数量。为了简化我们的估计，通过选择α=1（无扩展），如果我们单纯地滑动网络而不重新使用相邻网格图块之间的计算，我们将增加计算成本。滑动该网络可以由两个3×3的卷积层表示，其重用相邻图块之间的激活。这样，我们最终得到一个计算量减少到(9+9)/25的网络，通过这种分解导致了28％的相对增益。每个参数在每个单元的激活计算中只使用一次，所以参数计数具有完全相同的节约。不过，这个设置提出了两个一般性的问题：这种替换是否会导致任何表征力的丧失？如果我们的主要目标是对计算的线性部分进行分解，是不是建议在第一层保持线性激活？我们已经进行了几个控制实验（例如参见图2），并且在分解的所有阶段中使用线性激活总是逊于使用修正线性单元。我们将这个收益归因于网络可以学习的增强的空间变化，特别是如果我们对输出激活进行批标准化\[7\]。当对维度减小组件使用线性激活时，可以看到类似的效果。  
图2。两个Inception模型间几个控制实验中的一个，其中一个分解为线性层+ ReLU层，另一个使用两个ReLU层。在三亿八千六百万次运算后，在验证集上前者达到了76.2% top-1准确率，后者达到了77.2% top-1的准确率。

### 精读 

#### 方法 

将一个5×5的卷积换成两个3×3卷积，同理7×7卷积可以用3个3×3卷积。

#### 结果 

相同卷积核个数核feature map尺寸的情况下，5 × 5 卷积核比3 ×3卷积核计算量高了2.78倍。

#### 原因 

相邻感受野的权值共享（共享卷积核，即卷积核内的数值都是一样的，用同一个卷积核），所以减少了很多计算量。

![](https://i-blog.csdnimg.cn/blog_migrate/44e5c5a3d7f51e93d89a86e0affff3f5.png)

![](https://i-blog.csdnimg.cn/blog_migrate/26ed78357f104c00048167ea2af27ce9.png)

> 问题一：这种替代会影响网络的表达能力吗？
> 
> 直观的看是可行的，从结果看也是可行的。但是要问严谨的数学原理，确实难以解释。
> 
> 问题二：如果我们的目标是分解计算的线性部分，是否会建议保留第一层的线性激活性？
> 
> 对于分解后的激活函数，作者通过实验证明，保留对于原图的第一次3 ×3卷积的激活函数有较好效果（一层卷积变成两层了，增加了非线性变换，增强模型非线性表达能力），用BN后效果更好。

![](https://i-blog.csdnimg.cn/blog_migrate/076b13246eb6025d2761885a5edecf29.png)

### 3.2. Spatial Factorization into Asymmetric Convolutions—非对称分解卷积 

### 翻译 

上述结果表明，大于3×3的卷积滤波器可能不是通常有用的，因为它们总是可以简化为3×3卷积层序列。我们仍然可以问这个问题，是否应该把它们分解成更小的，例如2×2的卷积。然而，通过使用非对称卷积，可以做出甚至比2×2更好的效果，即n×1。例如使用3×1卷积后接一个1×3卷积，相当于以与3×3卷积相同的感受野滑动两层网络（参见图3）。如果输入和输出滤波器的数量相等，那么对于相同数量的输出滤波器，两层解决方案便宜33％。相比之下，将3×3卷积分解为两个2×2卷积表示仅节省了11％的计算量。  
图3替换3×3卷积的Mini网络。网络的更低层由带有3个输出单元的3×1构成。  
  在理论上，我们可以进一步论证，可以通过1×n卷积和后面接一个n×1卷积替换任何n×n卷积，并且随着n增长，计算成本节省显著增加（见图6）。实际上，我们发现，采用这种分解在前面的层次上不能很好地工作，但是对于中等网格尺寸（在m×m特征图上，其中m范围在12到20之间），其给出了非常好的结果。在这个水平上，通过使用1×7卷积，然后是7×1卷积可以获得非常好的结果。

图6n×n卷积分解后的Inception模块。在我们提出的架构中，对17×17的网格我们选择n=7。（滤波器尺寸可以通过原则3选择）

### 精读 

#### 方法 

将3x3卷积可分解为1x3和3x1两个不对称卷积（空间可分离卷积）

![](https://i-blog.csdnimg.cn/blog_migrate/acc49adb933f6a8e32c3f3d50e88b101.png)

假设原始为3×3的图，使用3× 1的卷积核去卷积，得到一个feature map，然后再用1× 3的卷积核对刚才得到的feature map做卷积得到1×1的feature map。

#### 结果 

在输入和输出等同的情况下，参数降低33%（将3x3卷积核分解为两个2x2卷积核，只是降低了11%）

#### 结论 

（1）这种分解 (n ×n 分解成了 n×1 和1 ×n) ，n 越大节省的运算量越大。

（2）这种分解在前面的层效果不好，使用feature map大小在12-20之间。

#### 变种 

可以理解成上面这个是在深度上分解，而下面这个是在宽度上分解。 应用在最后的输出分类层之前，用该模块扩展特征维度生成高维稀疏特征（增加特征个数，符合原则二）。

![](https://i-blog.csdnimg.cn/blog_migrate/1bcab8b002582625b99dacfda4492987.png)

## 四、Utility of Auxiliary Classifiers—辅助分类器 

### 翻译 

  \[20\]引入了辅助分类器的概念，以改善非常深的网络的收敛。最初的动机是将有用的梯度推向较低层，使其立即有用，并通过抵抗非常深的网络中的消失梯度问题来提高训练过程中的收敛。Lee等人\[11\]也认为辅助分类器促进了更稳定的学习和更好的收敛。有趣的是，我们发现辅助分类器在训练早期并没有导致改善收敛：在两个模型达到高精度之前，有无侧边网络的训练进度看起来几乎相同。接近训练结束，辅助分支网络开始超越没有任何分支的网络的准确性，达到了更高的稳定水平。  
  另外，\[20\]在网络的不同阶段使用了两个侧分支。移除更下面的辅助分支对网络的最终质量没有任何不利影响。再加上前一段的观察结果，这意味着\[20\]最初的假设，这些分支有助于演变低级特征很可能是不适当的。相反，我们认为辅助分类器起着正则化项的作用。这是由于

### 精读 

#### 较之前的改进 

在GoogLeNet里面用了两个辅助分类器（4a和4b两个模块后面），但是事后实验证明，辅助分类器并未在训练初期改善收敛性，第一个没什么用，在v2，v3里面去掉了。

## 五、Efficient Grid Size Reduction—高效下降特征图尺寸 

### 翻译 

传统上，卷积网络使用一些池化操作来缩减特征图的网格大小。为了避免表示瓶颈，在应用最大池化或平均池化之前，需要扩展网络滤波器的激活维度。例如，开始有一个带有k个滤波器的d×d网格，如果我们想要达到一个带有2k个滤波器的![\frac{d}{2}\times \frac{d}{2}](https://latex.csdn.net/eq?%5Cfrac%7Bd%7D%7B2%7D%5Ctimes%20%5Cfrac%7Bd%7D%7B2%7D)网格，我们首先需要用2k个滤波器计算步长为1的卷积，然后应用一个额外的池化步骤。这意味着总体计算成本由在较大的网格上使用![2d^{2}k^{2}](https://latex.csdn.net/eq?2d%5E%7B2%7Dk%5E%7B2%7D)次运算的昂贵卷积支配。一种可能性是转换为带有卷积的池化，因此导致![2(\frac{d}{2})^{2}k^{2}](https://latex.csdn.net/eq?2%28%5Cfrac%7Bd%7D%7B2%7D%29%5E%7B2%7Dk%5E%7B2%7D)次运算，将计算成本降低为原来的四分之一。然而，由于表示的整体维度下降到![(\frac{d}{2})^{2} k](https://latex.csdn.net/eq?%28%5Cfrac%7Bd%7D%7B2%7D%29%5E%7B2%7D%20k)会导致表示能力较弱的网络（参见图9），这会产生一个表示瓶颈。我们建议另一种变体，其甚至进一步降低了计算成本，同时消除了表示瓶颈（见图10），而不是这样做。我们可以使用两个平行的步长为2的块：P和C。P是一个池化层（平均池化或最大池化）的激活，两者都是步长为2，其滤波器组连接如图10所示。

图9。减少网格尺寸的两种替代方式。左边的解决方案违反了第2节中不引入表示瓶颈的原则1。右边的版本计算量昂贵3倍。

图10。缩减网格尺寸的同时扩展滤波器组的Inception模块。它不仅廉价并且避免了原则1中提出的表示瓶颈。右侧的图表示相同的解决方案，但是从网格大小而不是运算的角度来看。  


### 精读 

#### 传统降维方法 

方法一：先对feature map池化会导致表征瓶颈，信息量丢失很多；（池化->卷积）

方法二：信息保留下了，但计算量比较大； （卷积->池化）

![](https://i-blog.csdnimg.cn/blog_migrate/f63b95a7e505f98886dbfdccaf1079e5.png)

#### 计算量比较 

![](https://i-blog.csdnimg.cn/blog_migrate/4e754d854fac925a69b0fd0901156f3b.png)

(图片来源于b站：同济子豪兄，下同）

#### 本文改进 

并行执行（卷积C+池化P），再进行feature map的堆叠。

inception并行模块结构图: 左边两路卷积，右边池化。最后再叠加，可以在不丢失信息的情况下减小参数量。

![](https://i-blog.csdnimg.cn/blog_migrate/88619d5b2355d2714af5945688471fd3.png)

## 六、Inception-v2 

### 翻译 

在这里，我们连接上面的点，并提出了一个新的架构，在ILSVRC 2012分类基准数据集上提高了性能。我们的网络布局在表1中给出。注意，基于与3.1节中描述的同样想法，我们将传统的7×7卷积分解为3个3×3卷积。对于网络的Inception部分，我们在35×35处有3个传统的Inception模块，每个模块有288个滤波器。使用第5节中描述的网格缩减技术，这将缩减为17×17的网格，具有768个滤波器。这之后是图5所示的5个分解的Inception模块实例。使用图10所示的网格缩减技术，这被缩减为8×8×1280的网格。在最粗糙的8×8级别，我们有两个如图6所示的Inception模块，每个块连接的输出滤波器组的大小为2048。网络的详细结构，包括Inception模块内滤波器组的大小，在补充材料中给出，在提交的tar文件中的model.txt中给出。然而，我们已经观察到，只要遵守第2节的原则，对于各种变化网络的质量就相对稳定。虽然我们的网络深度是42层，但我们的计算成本仅比GoogLeNet高出约2.5倍，它仍比VGGNet要高效的多。  
表1。提出的网络架构的轮廓。每个模块的输出大小是下一模块的输入大小。我们正在使用图10所示的缩减技术的变种，以缩减应用时Inception块间的网格大小。我们用0填充标记了卷积，用于保持网格大小。这些Inception模块内部也使用0填充，不会减小网格大小。所有其它层不使用填充。选择各种滤波器组大小来观察第2节的原理4。  
图7。具有扩展的滤波器组输出的Inception模块。这种架构被用于最粗糙的（8×8）网格，以提升高维表示，如第2节原则2所建议的那样。我们仅在最粗的网格上使用了此解决方案，因为这是产生高维度的地方，稀疏表示是最重要的，因为与空间聚合相比，局部处理（1×1 卷积）的比率增加。

图8。最后17×17层之上的辅助分类器。 侧头中的层的批标准化\[7\]导致top-1 0.4％的绝对收益。下轴显示执行的迭代次数，每个批次大小为32。

### 精读 

作者基于以上的分析与所提出的通用性准则提出了改进后的Inception-v2架构：

![](https://i-blog.csdnimg.cn/blog_migrate/00f0dee97091cb77c3ec4f0be0ad6813.png)

 *  figure5 ——把5×5卷积用两个3×3卷积代替
 *  figure6 ——应用的那个不对称卷积
 *  figure7 ——用在输出分类前的那个扩展组合

相比起Inception-v1结构，这里将开始的7x7卷积使用3个3x3卷积替代，在后面的Inception模块中分别使用了三种不同的模式：

第一部分输入的特征图尺寸为35x35x288,采用了图5中的架构，将5x5以两个3x3代替。

第二部分输入特征图尺寸为17x17x768，采用了图6中nx1+1xn的结构

第三部分输入特征图尺寸为8x8x1280, 采用了图7中所示的并行模块的结构

#### 结果 

这个网络是符合上面说的设计的四大原则的， 网络有42层深，计算量是googlenet的2.5倍(仍比VGG高效)。

## 七、Model Regularization via Label Smoothing（LSR）——通过标签平滑进行模型正则化 

### 翻译 

在这里，我们提出了一种机制，通过估计训练过程中标签丢失的边缘化作用来对分类器层进行正则化。

 对于每个训练样本x，我们的模型计算每个标签k∈\{1…K\}的概率：![p(k|x)=\frac{exp(z_{k})}{\sum_{i=1}^{k}exp(z_{i})}](https://latex.csdn.net/eq?p%28k%7Cx%29%3D%5Cfrac%7Bexp%28z_%7Bk%7D%29%7D%7B%5Csum_%7Bi%3D1%7D%5E%7Bk%7Dexp%28z_%7Bi%7D%29%7D)。其中zi是对数或未归一化的对数概率。在本训练样本中，考虑标签q(k|x)上的真实分布，因为归一化有![\sum_{k}^{} q(k|x)=1](https://latex.csdn.net/eq?%5Csum_%7Bk%7D%5E%7B%7D%20q%28k%7Cx%29%3D1)。为简洁起见，我们省略p和q对样本x的依赖关系。我们将样本的损失定义为交叉熵：![\pounds =-\sum_{k=1}^{k}\log (p(k))q(k)](https://latex.csdn.net/eq?%5Cpounds%20%3D-%5Csum_%7Bk%3D1%7D%5E%7Bk%7D%5Clog%20%28p%28k%29%29q%28k%29)。将其最小化等效于最大化标签的期望对数似然，其中标签是根据真实分布q(k)选择的。交叉熵损失相对于对数zk是可微的，因此可以用于深度模型的梯度训练。梯度有一个相当简单的形式：![\frac{\partial \pounds }{\partial z_{k}}=p(k)-q(k)](https://latex.csdn.net/eq?%5Cfrac%7B%5Cpartial%20%5Cpounds%20%7D%7B%5Cpartial%20z_%7Bk%7D%7D%3Dp%28k%29-q%28k%29)，其限定在-1和1之间。

 考虑单个真实标签y的情况，因此对于所有k ≠ y，q(y)=1且q(k)=0。在这种情况下，最小化交叉熵相当于最大化正确标签的对数似然。对于具有标签y的特定样本，对数似然最大化为![q(k)=\delta _{k,y}](https://latex.csdn.net/eq?q%28k%29%3D%5Cdelta%20_%7Bk%2Cy%7D)，其中![\delta _{k,y}](https://latex.csdn.net/eq?%5Cdelta%20_%7Bk%2Cy%7D)为狄拉克函数，k=y时等于1，否则为0。对于有限的zk，无法达到此最大值，但是对于所有k ≠ y 来说，如果![z_{y}](https://latex.csdn.net/eq?z_%7By%7D)≫![z_{k}](https://latex.csdn.net/eq?z_%7Bk%7D) ，可以接近最大值，即对应于真实标签的对数比所有其他对数大得多时，可以接近该最大值。然而，这可能导致两个问题。首先，它可能导致过拟合：如果模型学会为每个训练样本分配全部概率给groundtruth标签，则不能保证将其泛化能力。其次，它鼓励最大对数和所有其他对数之间的差异变大，这与有界梯度![\frac{\partial\pounds }{\partial z_{k}}](https://latex.csdn.net/eq?%5Cfrac%7B%5Cpartial%5Cpounds%20%7D%7B%5Cpartial%20z_%7Bk%7D%7D)相结合，降低了模型的适应能力。直观地说，这是因为模型对其预测过于自信。

 我们提出了一个机制来鼓励模型少一点自信。如果目标是最大化训练标签的对数似然，这么做可能不是所期望的，但它确实使模型正则化并使其更具适应性。方法很简单。考虑独立于训练样本x的标签的分布u(k)和平滑参数ϵ 。对于真实标签为y的训练样本，我们将标签分布![q(k)=\delta _{k,y}](https://latex.csdn.net/eq?q%28k%29%3D%5Cdelta%20_%7Bk%2Cy%7D)替换![q{}'(k|x)=(1-\epsilon )\delta k_{y}+\epsilon u(k)](https://latex.csdn.net/eq?q%7B%7D%27%28k%7Cx%29%3D%281-%5Cepsilon%20%29%5Cdelta%20k_%7By%7D&plus;%5Cepsilon%20u%28k%29)它是原始真实分布q(k∣x)和固定分布u(k)的混合，权重分别为1−ϵ和ϵ。这可以看作是按如下方式获得的标签k的分布：首先，将其设置为真实标签k=y；然后，以概率q用从分布u(k)中抽取的样本替换k。我们建议使用标签上的先验分布作为u(k)。在我们的实验中，我们使用均匀分布u(k)=1/K，因此![q{}'(k|x)=(1-\epsilon )\delta k_{y}+\frac{\epsilon }{k}](https://latex.csdn.net/eq?q%7B%7D%27%28k%7Cx%29%3D%281-%5Cepsilon%20%29%5Cdelta%20k_%7By%7D&plus;%5Cfrac%7B%5Cepsilon%20%7D%7Bk%7D)我们将真实标签分布的这种变化称为标签平滑正则化或LSR。

 请注意，LSR实现了防止最大对数比其他所有对数大得多的预期目标。事实上，如果这种情况发生，那么一个q(k)将趋近于1，而其他所有q(k)将趋近于0。这将导致与q′(k)产生较大的交叉熵，因为与![q(k)=\delta _{k,y}](https://latex.csdn.net/eq?q%28k%29%3D%5Cdelta%20_%7Bk%2Cy%7D)不同，所有q′(k)都有一个正的下界。通过考虑交叉熵可以得到LSR的另一种解释：![H(q{}',p)=-\sum_{k=1}^{k}\log p(k)q{}'(k)=(1-\epsilon )H(q,p)+\epsilon H(u,p)](https://latex.csdn.net/eq?H%28q%7B%7D%27%2Cp%29%3D-%5Csum_%7Bk%3D1%7D%5E%7Bk%7D%5Clog%20p%28k%29q%7B%7D%27%28k%29%3D%281-%5Cepsilon%20%29H%28q%2Cp%29&plus;%5Cepsilon%20H%28u%2Cp%29)

因此，LSR相当于将一个单一的交叉熵损失H(q,p)替换为一对这样的损失H(q,p)和 H(u,p)。第二个损失惩罚了预测标签分布p与先验u的偏差，相对权重为![\frac{\epsilon }{1-\epsilon }](https://latex.csdn.net/eq?%5Cfrac%7B%5Cepsilon%20%7D%7B1-%5Cepsilon%20%7D)。注意，由于![H(u,p)=D_{K,L}(u ||p)+H(u)](https://latex.csdn.net/eq?H%28u%2Cp%29%3DD_%7BK%2CL%7D%28u%20%7C%7Cp%29&plus;H%28u%29)是固定的，因此可以用KL散度等效地捕获此偏差。当u是均匀分布时，H(u,p)是预测的分布p与均匀分布的不相似程度的度量，也可以用负熵−H(p)来度量（但不相等）；我们还没有尝试过这种方法。

 在我们使用K=1000类的ImageNet实验中，我们使用u(k)=1/1000和ϵ=0.1。对于ILSVRC2012，我们发现top-1错误率和top-5错误率都有0.2%的绝对改善（参见表3）。

### 精读 

(这一节非常复杂，涉及大量公式，本人水平有限就不再推理，这里只解释相关概念和算法效果。相关计算推导推荐大家看b站同济子豪兄)

> #### 相关问题 
> 
> Q1：one-hot是什么？
> 
> 独热编码即 One-Hot 编码，又称一位有效编码，其方法是使用N位状态寄存器来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候，其中只有一位有效。（简单地说，就是对于多分类向量，计算机中往往用\[0, 1, 3\]等此类离散的、随机的而非有序(连续)的向量表示，而one-hot vector 对应的向量便可表示为\[0, 1, 0\]，即对于长度为n 的数组，只有一个元素是1，其余都为0。因此表征我们已知样本属于某一类别的概率是为1的确定事件，属于其他类别的概率则均为0。）
> 
> Q2：one-hot缺点？
> 
> 1.过拟合：因为模型使得正确标签的概率足够大
> 
> 2.降低模型的泛化能力，降低了网络的适应性
> 
> Q3：LSR是什么？
> 
> 在深度学习样本训练的过程中，我们采用one-hot标签去进行计算交叉熵损失时，只考虑到训练样本中正确的标签位置（one-hot标签为1的位置）的损失，而忽略了错误标签位置（one-hot标签为0的位置）的损失。这样一来，模型可以在训练集上拟合的很好，但由于其他错误标签位置的损失没有计算，导致预测的时候，预测错误的概率增大。Label Smoothing Regularization（LSR）标签平滑正则化,机器学习中的一种正则化方法，是一种通过在输出y中添加噪声，实现对模型进行约束，降低模型过拟合（overfitting）程度的一种约束方法（regularization methed）。
> 
> Q4：LSR的实质？
> 
> 标签平滑的实质就是促使神经网络中进行softmax激活函数激活之后的分类概率结果向正确分类靠近，即正确的分类概率输出大（对应的one-hot标签为1位置的softmax概率大），并且同样尽可能的远离错误分类（对应的one-hot标签为0位置的softmax概率小），即错误的分类概率输出小。

#### Label smooth实现 

代码

```java
 new_labels = (1.0 - label_smoothing) * one_hot_labels + label_smoothing / num_classes
```

参数

label\_smoothing = 0.1

num\_classes = 1000

#### 结果 

Label smooth提高了网络精度0.2%

## 八、Training Methodology—训练方法 

### 翻译 

我们在TensorFlow\[1\]分布式机器学习系统上使用随机梯度方法训练了我们的网络，使用了50个副本，每个副本在一个NVidia Kepler GPU上运行，批处理大小为32，100个epoch。我们之前的实验使用动量方法\[19\]，衰减值为0.9，而我们最好的模型是用RMSProp \[21\]实现的，衰减值为0.9，ϵ=1.0。我们使用0.045的学习率，每两个epoch以0.94的指数速率衰减。此外，阈值为2.0的梯度裁剪\[14\]被发现对于稳定训练是有用的。使用随时间计算的运行参数的平均值来执行模型评估。

### 精读 

最优模型的优化方法：RMSProp + learning rate decay(0.9) , 同时使用了阈值为2的梯度截断使得训练更加稳定。

#### 具体参数 

<table> 
 <tbody> 
  <tr> 
   <td><span>初始模型</span></td> 
   <td>tesnsorflow</td> 
  </tr> 
  <tr> 
   <td><span>优化器</span></td> 
   <td>stochastic gradient</td> 
  </tr> 
  <tr> 
   <td><span>机器个数</span></td> 
   <td>50</td> 
  </tr> 
  <tr> 
   <td><span>batch size</span></td> 
   <td>32</td> 
  </tr> 
  <tr> 
   <td><span>epoch</span></td> 
   <td>100</td> 
  </tr> 
  <tr> 
   <td><span>最好模型</span></td> 
   <td>RMSProp</td> 
  </tr> 
  <tr> 
   <td><span>decay</span></td> 
   <td>0.9</td> 
  </tr> 
  <tr> 
   <td><span>初始学习率</span></td> 
   <td>0.045</td> 
  </tr> 
  <tr> 
   <td><span>指数衰减</span></td> 
   <td>0.94</td> 
  </tr> 
 </tbody> 
</table>

> 相关知识
> 
> 1.优化算法之AdaGrad
> 
> 该优化算法与普通的sgd算法差别就在于标黄的那部分，采取了累积平方梯度。
> 
> ![](https://i-blog.csdnimg.cn/blog_migrate/32ebb41c6bdc21f598f01edb2a9b0f6a.png)
> 
>  作用：起到的效果是在参数空间更为平缓的方向，会取得更大的进步（因为平缓，所以历史梯度平方和较小，对应学习下降的幅度较小），并且能够使得陡峭的方向变得平缓，从而加快训练速度。
> 
> 2.优化算法之RMSProp
> 
> RMSProp算法不是像AdaGrad算法那样暴力直接的累加平方梯度，而是加了一个衰减系数来控制历史信息的获取多少。
> 
> ![](https://i-blog.csdnimg.cn/blog_migrate/ea5b2bbdd969a23798a06efbb5ae1769.png)
> 
>  作用：RMSProp算法是AdaGrad算法的改进。鉴于神经网络都是非凸条件下的，RMSProp在非凸条件下结果更好，改变梯度累积为指数衰减的移动平均以丢弃遥远的过去历史。

## 九、Performance on Lower Resolution Input—小分辨率输入性能 

### 翻译 

视觉网络的典型用例是用于检测的后期分类，例如在Multibox \[4\]上下文中。这包括分析在某个上下文中包含单个对象的相对较小的图像块。任务是确定图像块的中心部分是否对应某个对象，如果是，则确定该对象的类别。这个挑战的是对象往往比较小，分辨率低。这就提出了如何正确处理低分辨率输入的问题。  
  普遍的看法是，使用更高分辨率感受野的模型倾向于导致显著改进的识别性能。然而，区分第一层感受野分辨率增加的效果和较大的模型容量、计算量的效果是很重要的。如果我们只是改变输入的分辨率而不进一步调整模型，那么我们最终将使用计算上更便宜的模型来解决更困难的任务。当然，由于减少了计算量，这些解决方案很自然就出来了。为了做出准确的评估，模型需要分析模糊的提示，以便能够“幻化”细节。这在计算上是昂贵的。因此问题依然存在：如果计算量保持不变，更高的输入分辨率会有多少帮助。确保不断努力的一个简单方法是在较低分辨率输入的情况下减少前两层的步长，或者简单地移除网络的第一个池化层。为了这个目的我们进行了以下三个实验：

1.步长为2，大小为299×299的感受野和最大池化。  
2.步长为1，大小为151×151的感受野和最大池化。  
3.步长为1，大小为79×79的感受野和第一层之后没有池化。  
所有三个网络具有几乎相同的计算成本。虽然第三个网络稍微便宜一些，但是池化层的成本是无足轻重的（在总成本的1％以内）。在每种情况下，网络都进行了训练，直到收敛，并在ImageNet ILSVRC 2012分类基准数据集的验证集上衡量其质量。结果如表2所示。虽然分辨率较低的网络需要更长时间去训练，但最终结果却与较高分辨率网络的质量相当接近。表2。当感受野尺寸变化时，识别性能的比较，但计算代价是不变的。但是，如果只是单纯地按照输入分辨率减少网络尺寸，那么网络的性能就会差得多。然而，这将是一个不公平的比较，因为我们将在比较困难的任务上比较一个便宜16倍的模型。表2的这些结果也表明，有人可能会考虑在R-CNN \[5\]的上下文中对更小的对象使用专用的高成本低分辨率网络。

### 精读 

#### 目标检测难点 

图像中低分辨率的目标难以检测，如何处理低分辨率的输入？

#### 常规方法 

对这些使用更大感受野来进行卷积，以提高在低分辨率输入上的识别准确率。

#### 不足 

这很难说明性能的提升是来自于感受野的扩大还是模型能力及计算量的增加。

#### 引出问题 

在保持计算代价不变的情况下，如何最大限度的提高分辨率？

#### 方法 

保持模型复杂度不变，降低前两层的步长，或者移除第一个池化层。

#### 实验验证 

三组实验感受野逐步下降而计算量保持不变：

（1）79x79的感受野，stride = 1，不进行maxpooling

（2）151x151的感受野，strde = 1，加上maxpooling

（3）299x299的感受野，stride = 2，加上maxpooling

![](https://i-blog.csdnimg.cn/blog_migrate/3f17e2e1838b7ede11057f49228d122f.png)

 结论：实验表明虽然感受野增大，但是在保持计算量不变的情况下模型性能相差不大

## 十、Experimental Results and Comparisons—实验结果和比较 

### 翻译 

表3显示了我们提出的体系结构（Inception-v2）识别性能的实验结果，架构如第6节所述。每个Inception-v2行显示了累积变化的结果，包括突出显示的新修改加上所有先前修改的结果。标签平滑是指在第7节中描述的方法。分解的7×7包括将第一个7×7卷积层分解成3×3卷积层序列的改变。BN-auxiliary是指辅助分类器的全连接层也批标准化的版本，而不仅仅是卷积。我们将表3最后一行的模型称为Inception-v3，并在多裁剪图像和组合设置中评估其性能。我们所有的评估都在ILSVRC-2012验证集上的48238个非黑名单样本中完成，如\[16\]所示。我们也对所有50000个样本进行了评估，结果在top-5错误率中大约为0.1%，在top-1错误率中大约为0.2%。在本文即将出版的版本中，我们将在测试集上验证我们的组合结果，但是我们上一次对BN-Inception的春季测试\[7\]表明测试集和验证集错误趋于相关性很好。11. 结论我们提供了几个设计原则来扩展卷积网络，并在Inception体系结构的背景下进行研究。这个指导可以导致高性能的视觉网络，与更简单、更单一的体系结构相比，它具有相对适中的计算成本。Inception-v3的最高质量版本在ILSVR 2012分类上的单裁剪图像评估中达到了21.2\\％  
的top-1错误率和5.6\\％的top-5错误率，达到了新的水平。与Ioffe等\[7\]中描述的网络相比，这是通过增加相对适中（2.5/times  
）的计算成本来实现的。尽管如此，我们的解决方案所使用的计算量比基于更密集网络公布的最佳结果要少得多：我们的模型比He等\[6\]的结果更好——将top-5(top-1)的错误率相对分别减少了25% (14%)，然而在计算代价上便宜了六倍，并且使用了至少减少了五倍的参数（估计值）。我们的四个Inception-v3模型的组合效果达到了3.5\\％，多裁剪图像评估达到了3.5\\％  
的top-5的错误率，这相当于比最佳发布的结果减少了25\\％以上，几乎是ILSVRC 2014的冠军GoogLeNet组合错误率的一半。我们还表明，可以通过感受野分辨率为79×79的感受野取得高质量的结果。这可能证明在检测相对较小物体的系统中是有用的。我们已经研究了在神经网络中如何分解卷积和积极降维可以导致计算成本相对较低的网络，同时保持高质量。较低的参数数量、额外的正则化、批标准化的辅助分类器和标签平滑的组合允许在相对适中大小的训练集上训练高质量的网络。

### 精读 

#### 对Inception-v2进行改进 

（1）InceptionV2 加入RMSProp(一种计算梯度的方法)

（2）在上面的基础上加入Label Smoothing(LSR,标签平滑正则化)

（3）在上面的基础上再加入7×7的卷积核分解(分解成3×3)

（4）在上面的基础上再加入含有BN的辅助分类器

![](https://i-blog.csdnimg.cn/blog_migrate/6f79396fce3915f5cec99d3bd6b942b1.png)

所以本文最终提出的InceptionV3=inceptionV2+RMSProp优化+LSR+BN-auxilary

同时也将Inception-v3进行了多尺度的训练，在单模型下的对比结果如下:

![](https://i-blog.csdnimg.cn/blog_migrate/46d382136ba843eea384244fb3be3339.png)

进一步地，Inception-v3进行模型融合（ensemble)后与其他模型做对比：

![](https://i-blog.csdnimg.cn/blog_migrate/7560b687e406925cef602ee7996f324f.png)

结论：可以看到Inception-v3在分类任务上都获得了更好的性能

## 十一、Conclusions—结论 

### 翻译 

我们提供了几个设计原则来扩展卷积网络，并在Inception体系结构的背景下进行研究。这个指导可以导致高性能的视觉网络，与更简单、更单一的体系结构相比，它具有相对适中的计算成本。Inception-v3的最高质量版本在ILSVR 2012分类上的单裁剪图像评估中达到了21.2％的`top-1`错误率和5.6％的`top-5`错误率，达到了新的水平。与Ioffe等\[7\]中描述的网络相比，这是通过增加相对适中（2.5/times）的计算成本来实现的。尽管如此，我们的解决方案所使用的计算量比基于更密集网络公布的最佳结果要少得多：我们的模型比He等\[6\]的结果更好——将`top-5(top-1)`的错误率相对分别减少了25%(14%)，然而在计算代价上便宜了六倍，并且使用了至少减少了五倍的参数（估计值）。我们的四个Inception-v3模型的组合效果达到了3.5％，多裁剪图像评估达到了3.5％的`top-5`的错误率，这相当于比最佳发布的结果减少了25％以上，几乎是ILSVRC 2014的冠军GoogLeNet组合错误率的一半。

我们还表明，可以通过感受野分辨率为79×79的感受野取得高质量的结果。这可能证明在检测相对较小物体的系统中是有用的。我们已经研究了在神经网络中如何分解卷积和积极降维可以导致计算成本相对较低的网络，同时保持高质量。较低的参数数量、额外的正则化、标准化的辅助分类器和标签平滑的组合允许在相对适中大小的训练集上训练高质量的网络。

### 精读 

总结一句话：我们的模型很好！

## 论文十问: 

> Q1：论文试图解决什么问题？
> 
> 在加宽和加深网络的同时，探索了可分离卷积和正则化去提高计算效率，从而达到优化原有GooogLeNet模型，提高网络性能。

> Q2：这是否是一个新的问题？
> 
> 不是，是对原有InceptionV1的优化

> Q3：这篇文章要验证一个什么科学假设？
> 
> 探索了可分离卷积和正则化去提高计算效率

> Q4：有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？
> 
> 无

> Q5：论文中提到的解决方案之关键是什么？
> 
> 1.卷积核分解：（1）将大的卷积核分解成小的（2）非对称分解
> 
> 2.并行执行（卷积C+池化P），再进行feature map的堆叠
> 
> 3.标签平滑（LSR）进行模型正则化
> 
> 4.辅助分类器：在本文中，作者纠正了辅助分类器的作用，靠近输入辅助分类器（加不加没有影响），但是靠近输出的辅助分类器（加上BN后）可以起到正则化的作用。

> Q6：论文中的实验是如何设计的？
> 
> 在原有Inception-v2逐步改进，形成对照实验组，引出V3
> 
> Inception-v3进行了多尺度的训练，在单模型下的对比结果
> 
> Inception-v3进行模型融合（ensemble)后与其他模型做对比

> Q7：用于定量评估的数据集是什么？代码有没有开源？
> 
> ImageNet2012，有开源

> Q8：论文中的实验及结果有没有很好地支持需要验证的科学假设？
> 
> 有，通过对照实验证明了结论

> Q9：这篇论文到底有什么贡献？
> 
> 1.  改进原有的Inception-V1结构，使得CNN模型在分类任务上获得更高的性能
> 2.  提出了四大设计原则
> 3.  分解大filters，使其小型化、多层化，提出了“非对称卷积”
> 4.  优化inception v1的auxiliary classifiers，并造了新的带BN的辅助分类器
> 5.  演示了即使是低分辨率的输入也可以达到较好的识别效果
> 6.  使用新的 inception并行模块解决了传统采样层的问题
> 7.  inceptionV3 = inceptionV2+RMSProp+LSR+BN-aux

> Q10：下一步呢？有什么工作可以继续深入？
> 
> 1.  如何在计算量不增加的情况下，解决由于信息压缩造成的信息损失问题。
> 2.  如何在计算量不增加的情况下，增加模型的拓扑结构，以提高模型的表达能力。

代码复现：[GoogLeNet InceptionV3代码复现＋超详细注释（PyTorch）][GoogLeNet InceptionV3_PyTorch]

下期预告： ResNet


[GoogLeNet]: https://blog.csdn.net/weixin_43334693/article/details/128267380?spm=1001.2014.3001.5501
[https_arxiv.org_pdf_1512.00567.pdf]: https://arxiv.org/pdf/1512.00567.pdf
[AlexNet]: https://blog.csdn.net/weixin_43334693/article/details/128127653?spm=1001.2014.3001.5502
[VGGNet]: https://blog.csdn.net/weixin_43334693/article/details/128148803?spm=1001.2014.3001.5501
[Link 1]: #%E5%89%8D%E8%A8%80
[Abstract]: #%C2%A0Abstract%E2%80%94%E6%91%98%E8%A6%81
[Introduction]: #%E4%B8%80%E3%80%81Introduction%E2%80%94%E4%BB%8B%E7%BB%8D
[General Design Principles]: #%E4%BA%8C%E3%80%81General%20Design%20Principles%E2%80%94%E9%80%9A%E7%94%A8%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99
[Factorizing Convolutions with Large Filter Size]: #%E4%B8%89%E3%80%81Factorizing%20Convolutions%20with%20Large%20Filter%20Size%E2%80%94%E5%88%86%E8%A7%A3%E5%A4%A7%E5%8D%B7%E7%A7%AF%E6%A0%B8%E7%9A%84%E5%8D%B7%E7%A7%AF
[3.1Factorization into smaller convolutions]: #3.1Factorization%20into%20smaller%20convolutions%E2%80%94%E5%88%86%E8%A7%A3%E6%88%90%E6%9B%B4%E5%B0%8F%E7%9A%84%E5%8D%B7%E7%A7%AF
[3.2. Spatial Factorization into Asymmetric Convolutions]: #3.2.%20Spatial%20Factorization%20into%20Asymmetric%20Convolutions%E2%80%94%E9%9D%9E%E5%AF%B9%E7%A7%B0%E5%88%86%E8%A7%A3%E5%8D%B7%E7%A7%AF
[Utility of Auxiliary Classifiers]: #%E5%9B%9B%E3%80%81Utility%20of%20Auxiliary%20Classifiers%E2%80%94%E8%BE%85%E5%8A%A9%E5%88%86%E7%B1%BB%E5%99%A8
[Efficient Grid Size Reduction]: #%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E4%BA%94%E3%80%81Efficient%20Grid%20Size%20Reduction%E2%80%94%E9%AB%98%E6%95%88%E4%B8%8B%E9%99%8D%E7%89%B9%E5%BE%81%E5%9B%BE%E5%B0%BA%E5%AF%B8
[Inception-v2]: #%E5%85%AD%E3%80%81Inception-v2
[Model Regularization via Label Smoothing_LSR]: #%E4%B8%83%E3%80%81Model%20Regularization%20via%20Label%20Smoothing%EF%BC%88LSR%EF%BC%89%E2%80%94%E2%80%94%E9%80%9A%E8%BF%87%E6%A0%87%E7%AD%BE%E5%B9%B3%E6%BB%91%E8%BF%9B%E8%A1%8C%E6%A8%A1%E5%9E%8B%E6%AD%A3%E5%88%99%E5%8C%96
[Training Methodology]: #%E5%85%AB%E3%80%81Training%20Methodology%E2%80%94%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95
[Performance on Lower Resolution Input]: #%E4%B9%9D%E3%80%81Performance%20on%20Lower%20Resolution%20Input%E2%80%94%E5%B0%8F%E5%88%86%E8%BE%A8%E7%8E%87%E8%BE%93%E5%85%A5%E6%80%A7%E8%83%BD
[Experimental Results and Comparisons]: #%E5%8D%81%E3%80%81Experimental%20Results%20and%20Comparisons%E2%80%94%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%E5%92%8C%E6%AF%94%E8%BE%83
[Conclusions]: #%E5%8D%81%E4%B8%80%E3%80%81Conclusions%E2%80%94%E7%BB%93%E8%AE%BA
[Link 2]: #%E8%AE%BA%E6%96%87%E5%8D%81%E9%97%AE
[GoogLeNet InceptionV3_PyTorch]: https://blog.csdn.net/weixin_43334693/article/details/128345872?spm=1001.2014.3001.5502