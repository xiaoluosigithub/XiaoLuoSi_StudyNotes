![](https://i-blog.csdnimg.cn/blog_migrate/dd9c8e56b362b4826c1bafd7f843c29f.gif)

![](https://i-blog.csdnimg.cn/blog_migrate/a0fd89d5bf65f91d235dbef3111468c6.png)

![962f7cb1b48f44e29d9beb1d499d0530.gif](https://i-blog.csdnimg.cn/blog_migrate/ac3c5d6bfbcbf982e8e9e3632d7f20d1.gif)ã€YOLOv5æ”¹è¿›ç³»åˆ—ã€‘å‰æœŸå›é¡¾ï¼š

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ0ï¼‰â€”â€”é‡è¦æ€§èƒ½æŒ‡æ ‡ä¸è®­ç»ƒç»“æœè¯„ä»·åŠåˆ†æ][YOLOv5_0]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ1ï¼‰â€”â€”æ·»åŠ SEæ³¨æ„åŠ›æœºåˆ¶][YOLOv5_1_SE]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ2ï¼‰â€”â€”æ·»åŠ CBAMæ³¨æ„åŠ›æœºåˆ¶][YOLOv5_2_CBAM]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ3ï¼‰â€”â€”æ·»åŠ CAæ³¨æ„åŠ›æœºåˆ¶][YOLOv5_3_CA]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ4ï¼‰â€”â€”æ·»åŠ ECAæ³¨æ„åŠ›æœºåˆ¶][YOLOv5_4_ECA]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ5ï¼‰â€”â€”æ›¿æ¢ä¸»å¹²ç½‘ç»œä¹‹ MobileNetV3][YOLOv5_5_ MobileNetV3]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ6ï¼‰â€”â€”æ›¿æ¢ä¸»å¹²ç½‘ç»œä¹‹ ShuffleNetV2][YOLOv5_6_ ShuffleNetV2]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ7ï¼‰â€”â€”æ·»åŠ SimAMæ³¨æ„åŠ›æœºåˆ¶][YOLOv5_7_SimAM]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ8ï¼‰â€”â€”æ·»åŠ SOCAæ³¨æ„åŠ›æœºåˆ¶][YOLOv5_8_SOCA]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ9ï¼‰â€”â€”æ›¿æ¢ä¸»å¹²ç½‘ç»œä¹‹EfficientNetv2][YOLOv5_9_EfficientNetv2]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ10ï¼‰â€”â€”æ›¿æ¢ä¸»å¹²ç½‘ç»œä¹‹GhostNet][YOLOv5_10_GhostNet]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ11ï¼‰â€”â€”æ·»åŠ æŸå¤±å‡½æ•°ä¹‹EIoUã€AlphaIoUã€SIoUã€WIoU][YOLOv5_11_EIoU_AlphaIoU_SIoU_WIoU]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ12ï¼‰â€”â€”æ›´æ¢Neckä¹‹BiFPN][YOLOv5_12_Neck_BiFPN]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ13ï¼‰â€”â€”æ›´æ¢æ¿€æ´»å‡½æ•°ä¹‹SiLUï¼ŒReLUï¼ŒELUï¼ŒHardswishï¼ŒMishï¼ŒSoftplusï¼ŒAconCç³»åˆ—ç­‰][YOLOv5_13_SiLU_ReLU_ELU_Hardswish_Mish_Softplus_AconC]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ14ï¼‰â€”â€”æ›´æ¢NMSï¼ˆéæå¤§æŠ‘åˆ¶ï¼‰ä¹‹ DIoU-NMSã€CIoU-NMSã€EIoU-NMSã€GIoU-NMS ã€SIoU-NMSã€Soft-NMS][YOLOv5_14_NMS_ DIoU-NMS_CIoU-NMS_EIoU-NMS_GIoU-NMS _SIoU-NMS_Soft-NMS]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ15ï¼‰â€”â€”å¢åŠ å°ç›®æ ‡æ£€æµ‹å±‚][YOLOv5_15]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ16ï¼‰â€”â€”æ·»åŠ EMAæ³¨æ„åŠ›æœºåˆ¶ï¼ˆICASSP2023|å®æµ‹æ¶¨ç‚¹ï¼‰][YOLOv5_16_EMA_ICASSP2023]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ17ï¼‰â€”â€”æ›´æ¢IoUä¹‹MPDIoUï¼ˆELSEVIER 2023|è¶…è¶ŠWIoUã€EIoUç­‰|å®æµ‹æ¶¨ç‚¹ï¼‰][YOLOv5_17_IoU_MPDIoU_ELSEVIER 2023_WIoU_EIoU]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ18ï¼‰â€”â€”æ›´æ¢Neckä¹‹AFPNï¼ˆå…¨æ–°æ¸è¿›ç‰¹å¾é‡‘å­—å¡”|è¶…è¶ŠPAFPN|å®æµ‹æ¶¨ç‚¹ï¼‰][YOLOv5_18_Neck_AFPN_PAFPN]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ19ï¼‰â€”â€”æ›¿æ¢ä¸»å¹²ç½‘ç»œä¹‹Swin TransformerV1ï¼ˆå‚æ•°é‡æ›´å°çš„ViTæ¨¡å‹ï¼‰][YOLOv5_19_Swin TransformerV1_ViT]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ21ï¼‰â€”â€”æ›¿æ¢ä¸»å¹²ç½‘ç»œä¹‹RepViTï¼ˆæ¸…å ICCV 2023|æœ€æ–°å¼€æºç§»åŠ¨ç«¯ViTï¼‰][YOLOv5_21_RepViT_ ICCV 2023_ViT]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ22ï¼‰â€”â€”æ›¿æ¢ä¸»å¹²ç½‘ç»œä¹‹MobileViTv1ï¼ˆä¸€ç§è½»é‡çº§çš„ã€é€šç”¨çš„ç§»åŠ¨è®¾å¤‡ ViTï¼‰][YOLOv5_22_MobileViTv1_ ViT]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ23ï¼‰â€”â€”æ›¿æ¢ä¸»å¹²ç½‘ç»œä¹‹MobileViTv2ï¼ˆç§»åŠ¨è§†è§‰ Transformer çš„é«˜æ•ˆå¯åˆ†ç¦»è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼‰][YOLOv5_23_MobileViTv2_ Transformer]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ24ï¼‰â€”â€”æ›¿æ¢ä¸»å¹²ç½‘ç»œä¹‹MobileViTv3ï¼ˆç§»åŠ¨ç«¯è½»é‡åŒ–ç½‘ç»œçš„è¿›ä¸€æ­¥å‡çº§ï¼‰][YOLOv5_24_MobileViTv3]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ25ï¼‰â€”â€”æ·»åŠ LSKNetæ³¨æ„åŠ›æœºåˆ¶ï¼ˆå¤§é€‰æ‹©æ€§å·ç§¯æ ¸çš„é¢†åŸŸé¦–æ¬¡æ¢ç´¢ï¼‰][YOLOv5_25_LSKNet]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ26ï¼‰â€”â€”æ·»åŠ RFAConvæ³¨æ„åŠ›å·ç§¯ï¼ˆæ„Ÿå—é‡æ³¨æ„åŠ›å·ç§¯è¿ç®—ï¼‰][YOLOv5_26_RFAConv]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ27ï¼‰â€”â€”æ·»åŠ SCConvæ³¨æ„åŠ›å·ç§¯ï¼ˆCVPR 2023|å³æ’å³ç”¨çš„é«˜æ•ˆå·ç§¯æ¨¡å—ï¼‰][YOLOv5_27_SCConv_CVPR 2023]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ28ï¼‰â€”â€”æ·»åŠ DSConvæ³¨æ„åŠ›å·ç§¯ï¼ˆICCV 2023|ç”¨äºç®¡çŠ¶ç»“æ„åˆ†å‰²çš„åŠ¨æ€è›‡å½¢å·ç§¯ï¼‰][YOLOv5_28_DSConv_ICCV 2023]

![](https://i-blog.csdnimg.cn/blog_migrate/4b4c71ac227176648955d7cff525800e.gif)

ç›®å½•

[ğŸš€ ä¸€ã€DilateFormerä»‹ç»][_DilateFormer]

[1.1 DilateFormerç®€ä»‹ ][1.1 DilateFormer_]

[1.2 DilateFormerç½‘ç»œç»“æ„][1.2 DilateFormer]

[â‘  DilateFormer][DilateFormer]

[â‘¡ MSDA][MSDA]

[ğŸš€äºŒã€å…·ä½“æ·»åŠ æ–¹æ³•][Link 1]

[2.1 æ·»åŠ é¡ºåº ][2.1 _]

[2.2 å…·ä½“æ·»åŠ æ­¥éª¤ ][2.2 _]

[ç¬¬â‘ æ­¥ï¼šåœ¨common.pyä¸­æ·»åŠ DilateFormeræ¨¡å— ][common.py_DilateFormer_]

[ç¬¬â‘¡æ­¥ï¼šä¿®æ”¹yolo.pyæ–‡ä»¶ ][yolo.py_]

[ç¬¬â‘¢æ­¥ï¼šåˆ›å»ºè‡ªå®šä¹‰çš„yamlæ–‡ä»¶ ][yaml_]

[ç¬¬â‘£æ­¥ï¼šéªŒè¯æ˜¯å¦åŠ å…¥æˆåŠŸ][Link 2]

[ğŸŒŸæœ¬äººYOLOv5ç³»åˆ—å¯¼èˆª][YOLOv5]

![](https://i-blog.csdnimg.cn/blog_migrate/102a36e3355d23b67c415a295a5b2ed3.gif)

## ğŸš€ ä¸€ã€DilateFormerä»‹ç» 

> å­¦ä¹ èµ„æ–™ï¼š
> 
>  *  è®ºæ–‡é¢˜ç›®ï¼šã€ŠDilateFormer: Multi-Scale Dilated [Transformer][] for Visual Recognitionã€‹
>  *  è®ºæ–‡åœ°å€ï¼š[https://arxiv.org/abs/2302.01791][https_arxiv.org_abs_2302.01791]
>  *  æºç åœ°å€ï¼š[https://github.com/abhinavsagar/mssa][https_github.com_abhinavsagar_mssa]

### 1.1 DilateFormerç®€ä»‹ 

èƒŒæ™¯

ViT æ¨¡å‹åœ¨è®¡ç®—å¤æ‚æ€§å’Œæ„Ÿå—é‡å¤§å°ä¹‹é—´çš„æƒè¡¡ä¸Šå­˜åœ¨çŸ›ç›¾ï¼š

 *  å…·ä½“æ¥è¯´ï¼ŒViT æ¨¡å‹ä½¿ç”¨å…¨å±€æ³¨æ„åŠ›æœºåˆ¶ï¼Œèƒ½å¤Ÿåœ¨ä»»æ„å›¾åƒå—ä¹‹é—´å»ºç«‹é•¿è¿œè·ç¦»ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»ï¼Œä½†æ˜¯å…¨å±€æ„Ÿå—é‡å¸¦æ¥çš„æ˜¯å¹³æ–¹çº§åˆ«çš„è®¡ç®—ä»£ä»·ã€‚
 *  åŒæ—¶ï¼Œåœ¨æµ…å±‚ç‰¹å¾ä¸Šï¼Œç›´æ¥è¿›è¡Œå…¨å±€ä¾èµ–æ€§å»ºæ¨¡å¯èƒ½å­˜åœ¨å†—ä½™ï¼Œå› æ­¤æ˜¯æ²¡å¿…è¦çš„ã€‚

æœ¬æ–‡æ–¹æ³• 

ï¼ˆ1ï¼‰å¤šå°ºåº¦ç©ºæ´æ³¨æ„åŠ›ï¼ˆMSDAï¼‰ 

 *  MSDA èƒ½å¤Ÿæ¨¡æ‹Ÿå°èŒƒå›´å†…çš„å±€éƒ¨å’Œç¨€ç–çš„å›¾åƒå—äº¤äº’
 *  åœ¨æµ…å±‚æ¬¡ä¸Šï¼Œæ³¨æ„åŠ›çŸ©é˜µå…·æœ‰å±€éƒ¨æ€§å’Œç¨€ç–æ€§ä¸¤ä¸ªå…³é”®å±æ€§
 *  è¿™è¡¨æ˜åœ¨æµ…å±‚æ¬¡çš„è¯­ä¹‰å»ºæ¨¡ä¸­ï¼Œè¿œç¦»æŸ¥è¯¢å—çš„å—å¤§éƒ¨åˆ†æ— å…³
 *  å› æ­¤ï¼Œå…¨å±€æ³¨æ„åŠ›æ¨¡å—ä¸­å­˜åœ¨å¤§é‡çš„å†—ä½™ã€‚

ï¼ˆ2ï¼‰DilateFormer 

 *  åœ¨åº•å±‚é˜¶æ®µå †å  MSDA å—
 *  åœ¨é«˜å±‚é˜¶æ®µä½¿ç”¨å…¨å±€å¤šå¤´è‡ªæ³¨æ„åŠ›å—
 *  è¿™æ ·çš„è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨å¤„ç†ä½çº§ä¿¡æ¯æ—¶ï¼Œå……åˆ†åˆ©ç”¨å±€éƒ¨æ€§å’Œç¨€ç–æ€§ï¼Œè€Œåœ¨å¤„ç†é«˜çº§ä¿¡æ¯æ—¶ï¼Œåˆèƒ½æ¨¡æ‹Ÿè¿œè·ç¦»ä¾èµ–

å®ç°æ•ˆæœ

 *  è®ºæ–‡é€šè¿‡åœ¨ä¸åŒçš„è§†è§‰ä»»åŠ¡ä¸Šè¿›è¡Œå®éªŒï¼Œå‘ç° DilateFormer æ¨¡å‹å–å¾—äº†å¾ˆå¥½çš„æ•ˆæœã€‚
 *  åœ¨ ImageNet-1K åˆ†ç±»ä»»åŠ¡ä¸Šï¼Œä¸ç°æœ‰æœ€ä¼˜ç§€çš„æ¨¡å‹ç›¸æ¯”ï¼ŒDilateFormer æ¨¡å‹çš„æ€§èƒ½ç›¸å½“ï¼Œä½†æ‰€éœ€çš„FLOPsï¼ˆæµ®ç‚¹è¿ç®—æ¬¡æ•°ï¼‰å‡å°‘äº†70%ã€‚
 *  åœ¨å…¶å®ƒçš„è§†è§‰ä»»åŠ¡ï¼Œå¦‚ COCO å¯¹è±¡æ£€æµ‹/å®ä¾‹åˆ†å‰²ä»»åŠ¡å’Œ ADE20K è¯­ä¹‰åˆ†å‰²ä»»åŠ¡ï¼ŒDilateFormer æ¨¡å‹åŒæ ·å–å¾—äº†ä¼˜ç§€çš„è¡¨ç°ã€‚

### 1.2 DilateFormerç½‘ç»œç»“æ„ 

#### â‘  DilateFormer 

![](https://i-blog.csdnimg.cn/blog_migrate/18b7346d7cdd27499733a23d5fb44883.png)

å¦‚ä¸Šå›¾æ‰€ç¤ºï¼ŒDilateFormerçš„æ•´ä½“æ¶æ„ä¸»è¦ç”±å››ä¸ªé˜¶æ®µæ„æˆï¼š

 *  åœ¨ç¬¬ä¸€é˜¶æ®µå’Œç¬¬äºŒé˜¶æ®µï¼Œä½¿ç”¨ MSDA
 *  åœ¨ç¬¬ä¸‰é˜¶æ®µå’Œç¬¬å››é˜¶æ®µï¼Œä½¿ç”¨æ™®é€šçš„å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆMHSAï¼‰

å¯¹äºå›¾åƒè¾“å…¥ï¼šDilateFormer é¦–å…ˆä½¿ç”¨é‡å çš„åˆ†è¯å™¨è¿›è¡Œ patch åµŒå…¥ï¼Œç„¶åé€šè¿‡äº¤æ›¿æ§åˆ¶å·ç§¯æ ¸çš„æ­¥é•¿å¤§å°ï¼ˆ1æˆ–2ï¼‰æ¥è°ƒæ•´è¾“å‡ºç‰¹å¾å›¾çš„åˆ†è¾¨ç‡ã€‚

å¯¹äºå‰ä¸€é˜¶æ®µçš„ patchesï¼Œé‡‡ç”¨äº†ä¸€ä¸ªé‡å çš„ä¸‹é‡‡æ ·å™¨ï¼Œå…·æœ‰é‡å çš„å†…æ ¸å¤§å°ä¸º 3ï¼Œæ­¥é•¿ä¸º 2ã€‚

æ•´ä¸ªæ¨¡å‹çš„æ¯ä¸€éƒ¨åˆ†éƒ½ä½¿ç”¨äº†æ¡ä»¶ä½ç½®åµŒå…¥ï¼ˆCPEï¼‰æ¥ä½¿ä½ç½®ç¼–ç é€‚åº”ä¸åŒåˆ†è¾¨ç‡çš„è¾“å…¥ã€‚

ä»£ç ï¼š

```java
class DilateBlock(nn.Module):
    "Implementation of Dilate-attention block"
    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False,qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0.,act_layer=nn.GELU, norm_layer=nn.LayerNorm, kernel_size=3, dilation=[1, 2, 3],
                 cpe_per_block=False):
        super().__init__()
        self.dim = dim # è¡¨ç¤ºè¾“å…¥ç‰¹å¾çš„ç»´åº¦
        self.num_heads = num_heads # è¡¨ç¤ºå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„æ³¨æ„åŠ›å¤´æ•°
        self.mlp_ratio = mlp_ratio # MLPï¼ˆå¤šå±‚æ„ŸçŸ¥æœºï¼‰éšè—å±‚ç»´åº¦ä¸è¾“å…¥ç»´åº¦ä¹‹æ¯”
        self.kernel_size = kernel_size 
        self.dilation = dilation # å·ç§¯æ ¸çš„è†¨èƒ€ç‡ï¼ŒæŒ‡å®šæ¯ä¸€å±‚çš„è†¨èƒ€ç‡
        self.cpe_per_block = cpe_per_block  # æ˜¯å¦åœ¨æ¯ä¸ªå—ä¸­ä½¿ç”¨ä½ç½®åµŒå…¥
        if self.cpe_per_block:
            self.pos_embed = nn.Conv2d(dim, dim, 3, padding=1, groups=dim)
        self.norm1 = norm_layer(dim)
        self.attn = MultiDilatelocalAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,
                                                attn_drop=attn_drop, kernel_size=kernel_size, dilation=dilation)
        '''
         : qkv_biasï¼šæ˜¯å¦å…è®¸æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„æŸ¥è¯¢ã€é”®ã€å€¼çš„åç½®ã€‚
         : qk_scaleï¼šæ³¨æ„åŠ›æœºåˆ¶ä¸­æŸ¥è¯¢å’Œé”®çš„ç¼©æ”¾å› å­ã€‚
         : dropï¼šMLPä¸­çš„dropoutæ¦‚ç‡ã€‚
         : attn_dropï¼šæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„dropoutæ¦‚ç‡ã€‚
        '''
        self.drop_path = DropPath(
            drop_path) if drop_path > 0. else nn.Identity() 

        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,
                       act_layer=act_layer, drop=drop)

    def forward(self, x):
        if self.cpe_per_block:
            x = x + self.pos_embed(x)
        x = x.permute(0, 2, 3, 1)
        x = x + self.drop_path(self.attn(self.norm1(x)))
        x = x + self.drop_path(self.mlp(self.norm2(x)))
        x = x.permute(0, 3, 1, 2)
        #B, C, H, W
        return x
```

è¿‡ç¨‹å¦‚ä¸‹ï¼š 

1.  é¦–å…ˆï¼Œå¦‚æœå¯ç”¨äº†ä½ç½®åµŒå…¥ï¼ˆ`cpe_per_block`ä¸ºtrueï¼‰ï¼Œåˆ™å°†è¾“å…¥å¼ é‡ä¸é€šè¿‡`nn.Conv2d`å®ç°çš„ä½ç½®åµŒå…¥ç›¸åŠ ã€‚
2.  æ¥ç€ï¼Œå°†è¾“å…¥å¼ é‡ç»´åº¦è¿›è¡Œè°ƒæ•´ï¼Œå°†é€šé“ç»´åº¦æ”¾åˆ°æœ€åã€‚
3.  ç„¶åï¼Œé€šè¿‡å¤šå¤´è†¨èƒ€å±€éƒ¨æ³¨æ„åŠ›æœºåˆ¶å¤„ç†è¾“å…¥ï¼Œå†é€šè¿‡MLPå¤„ç†ä¸Šä¸€æ­¥çš„è¾“å‡ºã€‚
4.  æœ€åï¼Œå†æ¬¡è°ƒæ•´å¼ é‡ç»´åº¦ï¼Œå¹¶è¿”å›å¤„ç†åçš„ç»“æœã€‚

#### â‘¡ MSDA 

![](https://i-blog.csdnimg.cn/blog_migrate/479722a1272e23618dbec6da26db05bb.png)

MSAD æ¨¡å—æ˜¯DilateFormer çš„æ ¸å¿ƒéƒ¨åˆ†ã€‚

å¦‚ä¸Šå›¾æ‰€ç¤ºï¼ŒMSDA æ¨¡å—åŒæ ·é‡‡ç”¨å¤šå¤´çš„è®¾è®¡ï¼šå°†ç‰¹å¾å›¾çš„é€šé“åˆ†ä¸º n ä¸ªä¸åŒçš„å¤´éƒ¨ï¼Œå¹¶åœ¨ä¸åŒçš„å¤´éƒ¨ä½¿ç”¨ä¸åŒçš„ç©ºæ´ç‡æ‰§è¡Œæ»‘åŠ¨çª—å£è†¨èƒ€æ³¨æ„åŠ›ï¼ˆSWDAï¼‰ã€‚

ç›®çš„ï¼šè¿™æ ·å¯ä»¥åœ¨è¢«å…³æ³¨çš„æ„Ÿå—é‡å†…çš„å„ä¸ªå°ºåº¦ä¸Šèšåˆè¯­ä¹‰ä¿¡æ¯ï¼Œå¹¶æœ‰æ•ˆåœ°å‡å°‘è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å†—ä½™ï¼Œæ— éœ€å¤æ‚çš„æ“ä½œå’Œé¢å¤–çš„è®¡ç®—æˆæœ¬ã€‚

å…·ä½“çš„æ“ä½œå¦‚ä¸‹ï¼š

1.  å¯¹äºæ¯ä¸ªå¤´éƒ¨ï¼Œéƒ½ä¼šæœ‰ä¸€ä¸ªç‹¬ç«‹çš„è†¨èƒ€ç‡![r_{i}](https://latex.csdn.net/eq?r_%7Bi%7D)
2.  ä»ç‰¹å¾å›¾ä¸­è·å–åˆ‡ç‰‡![Q_{i}](https://latex.csdn.net/eq?Q_%7Bi%7D)ï¼Œ![K_{i}](https://latex.csdn.net/eq?K_%7Bi%7D)ï¼Œ![V_{i}](https://latex.csdn.net/eq?V_%7Bi%7D)ï¼Œæ‰§è¡Œ SWDAï¼Œå¾—åˆ°è¾“å‡º![h_{i}](https://latex.csdn.net/eq?h_%7Bi%7D)
3.  å°†æ‰€æœ‰å¤´éƒ¨çš„è¾“å‡ºè¿æ¥åœ¨ä¸€èµ·ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªçº¿æ€§å±‚è¿›è¡Œç‰¹å¾èšåˆ

ä»£ç ï¼š

```java
class DilateAttention(nn.Module):
    "Implementation of Dilate-attention"
    def __init__(self, head_dim, qk_scale=None, attn_drop=0, kernel_size=3, dilation=1):
        super().__init__()
        self.head_dim = head_dim # æ³¨æ„åŠ›å¤´çš„ç»´åº¦
        self.scale = qk_scale or head_dim ** -0.5 # æŸ¥è¯¢å’Œé”®çš„ç¼©æ”¾å› å­ï¼Œé»˜è®¤ä¸º head_dim ** -0.5ã€‚
        self.kernel_size=kernel_size
        self.unfold = nn.Unfold(kernel_size, dilation, dilation*(kernel_size-1)//2, 1)
        self.attn_drop = nn.Dropout(attn_drop) # æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„dropoutæ¦‚ç‡

    def forward(self,q,k,v):
        #B, C//3, H, W
        B,d,H,W = q.shape
        q = q.reshape([B, d//self.head_dim, self.head_dim, 1 ,H*W]).permute(0, 1, 4, 3, 2)  # B,h,N,1,d
        k = self.unfold(k).reshape([B, d//self.head_dim, self.head_dim, self.kernel_size*self.kernel_size, H*W]).permute(0, 1, 4, 2, 3)  #B,h,N,d,k*k
        attn = (q @ k) * self.scale  # B,h,N,1,k*k
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)
        v = self.unfold(v).reshape([B, d//self.head_dim, self.head_dim, self.kernel_size*self.kernel_size, H*W]).permute(0, 1, 4, 3, 2)  # B,h,N,k*k,d
        x = (attn @ v).transpose(1, 2).reshape(B, H, W, d)
        return x

class MultiDilatelocalAttention(nn.Module):
    "Implementation of Dilate-attention"

    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None,
                 attn_drop=0.,proj_drop=0., kernel_size=3, dilation=[1, 2, 3]):
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.dilation = dilation
        self.kernel_size = kernel_size
        self.scale = qk_scale or head_dim ** -0.5
        self.num_dilation = len(dilation)
        assert num_heads % self.num_dilation == 0, f"num_heads{num_heads} must be the times of num_dilation{self.num_dilation}!!"
        self.qkv = nn.Conv2d(dim, dim * 3, 1, bias=qkv_bias)
        self.dilate_attention = nn.ModuleList(
            [DilateAttention(head_dim, qk_scale, attn_drop, kernel_size, dilation[i])
             for i in range(self.num_dilation)])
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, x):
        B, H, W, C = x.shape
        x = x.permute(0, 3, 1, 2)# B, C, H, W
        qkv = self.qkv(x).reshape(B, 3, self.num_dilation, C//self.num_dilation, H, W).permute(2, 1, 0, 3, 4, 5)
        #num_dilation,3,B,C//num_dilation,H,W
        x = x.reshape(B, self.num_dilation, C//self.num_dilation, H, W).permute(1, 0, 3, 4, 2 )
        # num_dilation, B, H, W, C//num_dilation
        for i in range(self.num_dilation):
            x[i] = self.dilate_attention[i](qkv[i][0], qkv[i][1], qkv[i][2])# B, H, W,C//num_dilation
        x = x.permute(1, 2, 3, 0, 4).reshape(B, H, W, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x
```

 class DilateAttention

1.  é¦–å…ˆï¼Œå¯¹è¾“å…¥çš„`q`ã€`k`ã€`v`è¿›è¡Œä¸€äº›é¢„å¤„ç†ã€‚
2.  ç„¶åï¼Œå°†`q`å’Œ`k`è¿›è¡ŒçŸ©é˜µä¹˜æ³•ï¼Œå¹¶ä½¿ç”¨ç¼©æ”¾å› å­è¿›è¡Œç¼©æ”¾ï¼Œå¾—åˆ°çš„çŸ©é˜µè¡¨ç¤ºæ³¨æ„åŠ›åˆ†å¸ƒã€‚
3.  æ¥ç€ï¼Œå¯¹æ³¨æ„åŠ›åˆ†å¸ƒè¿›è¡Œ softmaxã€dropout æ“ä½œï¼Œå°†æ³¨æ„åŠ›åˆ†å¸ƒä¸`v`è¿›è¡ŒçŸ©é˜µä¹˜æ³•ã€‚
4.  æœ€åï¼Œé€šè¿‡ä¸€ç³»åˆ—çš„ç»´åº¦è°ƒæ•´å¾—åˆ°è¾“å‡ºå¼ é‡ `x`ã€‚

class MultiDilatelocalAttention 

1.  é¦–å…ˆï¼Œå¯¹è¾“å…¥å¼ é‡è¿›è¡Œç»´åº¦è°ƒæ•´ï¼Œå°†é€šé“ç»´åº¦æ”¾åˆ°ç¬¬äºŒä¸ªä½ç½®ã€‚
2.  ç„¶åï¼Œä½¿ç”¨ä¸€ä¸ªå·ç§¯å±‚ `self.qkv` å¯¹è¾“å…¥å¼ é‡è¿›è¡Œå¤„ç†ï¼Œå¾—åˆ°åŒ…å«æŸ¥è¯¢ã€é”®ã€å€¼çš„å¼ é‡ï¼Œå†è¿›è¡Œç»´åº¦è°ƒæ•´ã€‚
3.  æ¥ç€ï¼Œä½¿ç”¨ä¸€ä¸ª `nn.ModuleList` åŒ…å«å¤šä¸ª`DilateAttention`æ¨¡å—ï¼Œæ¯ä¸ªæ¨¡å—å¯¹è¾“å…¥è¿›è¡Œå¤šå¤´è†¨èƒ€å±€éƒ¨æ³¨æ„åŠ›æœºåˆ¶çš„å¤„ç†ã€‚
4.  æœ€åï¼Œå°†å¤„ç†åçš„å¼ é‡ç»è¿‡å…¨è¿æ¥å±‚å’Œdropoutå±‚ï¼Œå¾—åˆ°æœ€ç»ˆçš„è¾“å‡ºã€‚

## ğŸš€äºŒã€å…·ä½“æ·»åŠ æ–¹æ³• 

### 2.1 æ·»åŠ é¡ºåº 

ï¼ˆ1ï¼‰models/common.py --> åŠ å…¥æ–°å¢çš„ç½‘ç»œç»“æ„

ï¼ˆ2ï¼‰ models/yolo.py --> è®¾å®šç½‘ç»œç»“æ„çš„ä¼ å‚ç»†èŠ‚ï¼Œå°†DilateFormerç±»ååŠ å…¥å…¶ä¸­ã€‚ï¼ˆå½“æ–°çš„è‡ªå®šä¹‰æ¨¡å—ä¸­å­˜åœ¨è¾“å…¥è¾“å‡ºç»´åº¦æ—¶ï¼Œè¦ä½¿ç”¨qwè°ƒæ•´è¾“å‡ºç»´åº¦ï¼‰  
ï¼ˆ3ï¼‰ models/yolov5\*.yaml -->  æ–°å»ºä¸€ä¸ªæ–‡ä»¶å¤¹ï¼Œå¦‚yolov5s\_DilateFormer.yamlï¼Œä¿®æ”¹ç°æœ‰æ¨¡å‹ç»“æ„é…ç½®æ–‡ä»¶ã€‚ï¼ˆå½“å¼•å…¥æ–°çš„å±‚æ—¶ï¼Œè¦ä¿®æ”¹åç»­çš„ç»“æ„ä¸­çš„fromå‚æ•°ï¼‰  
ï¼ˆ4ï¼‰ train.py --> ä¿®æ”¹â€˜--cfgâ€™é»˜è®¤å‚æ•°ï¼Œè®­ç»ƒæ—¶æŒ‡å®šæ¨¡å‹ç»“æ„é…ç½®æ–‡ä»¶

### 2.2 å…·ä½“æ·»åŠ æ­¥éª¤ 

#### ç¬¬â‘ æ­¥ï¼šåœ¨common.pyä¸­æ·»åŠ DilateFormeræ¨¡å—  

å°†ä¸‹é¢çš„DilateFormerä»£ç å¤åˆ¶ç²˜è´´åˆ°common.pyæ–‡ä»¶çš„æœ«å°¾ã€‚

```java
import torch
import torch.nn as nn
from functools import partial
from timm.models.layers import DropPath, to_2tuple, trunc_normal_
 
 
class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)
 
    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x
 
 
class DilateAttention(nn.Module):
    "Implementation of Dilate-attention"
    def __init__(self, head_dim, qk_scale=None, attn_drop=0, kernel_size=3, dilation=1):
        super().__init__()
        self.head_dim = head_dim
        self.scale = qk_scale or head_dim ** -0.5
        self.kernel_size=kernel_size
        self.unfold = nn.Unfold(kernel_size, dilation, dilation*(kernel_size-1)//2, 1)
        self.attn_drop = nn.Dropout(attn_drop)
 
    def forward(self,q,k,v):
        #B, C//3, H, W
        B,d,H,W = q.shape
 
        q = q.reshape([B, d//self.head_dim, self.head_dim, 1 ,H*W]).permute(0, 1, 4, 3, 2)  # B,h,N,1,d
        k = self.unfold(k).reshape([B, d//self.head_dim, self.head_dim, self.kernel_size*self.kernel_size, H*W]).permute(0, 1, 4, 2, 3)  #B,h,N,d,k*k
        attn = (q @ k) * self.scale  # B,h,N,1,k*k
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)
        v = self.unfold(v).reshape([B, d//self.head_dim, self.head_dim, self.kernel_size*self.kernel_size, H*W]).permute(0, 1, 4, 3, 2)  # B,h,N,k*k,d
        x = (attn @ v).transpose(1, 2).reshape(B, H, W, d)
        return x
 
 
class MultiDilatelocalAttention(nn.Module):
    "Implementation of Dilate-attention"
 
    def __init__(self, dim, num_heads=4, qkv_bias=False, qk_scale=None,
                 attn_drop=0.,proj_drop=0., kernel_size=3, dilation=[1, 2]):
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.dilation = dilation
        self.kernel_size = kernel_size
        self.scale = qk_scale or head_dim ** -0.5
        self.num_dilation = len(dilation)
 
        assert num_heads % self.num_dilation == 0, f"num_heads{num_heads} must be the times of num_dilation{self.num_dilation}!!"
        self.qkv = nn.Conv2d(dim, dim * 3, 1, bias=qkv_bias)
        self.dilate_attention = nn.ModuleList(
            [DilateAttention(head_dim, qk_scale, attn_drop, kernel_size, dilation[i])
             for i in range(self.num_dilation)])
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
 
    def forward(self, x):
 
        x = x.permute(0, 3, 1, 2)  # B, C, H, W
        B, C, H, W = x.shape
 
        qkv = self.qkv(x).reshape(B, 3, self.num_dilation, C //self.num_dilation, H, W).permute(2, 1, 0, 3, 4, 5)
        #num_dilation,3,B,C//num_dilation,H,W
        x = x.reshape(B, self.num_dilation, C//self.num_dilation, H, W).permute(1, 0, 3, 4, 2 )
        # num_dilation, B, H, W, C//num_dilation
        for i in range(self.num_dilation):
            x[i] = self.dilate_attention[i](qkv[i][0], qkv[i][1], qkv[i][2])# B, H, W,C//num_dilation
        x = x.permute(1, 2, 3, 0, 4).reshape(B, H, W, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x
 
 
class DilateBlock(nn.Module):
    "Implementation of Dilate-attention block"
    def __init__(self, dim, num_heads=4, mlp_ratio=4., qkv_bias=False,qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0.,act_layer=nn.GELU, norm_layer=nn.LayerNorm, kernel_size=3, dilation=[1, 2],
                 cpe_per_block=False):
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        self.mlp_ratio = mlp_ratio
        self.kernel_size = kernel_size
        self.dilation = dilation
        self.cpe_per_block = cpe_per_block
        if self.cpe_per_block:
            self.pos_embed = nn.Conv2d(dim, dim, 3, padding=1, groups=dim)
        self.norm1 = norm_layer(dim)
        self.attn = MultiDilatelocalAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,
                                                attn_drop=attn_drop, kernel_size=kernel_size, dilation=dilation)
 
        self.drop_path = DropPath(
            drop_path) if drop_path > 0. else nn.Identity()
 
 
 
    def forward(self, x):
 
        x = x.permute(0, 3, 2, 1)
        x = x + self.drop_path(self.attn(self.norm1(x)))
        x = x.permute(0, 3, 2, 1)
        #B, C, H, W
        return x
 
 
def autopad(k, p=None, d=1):  # kernel, padding, dilation
    # Pad to 'same' shape outputs
    if d > 1:
        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]  # actual kernel-size
    if p is None:
        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad
    return p
 
 
class Conv(nn.Module):
    # Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation)
    default_act = nn.SiLU()  # default activation
 
    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):
        super().__init__()
        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)
        self.bn = nn.BatchNorm2d(c2)
        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()
 
    def forward(self, x):
        return self.act(self.bn(self.conv(x)))
 
    def forward_fuse(self, x):
        return self.act(self.conv(x))
 
class C3_DilateBlock(nn.Module):
    # CSP Bottleneck with 3 convolutions
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c1, c_, 1, 1)
        self.cv3 = Conv(2 * c_, c2, 1)  # optional act=FReLU(c2)
        self.m = nn.Sequential(*(DilateBlock(c_) for _ in range(n)))
 
    def forward(self, x):
        return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1))
```

#### ç¬¬â‘¡æ­¥ï¼šä¿®æ”¹yolo.pyæ–‡ä»¶ 

é¦–å…ˆæ‰¾åˆ°yolo.pyé‡Œé¢parse\_modelå‡½æ•°çš„è¿™ä¸€è¡Œ

![](https://i-blog.csdnimg.cn/blog_migrate/015ea2925046c213eeb09ba320482cfe.png)

åŠ å…¥DilateBlockã€C3\_DilateBlockè¿™ä¸¤ä¸ªæ¨¡å—

![](https://i-blog.csdnimg.cn/blog_migrate/1eed8944e62bf370e7d2d7a5a37ec3c4.png)

#### ç¬¬â‘¢æ­¥ï¼šåˆ›å»ºè‡ªå®šä¹‰çš„yamlæ–‡ä»¶ 

ç¬¬1ç§ï¼Œåœ¨SPPFå‰å•ç‹¬åŠ ä¸€å±‚

```java
# YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license

# Parameters
nc: 80  # number of classes
depth_multiple: 0.33  # model depth multiple
width_multiple: 0.50  # layer channel multiple
anchors:
  - [10,13, 16,30, 33,23]  # P3/8
  - [30,61, 62,45, 59,119]  # P4/16
  - [116,90, 156,198, 373,326]  # P5/32

# YOLOv5 v6.0 backbone
backbone:
  # [from, number, module, args]
  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2
   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4
   [-1, 3, C3, [128]],
   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8
   [-1, 6, C3, [256]],
   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16
   [-1, 9, C3, [512]],
   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32
   [-1, 3, C3, [1024]],
   [-1, 3, DilateBlock, [1024]],
   [-1, 1, SPPF, [1024, 5]],  # 9

  ]

# YOLOv5 v6.0 head
head:
  [[-1, 1, Conv, [512, 1, 1]],
   [-1, 1, nn.Upsample, [None, 2, 'nearest']],
   [[-1, 6], 1, Concat, [1]],  # cat backbone P4
   [-1, 3, C3, [512, False]],  # 13

   [-1, 1, Conv, [256, 1, 1]],
   [-1, 1, nn.Upsample, [None, 2, 'nearest']],
   [[-1, 4], 1, Concat, [1]],  # cat backbone P3
   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)

   [-1, 1, Conv, [256, 3, 2]],
   [[-1, 14], 1, Concat, [1]],  # cat head P4
   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)

   [-1, 1, Conv, [512, 3, 2]],
   [[-1, 10], 1, Concat, [1]],  # cat head P5
   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)

   [[18, 21, 24], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)
  ]
```

ç¬¬2ç§ï¼Œæ›¿æ¢C3æ¨¡å—

```java
# YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license

# Parameters
nc: 80  # number of classes
depth_multiple: 0.33  # model depth multiple
width_multiple: 0.50  # layer channel multiple
anchors:
  - [10,13, 16,30, 33,23]  # P3/8
  - [30,61, 62,45, 59,119]  # P4/16
  - [116,90, 156,198, 373,326]  # P5/32

# YOLOv5 v6.0 backbone
backbone:
  # [from, number, module, args]
  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2
   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4
   [-1, 3, C3, [128]],
   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8
   [-1, 6, C3, [256]],
   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16
   [-1, 9, C3, [512]],
   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32
   [-1, 3, C3, [1024]],
   [-1, 1, SPPF, [1024, 5]],  # 9

  ]

# YOLOv5 v6.0 head
head:
  [[-1, 1, Conv, [512, 1, 1]],
   [-1, 1, nn.Upsample, [None, 2, 'nearest']],
   [[-1, 6], 1, Concat, [1]],  # cat backbone P4
   [-1, 3, C3_DilateBlock, [512, False]],  # 13

   [-1, 1, Conv, [256, 1, 1]],
   [-1, 1, nn.Upsample, [None, 2, 'nearest']],
   [[-1, 4], 1, Concat, [1]],  # cat backbone P3
   [-1, 3, C3_DilateBlock, [256, False]],  # 17 (P3/8-small)

   [-1, 1, Conv, [256, 3, 2]],
   [[-1, 14], 1, Concat, [1]],  # cat head P4
   [-1, 3, C3_DilateBlock, [512, False]],  # 20 (P4/16-medium)

   [-1, 1, Conv, [512, 3, 2]],
   [[-1, 10], 1, Concat, [1]],  # cat head P5
   [-1, 3, C3_DilateBlock, [1024, False]],  # 23 (P5/32-large)

   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)
  ]
```

#### ç¬¬â‘£æ­¥ï¼šéªŒè¯æ˜¯å¦åŠ å…¥æˆåŠŸ 

è¿è¡Œyolo.py

ç¬¬1ç§

![](https://i-blog.csdnimg.cn/blog_migrate/59b0d2b61f1564b0ec74987ed9042f66.png)ç¬¬2ç§

![](https://i-blog.csdnimg.cn/blog_migrate/a19c96ffd03a7cee00642e2d5a3ed206.png) è¿™æ ·å°±OKå•¦ï¼

## ğŸŒŸæœ¬äººYOLOv5ç³»åˆ—å¯¼èˆª 

![962f7cb1b48f44e29d9beb1d499d0530.gif](https://i-blog.csdnimg.cn/blog_migrate/ac3c5d6bfbcbf982e8e9e3632d7f20d1.gif) ğŸ€[YOLOv5æºç ][YOLOv5 1]è¯¦è§£ç³»åˆ—ï¼š

[YOLOv5æºç é€è¡Œè¶…è¯¦ç»†æ³¨é‡Šä¸è§£è¯»ï¼ˆ1ï¼‰â€”â€”é¡¹ç›®ç›®å½•ç»“æ„è§£æ][YOLOv5_1]

[YOLOv5æºç é€è¡Œè¶…è¯¦ç»†æ³¨é‡Šä¸è§£è¯»ï¼ˆ2ï¼‰â€”â€”æ¨ç†éƒ¨åˆ†detect.py][YOLOv5_2_detect.py]

[YOLOv5æºç é€è¡Œè¶…è¯¦ç»†æ³¨é‡Šä¸è§£è¯»ï¼ˆ3ï¼‰â€”â€”è®­ç»ƒéƒ¨åˆ†train.py][YOLOv5_3_train.py]

[YOLOv5æºç é€è¡Œè¶…è¯¦ç»†æ³¨é‡Šä¸è§£è¯»ï¼ˆ4ï¼‰â€”â€”éªŒè¯éƒ¨åˆ†valï¼ˆtestï¼‰.py][YOLOv5_4_val_test_.py]

[YOLOv5æºç é€è¡Œè¶…è¯¦ç»†æ³¨é‡Šä¸è§£è¯»ï¼ˆ5ï¼‰â€”â€”é…ç½®æ–‡ä»¶yolov5s.yaml][YOLOv5_5_yolov5s.yaml]

[YOLOv5æºç é€è¡Œè¶…è¯¦ç»†æ³¨é‡Šä¸è§£è¯»ï¼ˆ6ï¼‰â€”â€”ç½‘ç»œç»“æ„ï¼ˆ1ï¼‰yolo.py][YOLOv5_6_1_yolo.py]

[YOLOv5æºç é€è¡Œè¶…è¯¦ç»†æ³¨é‡Šä¸è§£è¯»ï¼ˆ7ï¼‰â€”â€”ç½‘ç»œç»“æ„ï¼ˆ2ï¼‰common.py][YOLOv5_7_2_common.py]

![962f7cb1b48f44e29d9beb1d499d0530.gif](https://i-blog.csdnimg.cn/blog_migrate/ac3c5d6bfbcbf982e8e9e3632d7f20d1.gif) ğŸ€[YOLOv5å…¥é—¨å®è·µ][YOLOv5 1]ç³»åˆ—ï¼š

[YOLOv5å…¥é—¨å®è·µï¼ˆ1ï¼‰â€”â€”æ‰‹æŠŠæ‰‹å¸¦ä½ ç¯å¢ƒé…ç½®æ­å»º][YOLOv5_1 1]

[YOLOv5å…¥é—¨å®è·µï¼ˆ2ï¼‰â€”â€”æ‰‹æŠŠæ‰‹æ•™ä½ åˆ©ç”¨labelimgæ ‡æ³¨æ•°æ®é›†][YOLOv5_2_labelimg]

[YOLOv5å…¥é—¨å®è·µï¼ˆ3ï¼‰â€”â€”æ‰‹æŠŠæ‰‹æ•™ä½ åˆ’åˆ†è‡ªå·±çš„æ•°æ®é›†][YOLOv5_3]

[YOLOv5å…¥é—¨å®è·µï¼ˆ4ï¼‰â€”â€”æ‰‹æŠŠæ‰‹æ•™ä½ è®­ç»ƒè‡ªå·±çš„æ•°æ®é›†][YOLOv5_4]

[YOLOv5å…¥é—¨å®è·µï¼ˆ5ï¼‰â€”â€”ä»é›¶å¼€å§‹ï¼Œæ‰‹æŠŠæ‰‹æ•™ä½ è®­ç»ƒè‡ªå·±çš„ç›®æ ‡æ£€æµ‹æ¨¡å‹ï¼ˆåŒ…å«pyqt5ç•Œé¢ï¼‰][YOLOv5_5_pyqt5]

![](https://i-blog.csdnimg.cn/blog_migrate/c38d39259a7080120a8da1fdc2250ac5.png)


[YOLOv5_0]: https://blog.csdn.net/weixin_43334693/article/details/130564848?spm=1001.2014.3001.5501
[YOLOv5_1_SE]: https://blog.csdn.net/weixin_43334693/article/details/130551913?spm=1001.2014.3001.5501
[YOLOv5_2_CBAM]: https://blog.csdn.net/weixin_43334693/article/details/130587102?spm=1001.2014.3001.5501
[YOLOv5_3_CA]: https://blog.csdn.net/weixin_43334693/article/details/130619604?spm=1001.2014.3001.5501
[YOLOv5_4_ECA]: https://blog.csdn.net/weixin_43334693/article/details/130641318?spm=1001.2014.3001.5501
[YOLOv5_5_ MobileNetV3]: https://blog.csdn.net/weixin_43334693/article/details/130832933?spm=1001.2014.3001.5501
[YOLOv5_6_ ShuffleNetV2]: https://blog.csdn.net/weixin_43334693/article/details/131008642?spm=1001.2014.3001.5501
[YOLOv5_7_SimAM]: https://blog.csdn.net/weixin_43334693/article/details/131031541?spm=1001.2014.3001.5501
[YOLOv5_8_SOCA]: https://blog.csdn.net/weixin_43334693/article/details/131053284?spm=1001.2014.3001.5501
[YOLOv5_9_EfficientNetv2]: https://blog.csdn.net/weixin_43334693/article/details/131207097?csdn_share_tail=%7B%22type%22%3A%22blog%22%2C%22rType%22%3A%22article%22%2C%22rId%22%3A%22131207097%22%2C%22source%22%3A%22weixin_43334693%22%7D
[YOLOv5_10_GhostNet]: https://blog.csdn.net/weixin_43334693/article/details/131235113?spm=1001.2014.3001.5501
[YOLOv5_11_EIoU_AlphaIoU_SIoU_WIoU]: https://blog.csdn.net/weixin_43334693/article/details/131350224?spm=1001.2014.3001.5501
[YOLOv5_12_Neck_BiFPN]: https://blog.csdn.net/weixin_43334693/article/details/131461294?spm=1001.2014.3001.5501
[YOLOv5_13_SiLU_ReLU_ELU_Hardswish_Mish_Softplus_AconC]: https://blog.csdn.net/weixin_43334693/article/details/131513850?spm=1001.2014.3001.5502
[YOLOv5_14_NMS_ DIoU-NMS_CIoU-NMS_EIoU-NMS_GIoU-NMS _SIoU-NMS_Soft-NMS]: https://blog.csdn.net/weixin_43334693/article/details/131552028?spm=1001.2014.3001.5501
[YOLOv5_15]: https://blog.csdn.net/weixin_43334693/article/details/131613721?spm=1001.2014.3001.5502
[YOLOv5_16_EMA_ICASSP2023]: https://blog.csdn.net/weixin_43334693/article/details/131973273?spm=1001.2014.3001.5501
[YOLOv5_17_IoU_MPDIoU_ELSEVIER 2023_WIoU_EIoU]: https://blog.csdn.net/weixin_43334693/article/details/131999141?spm=1001.2014.3001.5501
[YOLOv5_18_Neck_AFPN_PAFPN]: https://blog.csdn.net/weixin_43334693/article/details/132070079?spm=1001.2014.3001.5501
[YOLOv5_19_Swin TransformerV1_ViT]: https://blog.csdn.net/weixin_43334693/article/details/132161488?spm=1001.2014.3001.5501
[YOLOv5_21_RepViT_ ICCV 2023_ViT]: https://blog.csdn.net/weixin_43334693/article/details/132211831?spm=1001.2014.3001.5501
[YOLOv5_22_MobileViTv1_ ViT]: https://blog.csdn.net/weixin_43334693/article/details/132367429?csdn_share_tail=%7B%22type%22%3A%22blog%22%2C%22rType%22%3A%22article%22%2C%22rId%22%3A%22132367429%22%2C%22source%22%3A%22weixin_43334693%22%7D
[YOLOv5_23_MobileViTv2_ Transformer]: https://blog.csdn.net/weixin_43334693/article/details/132428203?spm=1001.2014.3001.5502
[YOLOv5_24_MobileViTv3]: https://blog.csdn.net/weixin_43334693/article/details/133199471?spm=1001.2014.3001.5502
[YOLOv5_25_LSKNet]: https://blog.csdn.net/weixin_43334693/article/details/135510571?spm=1001.2014.3001.5501
[YOLOv5_26_RFAConv]: https://blog.csdn.net/weixin_43334693/article/details/135562865?spm=1001.2014.3001.5501
[YOLOv5_27_SCConv_CVPR 2023]: https://blog.csdn.net/weixin_43334693/article/details/135610505?spm=1001.2014.3001.5502
[YOLOv5_28_DSConv_ICCV 2023]: https://blog.csdn.net/weixin_43334693/article/details/135758781?spm=1001.2014.3001.5502
[_DilateFormer]: #%F0%9F%9A%80%C2%A0%E4%B8%80%E3%80%81DSConv%E4%BB%8B%E7%BB%8D%C2%A0
[1.1 DilateFormer_]: #1.1%C2%A0DilateFormer%E7%AE%80%E4%BB%8B%C2%A0
[1.2 DilateFormer]: #1.2%C2%A0%C2%A0DilateFormer%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84
[DilateFormer]: #%E2%91%A0%C2%A0DilateFormer
[MSDA]: #%C2%A0%E2%91%A1%20MSDA
[Link 1]: #%F0%9F%9A%80%E4%BA%8C%E3%80%81%E5%85%B7%E4%BD%93%E6%B7%BB%E5%8A%A0%E6%96%B9%E6%B3%95
[2.1 _]: #2.1%20%E6%B7%BB%E5%8A%A0%E9%A1%BA%E5%BA%8F%C2%A0
[2.2 _]: #2.2%20%E5%85%B7%E4%BD%93%E6%B7%BB%E5%8A%A0%E6%AD%A5%E9%AA%A4%C2%A0%C2%A0
[common.py_DilateFormer_]: #%E7%AC%AC%E2%91%A0%E6%AD%A5%EF%BC%9A%E5%9C%A8common.py%E4%B8%AD%E6%B7%BB%E5%8A%A0LSK%E6%A8%A1%E5%9D%97%C2%A0
[yolo.py_]: #%E7%AC%AC%E2%91%A1%E6%AD%A5%EF%BC%9A%E4%BF%AE%E6%94%B9yolo.py%E6%96%87%E4%BB%B6%C2%A0
[yaml_]: #%C2%A0%E7%AC%AC%E2%91%A2%E6%AD%A5%EF%BC%9A%E5%88%9B%E5%BB%BA%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84yaml%E6%96%87%E4%BB%B6%C2%A0%C2%A0%C2%A0%C2%A0
[Link 2]: #%C2%A0%E7%AC%AC%E2%91%A3%E6%AD%A5%EF%BC%9A%E9%AA%8C%E8%AF%81%E6%98%AF%E5%90%A6%E5%8A%A0%E5%85%A5%E6%88%90%E5%8A%9F
[YOLOv5]: #%F0%9F%8C%9F%E6%9C%AC%E4%BA%BAYOLOv5%E7%B3%BB%E5%88%97%E5%AF%BC%E8%88%AA
[Transformer]: https://so.csdn.net/so/search?q=Transformer&spm=1001.2101.3001.7020
[https_arxiv.org_abs_2302.01791]: https://arxiv.org/abs/2302.01791
[https_github.com_abhinavsagar_mssa]: https://github.com/abhinavsagar/mssa
[YOLOv5 1]: https://so.csdn.net/so/search?q=YOLOv5%E6%BA%90%E7%A0%81&spm=1001.2101.3001.7020
[YOLOv5_1]: https://blog.csdn.net/weixin_43334693/article/details/129356033?spm=1001.2014.3001.5501
[YOLOv5_2_detect.py]: https://blog.csdn.net/weixin_43334693/article/details/129349094?spm=1001.2014.3001.5501
[YOLOv5_3_train.py]: https://blog.csdn.net/weixin_43334693/article/details/129460666?spm=1001.2014.3001.5501
[YOLOv5_4_val_test_.py]: https://blog.csdn.net/weixin_43334693/article/details/129649553?spm=1001.2014.3001.5501
[YOLOv5_5_yolov5s.yaml]: https://blog.csdn.net/weixin_43334693/article/details/129697521?spm=1001.2014.3001.5501
[YOLOv5_6_1_yolo.py]: https://blog.csdn.net/weixin_43334693/article/details/129803802?spm=1001.2014.3001.5501
[YOLOv5_7_2_common.py]: https://blog.csdn.net/weixin_43334693/article/details/129854764?spm=1001.2014.3001.5501
[YOLOv5_1 1]: https://blog.csdn.net/weixin_43334693/article/details/129981848?spm=1001.2014.3001.5501
[YOLOv5_2_labelimg]: https://blog.csdn.net/weixin_43334693/article/details/129995604?spm=1001.2014.3001.5501
[YOLOv5_3]: https://blog.csdn.net/weixin_43334693/article/details/130025866?spm=1001.2014.3001.5501
[YOLOv5_4]: https://blog.csdn.net/weixin_43334693/article/details/130043351?spm=1001.2014.3001.5501
[YOLOv5_5_pyqt5]: https://blog.csdn.net/weixin_43334693/article/details/130044342?spm=1001.2014.3001.5501