![](https://i-blog.csdnimg.cn/blog_migrate/8d65a1a503d26620f6fa4568ed0befff.gif)

![](https://i-blog.csdnimg.cn/blog_migrate/5530c8f1cbbf436a257d22aa7035fc4f.png)

![962f7cb1b48f44e29d9beb1d499d0530.gif](https://i-blog.csdnimg.cn/blog_migrate/ac3c5d6bfbcbf982e8e9e3632d7f20d1.gif)ã€YOLOv5æ”¹è¿›ç³»åˆ—ã€‘å‰æœŸå›é¡¾ï¼š

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ0ï¼‰â€”â€”é‡è¦æ€§èƒ½æŒ‡æ ‡ä¸è®­ç»ƒç»“æœè¯„ä»·åŠåˆ†æ][YOLOv5_0]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ1ï¼‰â€”â€”æ·»åŠ SEæ³¨æ„åŠ›æœºåˆ¶][YOLOv5_1_SE]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ2ï¼‰â€”â€”æ·»åŠ CBAMæ³¨æ„åŠ›æœºåˆ¶][YOLOv5_2_CBAM]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ3ï¼‰â€”â€”æ·»åŠ CAæ³¨æ„åŠ›æœºåˆ¶][YOLOv5_3_CA]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ4ï¼‰â€”â€”æ·»åŠ ECAæ³¨æ„åŠ›æœºåˆ¶][YOLOv5_4_ECA]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ5ï¼‰â€”â€”æ›¿æ¢ä¸»å¹²ç½‘ç»œä¹‹ MobileNetV3][YOLOv5_5_ MobileNetV3]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ6ï¼‰â€”â€”æ›¿æ¢ä¸»å¹²ç½‘ç»œä¹‹ ShuffleNetV2][YOLOv5_6_ ShuffleNetV2]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ7ï¼‰â€”â€”æ·»åŠ SimAMæ³¨æ„åŠ›æœºåˆ¶][YOLOv5_7_SimAM]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ8ï¼‰â€”â€”æ·»åŠ SOCAæ³¨æ„åŠ›æœºåˆ¶][YOLOv5_8_SOCA]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ9ï¼‰â€”â€”æ›¿æ¢ä¸»å¹²ç½‘ç»œä¹‹EfficientNetv2][YOLOv5_9_EfficientNetv2]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ10ï¼‰â€”â€”æ›¿æ¢ä¸»å¹²ç½‘ç»œä¹‹GhostNet][YOLOv5_10_GhostNet]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ11ï¼‰â€”â€”æ·»åŠ æŸå¤±å‡½æ•°ä¹‹EIoUã€AlphaIoUã€SIoUã€WIoU][YOLOv5_11_EIoU_AlphaIoU_SIoU_WIoU]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ12ï¼‰â€”â€”æ›´æ¢Neckä¹‹BiFPN][YOLOv5_12_Neck_BiFPN]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ13ï¼‰â€”â€”æ›´æ¢æ¿€æ´»å‡½æ•°ä¹‹SiLUï¼ŒReLUï¼ŒELUï¼ŒHardswishï¼ŒMishï¼ŒSoftplusï¼ŒAconCç³»åˆ—ç­‰][YOLOv5_13_SiLU_ReLU_ELU_Hardswish_Mish_Softplus_AconC]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ14ï¼‰â€”â€”æ›´æ¢NMSï¼ˆéæå¤§æŠ‘åˆ¶ï¼‰ä¹‹ DIoU-NMSã€CIoU-NMSã€EIoU-NMSã€GIoU-NMS ã€SIoU-NMSã€Soft-NMS][YOLOv5_14_NMS_ DIoU-NMS_CIoU-NMS_EIoU-NMS_GIoU-NMS _SIoU-NMS_Soft-NMS][YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ15ï¼‰â€”â€”å¢åŠ å°ç›®æ ‡æ£€æµ‹å±‚][YOLOv5_15]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ16ï¼‰â€”â€”æ·»åŠ EMAæ³¨æ„åŠ›æœºåˆ¶ï¼ˆICASSP2023|å®æµ‹æ¶¨ç‚¹ï¼‰][YOLOv5_16_EMA_ICASSP2023]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ17ï¼‰â€”â€”æ›´æ¢IoUä¹‹MPDIoUï¼ˆELSEVIER 2023|è¶…è¶ŠWIoUã€EIoUç­‰|å®æµ‹æ¶¨ç‚¹ï¼‰][YOLOv5_17_IoU_MPDIoU_ELSEVIER 2023_WIoU_EIoU]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ18ï¼‰â€”â€”æ›´æ¢Neckä¹‹AFPNï¼ˆå…¨æ–°æ¸è¿›ç‰¹å¾é‡‘å­—å¡”|è¶…è¶ŠPAFPN|å®æµ‹æ¶¨ç‚¹ï¼‰][YOLOv5_18_Neck_AFPN_PAFPN]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ19ï¼‰â€”â€”æ›¿æ¢ä¸»å¹²ç½‘ç»œä¹‹Swin TransformerV1ï¼ˆå‚æ•°é‡æ›´å°çš„ViTæ¨¡å‹ï¼‰][YOLOv5_19_Swin TransformerV1_ViT]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ20ï¼‰â€”â€”æ·»åŠ BiFormeræ³¨æ„åŠ›æœºåˆ¶ï¼ˆCVPR2023|å°ç›®æ ‡æ¶¨ç‚¹ç¥å™¨ï¼‰][YOLOv5_20_BiFormer_CVPR2023]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ21ï¼‰â€”â€”æ›¿æ¢ä¸»å¹²ç½‘ç»œä¹‹RepViTï¼ˆæ¸…å ICCV 2023|æœ€æ–°å¼€æºç§»åŠ¨ç«¯ViTï¼‰][YOLOv5_21_RepViT_ ICCV 2023_ViT]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ22ï¼‰â€”â€”æ›¿æ¢ä¸»å¹²ç½‘ç»œä¹‹MobileViTv1ï¼ˆä¸€ç§è½»é‡çº§çš„ã€é€šç”¨çš„ç§»åŠ¨è®¾å¤‡ ViTï¼‰][YOLOv5_22_MobileViTv1_ ViT]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ23ï¼‰â€”â€”æ›¿æ¢ä¸»å¹²ç½‘ç»œä¹‹MobileViTv2ï¼ˆç§»åŠ¨è§†è§‰ Transformer çš„é«˜æ•ˆå¯åˆ†ç¦»è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼‰][YOLOv5_23_MobileViTv2_ Transformer]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ24ï¼‰â€”â€”æ›¿æ¢ä¸»å¹²ç½‘ç»œä¹‹MobileViTv3ï¼ˆç§»åŠ¨ç«¯è½»é‡åŒ–ç½‘ç»œçš„è¿›ä¸€æ­¥å‡çº§ï¼‰][YOLOv5_24_MobileViTv3]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ25ï¼‰â€”â€”æ·»åŠ LSKNetæ³¨æ„åŠ›æœºåˆ¶ï¼ˆå¤§é€‰æ‹©æ€§å·ç§¯æ ¸çš„é¢†åŸŸé¦–æ¬¡æ¢ç´¢ï¼‰][YOLOv5_25_LSKNet]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ26ï¼‰â€”â€”æ·»åŠ RFAConvæ³¨æ„åŠ›å·ç§¯ï¼ˆæ„Ÿå—é‡æ³¨æ„åŠ›å·ç§¯è¿ç®—ï¼‰][YOLOv5_26_RFAConv]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ27ï¼‰â€”â€”æ·»åŠ SCConvæ³¨æ„åŠ›å·ç§¯ï¼ˆCVPR 2023|å³æ’å³ç”¨çš„é«˜æ•ˆå·ç§¯æ¨¡å—ï¼‰][YOLOv5_27_SCConv_CVPR 2023]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ28ï¼‰â€”â€”æ·»åŠ DSConvæ³¨æ„åŠ›å·ç§¯ï¼ˆICCV 2023|ç”¨äºç®¡çŠ¶ç»“æ„åˆ†å‰²çš„åŠ¨æ€è›‡å½¢å·ç§¯ï¼‰][YOLOv5_28_DSConv_ICCV 2023]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ29ï¼‰â€”â€”æ·»åŠ DilateFormerï¼ˆMSDAï¼‰æ³¨æ„åŠ›æœºåˆ¶ï¼ˆä¸­ç§‘é™¢ä¸€åŒºé¡¶åˆŠ|å³æ’å³ç”¨çš„å¤šå°ºåº¦å…¨å±€æ³¨æ„åŠ›æœºåˆ¶ï¼‰][YOLOv5_29_DilateFormer_MSDA]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ30ï¼‰â€”â€”æ·»åŠ iRMBæ³¨æ„åŠ›æœºåˆ¶ï¼ˆICCV 2023|å³æ’å³ç”¨çš„åå‘æ®‹å·®æ³¨æ„åŠ›æœºåˆ¶ï¼‰][YOLOv5_30_iRMB_ICCV 2023]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ31ï¼‰â€”â€”æ·»åŠ Dual-ViTæ³¨æ„åŠ›æœºåˆ¶ï¼ˆTPAMI 2023|äº¬ä¸œæå‡ºå¤šå°ºåº¦åŒè§†è§‰Transformerï¼Œé™ä½è®¡ç®—å¼€é”€ï¼‰][YOLOv5_31_Dual-ViT_TPAMI 2023_Transformer]

![](https://i-blog.csdnimg.cn/blog_migrate/2fd638cb6d5e991c328452d9bd850c95.gif)

ç›®å½•

[ğŸš€ ä¸€ã€PKINetä»‹ç» ][_PKINet_]

[1.1 PKINetç®€ä»‹][1.1 PKINet]

[1.2 PKINetæ–¹æ³•][1.2 PKINet]

[1.2.1 PKIæ¨¡å—][1.2.1 PKI]

[1.2.1 ä¸Šä¸‹æ–‡é”šç‚¹æ³¨æ„åŠ›ï¼ˆCAAï¼‰][1.2.1 _CAA]

[ğŸš€äºŒã€å…·ä½“æ·»åŠ æ–¹æ³•][Link 1]

[2.1 æ·»åŠ é¡ºåº ][2.1 _]

[2.2 å…·ä½“æ·»åŠ æ­¥éª¤ ][2.2 _]

[ç¬¬â‘ æ­¥ï¼šåœ¨modelsæ–‡ä»¶ä¸‹ï¼Œåˆ›å»ºPKINet.py][models_PKINet.py]

[ ç¬¬â‘¡æ­¥ï¼šä¿®æ”¹yolo.pyæ–‡ä»¶ ][_yolo.py_]

[ç¬¬â‘¢æ­¥ï¼šåˆ›å»ºè‡ªå®šä¹‰çš„yamlæ–‡ä»¶ ][yaml_]

[ç¬¬â‘£æ­¥ï¼šéªŒè¯æ˜¯å¦åŠ å…¥æˆåŠŸ][Link 2]

[ğŸŒŸæœ¬äººYOLOv5ç³»åˆ—å¯¼èˆª][YOLOv5]

![](https://i-blog.csdnimg.cn/blog_migrate/3bbaaddf0a85102f3a6bfa5319e7d6ca.gif)

## ğŸš€ ä¸€ã€PKINetä»‹ç» 

> å­¦ä¹ èµ„æ–™ï¼š
> 
>  *  è®ºæ–‡é¢˜ç›®ï¼šã€ŠPoly Kernel Inception Network for Remote Sensing Detectionã€‹
>  *  è®ºæ–‡åœ°å€ï¼š[https://export.arxiv.org/pdf/2403.06258][https_export.arxiv.org_pdf_2403.06258]
>  *  æºç åœ°å€ï¼š[https://github.com/NUST-Machine-Intelligence-Laboratory/PKINet][https_github.com_NUST-Machine-Intelligence-Laboratory_PKINet]

### 1.1 PKINetç®€ä»‹ 

èƒŒæ™¯

é¥æ„Ÿå›¾åƒä¸­ç›®æ ‡æ£€æµ‹é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜åŒ…æ‹¬ï¼š

1.  ç›®æ ‡å°ºåº¦å˜åŒ–å¤§
2.  å¤šæ ·çš„ä¸Šä¸‹æ–‡ç¯å¢ƒ

ä¼ ç»Ÿæ–¹æ³•ï¼š

é€šè¿‡æ‰©å¤§ç½‘ç»œçš„ç©ºé—´æ„Ÿå—é‡æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œ

ä¸è¶³ï¼š

å¤§æ ¸å·ç§¯å®¹æ˜“å¼•å…¥èƒŒæ™¯å™ªå£°ï¼Œè€Œç©ºæ´å·ç§¯å¯èƒ½ç”Ÿæˆè¿‡äºç¨€ç–çš„ç‰¹å¾ã€‚

æœ¬æ–‡æå‡ºçš„æ–¹æ³•ï¼š

1.  å¤šå°ºåº¦å·ç§¯æ ¸ï¼šä½¿ç”¨æ— ç©ºæ´çš„å¤šå°ºåº¦å·ç§¯æ ¸æ¥æå–ä¸åŒå°ºåº¦çš„ç›®æ ‡ç‰¹å¾ï¼Œè¿™æœ‰åŠ©äºæ•è·ä¸åŒå°ºåº¦ä¸‹çš„ç›®æ ‡ä¿¡æ¯ï¼Œå¹¶ä¸”èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•è·å±€éƒ¨ä¸Šä¸‹æ–‡ã€‚
2.  Context Anchor Attentionï¼ˆCAAï¼‰æ¨¡å—ï¼šå¹¶è¡Œå¼•å…¥CAAæ¨¡å—ï¼Œç”¨äºæ•è·é•¿è·ç¦»çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚è¿™ä¸ªæ¨¡å—æœ‰åŠ©äºæé«˜ç›®æ ‡æ£€æµ‹çš„ç²¾åº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤šæ ·çš„ä¸Šä¸‹æ–‡ç¯å¢ƒæ—¶ã€‚

![](https://i-blog.csdnimg.cn/blog_migrate/926d9af6f057a21b7407bd53b90169d1.png)

### 1.2 PKINetæ–¹æ³• 

PKINet è®¾è®¡ä¸ºç‰¹å¾æå–çš„éª¨å¹²ç½‘ç»œï¼Œç±»ä¼¼äºVGGå’ŒResNetï¼Œç”±å››ä¸ªé˜¶æ®µç»„æˆã€‚

æ¯ä¸ªé˜¶æ®µé‡‡ç”¨äº†äº¤å‰é˜¶æ®µéƒ¨åˆ†ï¼ˆCSPï¼‰ç»“æ„ï¼Œå…¶ä¸­è¾“å…¥è¢«åˆ†æˆä¸¤æ¡è·¯å¾„ï¼š

 *  ä¸€æ¡è·¯å¾„æ˜¯ç®€å•çš„å‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ã€‚
 *  å¦ä¸€æ¡è·¯å¾„åŒ…æ‹¬ä¸€ç³»åˆ—PKIå—ï¼Œæ¯ä¸ªå—ç”±PKIæ¨¡å—å’ŒCAAæ¨¡å—ç»„æˆã€‚

è¿™ä¸¤æ¡è·¯å¾„çš„è¾“å‡ºè¢«æ‹¼æ¥åœ¨ä¸€èµ·ï¼Œå½¢æˆæ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºã€‚

![](https://i-blog.csdnimg.cn/blog_migrate/2eec1178f3288181a5fca388b8685c1e.png)

#### 1.2.1 PKIæ¨¡å— 

PKIæ¨¡å—æ˜¯ä¸€ç§Inceptioné£æ ¼çš„æ¨¡å—ï¼Œå®ƒé¦–å…ˆé€šè¿‡ä¸€ä¸ªå°æ ¸å·ç§¯æ¥æ•è·å±€éƒ¨ä¿¡æ¯ï¼Œç„¶ååˆ©ç”¨ä¸€ç³»åˆ—å¹¶è¡Œçš„æ·±åº¦å·ç§¯æ¥è·¨å¤šä¸ªå°ºåº¦æ•è·ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

ç¬¬lé˜¶æ®µä¸­ç¬¬nä¸ªPKIå—å†…çš„PKIæ¨¡å—å¯ä»¥æ•°å­¦åœ°è¡¨ç¤ºå¦‚ä¸‹ï¼š

![](https://i-blog.csdnimg.cn/blog_migrate/35f8e6a5a6cdd897a2c162ceb24ffdac.png)

ç„¶åï¼Œé€šè¿‡å¤§å°ä¸º1Ã—1çš„å·ç§¯æ¥èåˆå±€éƒ¨å’Œä¸Šä¸‹æ–‡ç‰¹å¾ï¼Œè¡¨å¾ä¸åŒé€šé“ä¹‹é—´çš„ç›¸äº’å…³ç³»ï¼š

![](https://i-blog.csdnimg.cn/blog_migrate/0f3b739994e1a9d53f71d5eb2bf82fb7.png)

#### 1.2.1 ä¸Šä¸‹æ–‡é”šç‚¹æ³¨æ„åŠ›ï¼ˆCAAï¼‰ 

ä¸ºäº†æ•è·é•¿è·ç¦»çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæœ¬æ–‡è¿›ä¸€æ­¥åœ¨PKIå—ä¸­é›†æˆäº†ä¸Šä¸‹æ–‡é”šç‚¹æ³¨æ„åŠ›ï¼ˆCAAï¼‰æ¨¡å—ã€‚CAAæ—¨åœ¨æŠŠæ¡è¿œè·ç¦»åƒç´ ä¹‹é—´çš„ä¸Šä¸‹æ–‡ç›¸äº’ä¾èµ–æ€§ï¼ŒåŒæ—¶å¢å¼ºä¸­å¿ƒç‰¹å¾ã€‚

ä»¥ç¬¬lé˜¶æ®µä¸­ç¬¬nä¸ªPKIå—å†…çš„CAAæ¨¡å—ä¸ºä¾‹ï¼Œé‡‡ç”¨å¹³å‡æ± åŒ–åæ¥ä¸€ä¸ª1Ã—1å·ç§¯æ¥è·å–å±€éƒ¨åŒºåŸŸç‰¹å¾ï¼š

![](https://i-blog.csdnimg.cn/blog_migrate/2059747c5e1b783df2daea633e72c309.png)

ç„¶åï¼Œä½¿ç”¨ä¸¤ä¸ªæ·±åº¦å¯åˆ†ç¦»å·ç§¯ä½œä¸ºæ ‡å‡†å¤§æ ¸æ·±åº¦å·ç§¯çš„è¿‘ä¼¼ï¼š

![](https://i-blog.csdnimg.cn/blog_migrate/cda5b935abbfe5c95eadd7b68a0bf43f.png)

é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒCAAæ¨¡å—èƒ½å¤Ÿå¢å¼ºæ¨¡å‹å¯¹è¿œè·ç¦»ä¾èµ–æ€§çš„å»ºæ¨¡èƒ½åŠ›ï¼Œä»è€Œæå‡é¥æ„Ÿç›®æ ‡æ£€æµ‹çš„æ€§èƒ½ã€‚

æœ€åï¼Œæˆ‘ä»¬çš„CAAæ¨¡å—ç”Ÿæˆä¸€ä¸ªæ³¨æ„åŠ›æƒé‡![A_{l-1}, n\in R^{\frac{1}{2}C_{l}\times W_{l}\times H_{l}}](https://latex.csdn.net/eq?A_%7Bl-1%7D%2C%20n%5Cin%20R%5E%7B%5Cfrac%7B1%7D%7B2%7DC_%7Bl%7D%5Ctimes%20W_%7Bl%7D%5Ctimes%20H_%7Bl%7D%7D)ï¼Œè¯¥æƒé‡è¿›ä¸€æ­¥ç”¨äºå¢å¼ºPKIæ¨¡å—çš„è¾“å‡ºï¼ˆå‚è§å¼(4)ï¼‰ï¼š

![](https://i-blog.csdnimg.cn/blog_migrate/0a9ec3a65fbadfa8fe20b4a5ea214572.png)

ç¬¬lé˜¶æ®µä¸­ç¬¬nä¸ªPKIå—çš„è¾“å‡ºæ˜¯é€šè¿‡ä»¥ä¸‹æ–¹å¼è·å¾—çš„ï¼š

![](https://i-blog.csdnimg.cn/blog_migrate/aed49ecdb2e183ad24abf637e3a4ff37.png)

## ğŸš€äºŒã€å…·ä½“æ·»åŠ æ–¹æ³• 

### 2.1 æ·»åŠ é¡ºåº 

ï¼ˆ1ï¼‰models/PKINet.py -->  åˆ›å»ºæ–°æ–‡ä»¶ï¼Œæ–°å¢çš„ç½‘ç»œç»“æ„

ï¼ˆ2ï¼‰ models/yolo.py --> è®¾å®šç½‘ç»œç»“æ„çš„ä¼ å‚ç»†èŠ‚ï¼Œå¼•å…¥PKINet  
ï¼ˆ3ï¼‰ models/yolov5\*.yaml -->  æ–°å»ºä¸€ä¸ªæ–‡ä»¶å¤¹ï¼Œå¦‚yolov5s\_PKINet.yamlï¼Œä¿®æ”¹ç°æœ‰æ¨¡å‹ç»“æ„é…ç½®æ–‡ä»¶ã€‚ï¼ˆå½“å¼•å…¥æ–°çš„å±‚æ—¶ï¼Œè¦ä¿®æ”¹åç»­çš„ç»“æ„ä¸­çš„fromå‚æ•°ï¼‰  
ï¼ˆ4ï¼‰ train.py --> ä¿®æ”¹â€˜--cfgâ€™é»˜è®¤å‚æ•°ï¼Œè®­ç»ƒæ—¶æŒ‡å®šæ¨¡å‹ç»“æ„é…ç½®æ–‡ä»¶

### 2.2 å…·ä½“æ·»åŠ æ­¥éª¤ 

#### ç¬¬â‘ æ­¥ï¼šåœ¨modelsæ–‡ä»¶ä¸‹ï¼Œåˆ›å»ºPKINet.py 

![](https://i-blog.csdnimg.cn/blog_migrate/94da8d4b1778686155ab498c1cb27d4e.png)

åœ¨githubä¸Šæ‰¾åˆ°å¯¹åº”æºç æ–‡ä»¶ï¼Œåšå¦‚ä¸‹ä¿®æ”¹ï¼š

```java
import math
from typing import Optional, Union, Sequence

import torch
import torch.nn as nn
from torch.nn.modules.batchnorm import _BatchNorm
from mmcv.cnn import ConvModule, build_norm_layer
from mmcv.cnn.bricks import DropPath
from mmengine.model import BaseModule, constant_init
from mmengine.model.weight_init import trunc_normal_init, normal_init
from mmengine.logging import MMLogger

from models.common import C3,Conv
# from mmrotate.models.builder import ROTATED_BACKBONES
# from utils import autopad, make_divisible, BHWC2BCHW, BCHW2BHWC

 
def autopad(kernel_size: int, padding: int = None, dilation: int = 1):
    assert kernel_size % 2 == 1, 'if use autopad, kernel size must be odd'
    if dilation > 1:
        kernel_size = dilation * (kernel_size - 1) + 1
    if padding is None:
        padding = kernel_size // 2
    return padding


def make_divisible(value, divisor, min_value=None, min_ratio=0.9):
    """Make divisible function.
    This function rounds the channel number to the nearest value that can be
    divisible by the divisor. It is taken from the original tf repo. It ensures
    that all layers have a channel number that is divisible by divisor. It can
    be seen here: https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py  # noqa
    Args:
        value (int, float): The original channel number.
        divisor (int): The divisor to fully divide the channel number.
        min_value (int): The minimum value of the output channel.
            Default: None, means that the minimum value equal to the divisor.
        min_ratio (float): The minimum ratio of the rounded channel number to
            the original channel number. Default: 0.9.
    Returns:
        int: The modified output channel number.
    """
 
    if min_value is None:
        min_value = divisor
    new_value = max(min_value, int(value + divisor / 2) // divisor * divisor)
    # Make sure that round down does not go down by more than (1-min_ratio).
    if new_value < min_ratio * value:
        new_value += divisor
    return new_value
 
class BCHW2BHWC(nn.Module):
    def __init__(self):
        super().__init__()
 
    @staticmethod
    def forward(x):
        return x.permute([0, 2, 3, 1])
 
 
class BHWC2BCHW(nn.Module):
    def __init__(self):
        super().__init__()
 
    @staticmethod
    def forward(x):
        return x.permute([0, 3, 1, 2])
    
class GSiLU(BaseModule):
    """Global Sigmoid-Gated Linear Unit, reproduced from paper <SIMPLE CNN FOR VISION>"""
    def __init__(self):
        super().__init__()
        self.adpool = nn.AdaptiveAvgPool2d(1)

    def forward(self, x):
        return x * torch.sigmoid(self.adpool(x))


class CAA(BaseModule):
    """Context Anchor Attention"""
    def __init__(
            self,
            channels: int,
            h_kernel_size: int = 11,
            v_kernel_size: int = 11,
            norm_cfg: Optional[dict] = dict(type='BN', momentum=0.03, eps=0.001),
            act_cfg: Optional[dict] = dict(type='SiLU'),
            init_cfg: Optional[dict] = None,
    ):
        super().__init__(init_cfg)
        self.avg_pool = nn.AvgPool2d(7, 1, 3)
        self.conv1 = ConvModule(channels, channels, 1, 1, 0,
                                norm_cfg=norm_cfg, act_cfg=act_cfg)
        self.h_conv = ConvModule(channels, channels, (1, h_kernel_size), 1,
                                 (0, h_kernel_size // 2), groups=channels,
                                 norm_cfg=None, act_cfg=None)
        self.v_conv = ConvModule(channels, channels, (v_kernel_size, 1), 1,
                                 (v_kernel_size // 2, 0), groups=channels,
                                 norm_cfg=None, act_cfg=None)
        self.conv2 = ConvModule(channels, channels, 1, 1, 0,
                                norm_cfg=norm_cfg, act_cfg=act_cfg)
        self.act = nn.Sigmoid()

    def forward(self, x):
        attn_factor = self.act(self.conv2(self.v_conv(self.h_conv(self.conv1(self.avg_pool(x))))))
        return attn_factor


class ConvFFN(BaseModule):
    """Multi-layer perceptron implemented with ConvModule"""
    def __init__(
            self,
            in_channels: int,
            out_channels: Optional[int] = None,
            hidden_channels_scale: float = 4.0,
            hidden_kernel_size: int = 3,
            dropout_rate: float = 0.,
            add_identity: bool = True,
            norm_cfg: Optional[dict] = dict(type='BN', momentum=0.03, eps=0.001),
            act_cfg: Optional[dict] = dict(type='SiLU'),
            init_cfg: Optional[dict] = None,
    ):
        super().__init__(init_cfg)
        out_channels = out_channels or in_channels
        hidden_channels = int(in_channels * hidden_channels_scale)

        self.ffn_layers = nn.Sequential(
            BCHW2BHWC(),
            nn.LayerNorm(in_channels),
            BHWC2BCHW(),
            ConvModule(in_channels, hidden_channels, kernel_size=1, stride=1, padding=0,
                       norm_cfg=norm_cfg, act_cfg=act_cfg),
            ConvModule(hidden_channels, hidden_channels, kernel_size=hidden_kernel_size, stride=1,
                       padding=hidden_kernel_size // 2, groups=hidden_channels,
                       norm_cfg=norm_cfg, act_cfg=None),
            GSiLU(),
            nn.Dropout(dropout_rate),
            ConvModule(hidden_channels, out_channels, kernel_size=1, stride=1, padding=0,
                       norm_cfg=norm_cfg, act_cfg=act_cfg),
            nn.Dropout(dropout_rate),
        )
        self.add_identity = add_identity

    def forward(self, x):
        x = x + self.ffn_layers(x) if self.add_identity else self.ffn_layers(x)
        return x


class Stem(BaseModule):
    """Stem layer"""
    def __init__(
            self,
            in_channels: int,
            out_channels: int,
            expansion: float = 1.0,
            norm_cfg: Optional[dict] = dict(type='BN', momentum=0.03, eps=0.001),
            act_cfg: Optional[dict] = dict(type='SiLU'),
            init_cfg: Optional[dict] = None,
    ):
        super().__init__(init_cfg)
        hidden_channels = make_divisible(int(out_channels * expansion), 8)

        self.down_conv = ConvModule(in_channels, hidden_channels, kernel_size=3, stride=2, padding=1,
                                    norm_cfg=norm_cfg, act_cfg=act_cfg)
        self.conv1 = ConvModule(hidden_channels, hidden_channels, kernel_size=3, stride=1, padding=1,
                                norm_cfg=norm_cfg, act_cfg=act_cfg)
        self.conv2 = ConvModule(hidden_channels, out_channels, kernel_size=3, stride=1, padding=1,
                                norm_cfg=norm_cfg, act_cfg=act_cfg)

    def forward(self, x):
        return self.conv2(self.conv1(self.down_conv(x)))


class DownSamplingLayer(BaseModule):
    """Down sampling layer"""
    def __init__(
            self,
            in_channels: int,
            out_channels: Optional[int] = None,
            norm_cfg: Optional[dict] = dict(type='BN', momentum=0.03, eps=0.001),
            act_cfg: Optional[dict] = dict(type='SiLU'),
            init_cfg: Optional[dict] = None,
    ):
        super().__init__(init_cfg)
        out_channels = out_channels or (in_channels * 2)

        self.down_conv = ConvModule(in_channels, out_channels, kernel_size=3, stride=2, padding=1,
                                    norm_cfg=norm_cfg, act_cfg=act_cfg)

    def forward(self, x):
        return self.down_conv(x)


class InceptionBottleneck(BaseModule):
    """Bottleneck with Inception module"""
    def __init__(
            self,
            in_channels: int,
            out_channels: Optional[int] = None,
            kernel_sizes: Sequence[int] = (3, 5, 7, 9, 11),
            dilations: Sequence[int] = (1, 1, 1, 1, 1),
            expansion: float = 1.0,
            add_identity: bool = True,
            with_caa: bool = True,
            caa_kernel_size: int = 11,
            norm_cfg: Optional[dict] = dict(type='BN', momentum=0.03, eps=0.001),
            act_cfg: Optional[dict] = dict(type='SiLU'),
            init_cfg: Optional[dict] = None,
    ):
        super().__init__(init_cfg)
        out_channels = out_channels or in_channels
        hidden_channels = make_divisible(int(out_channels * expansion), 8)

        self.pre_conv = ConvModule(in_channels, hidden_channels, 1, 1, 0, 1,
                                   norm_cfg=norm_cfg, act_cfg=act_cfg)

        self.dw_conv = ConvModule(hidden_channels, hidden_channels, kernel_sizes[0], 1,
                                  autopad(kernel_sizes[0], None, dilations[0]), dilations[0],
                                  groups=hidden_channels, norm_cfg=None, act_cfg=None)
        self.dw_conv1 = ConvModule(hidden_channels, hidden_channels, kernel_sizes[1], 1,
                                   autopad(kernel_sizes[1], None, dilations[1]), dilations[1],
                                   groups=hidden_channels, norm_cfg=None, act_cfg=None)
        self.dw_conv2 = ConvModule(hidden_channels, hidden_channels, kernel_sizes[2], 1,
                                   autopad(kernel_sizes[2], None, dilations[2]), dilations[2],
                                   groups=hidden_channels, norm_cfg=None, act_cfg=None)
        self.dw_conv3 = ConvModule(hidden_channels, hidden_channels, kernel_sizes[3], 1,
                                   autopad(kernel_sizes[3], None, dilations[3]), dilations[3],
                                   groups=hidden_channels, norm_cfg=None, act_cfg=None)
        self.dw_conv4 = ConvModule(hidden_channels, hidden_channels, kernel_sizes[4], 1,
                                   autopad(kernel_sizes[4], None, dilations[4]), dilations[4],
                                   groups=hidden_channels, norm_cfg=None, act_cfg=None)
        self.pw_conv = ConvModule(hidden_channels, hidden_channels, 1, 1, 0, 1,
                                  norm_cfg=norm_cfg, act_cfg=act_cfg)

        if with_caa:
            self.caa_factor = CAA(hidden_channels, caa_kernel_size, caa_kernel_size, None, None)
        else:
            self.caa_factor = None

        self.add_identity = add_identity and in_channels == out_channels

        self.post_conv = ConvModule(hidden_channels, out_channels, 1, 1, 0, 1,
                                    norm_cfg=norm_cfg, act_cfg=act_cfg)

    def forward(self, x):
        x = self.pre_conv(x)

        y = x  # if there is an inplace operation of x, use y = x.clone() instead of y = x
        x = self.dw_conv(x)
        x = x + self.dw_conv1(x) + self.dw_conv2(x) + self.dw_conv3(x) + self.dw_conv4(x)
        x = self.pw_conv(x)
        if self.caa_factor is not None:
            y = self.caa_factor(y)
        if self.add_identity:
            y = x * y
            x = x + y
        else:
            x = x * y

        x = self.post_conv(x)
        return x


class PKIBlock(BaseModule):
    """Poly Kernel Inception Block"""
    def __init__(
            self,
            in_channels: int,
            out_channels: Optional[int] = None,
            kernel_sizes: Sequence[int] = (3, 5, 7, 9, 11),
            dilations: Sequence[int] = (1, 1, 1, 1, 1),
            with_caa: bool = True,
            caa_kernel_size: int = 11,
            expansion: float = 1.0,
            ffn_scale: float = 4.0,
            ffn_kernel_size: int = 3,
            dropout_rate: float = 0.,
            drop_path_rate: float = 0.,
            layer_scale: Optional[float] = 1.0,
            add_identity: bool = True,
            norm_cfg: Optional[dict] = dict(type='BN', momentum=0.03, eps=0.001),
            act_cfg: Optional[dict] = dict(type='SiLU'),
            init_cfg: Optional[dict] = None,
    ):
        super().__init__(init_cfg)
        out_channels = out_channels or in_channels
        hidden_channels = make_divisible(int(out_channels * expansion), 8)

        if norm_cfg is not None:
            self.norm1 = build_norm_layer(norm_cfg, in_channels)[1]
            self.norm2 = build_norm_layer(norm_cfg, hidden_channels)[1]
        else:
            self.norm1 = nn.BatchNorm2d(in_channels)
            self.norm2 = nn.BatchNorm2d(hidden_channels)

        self.block = InceptionBottleneck(in_channels, hidden_channels, kernel_sizes, dilations,
                                         expansion=1.0, add_identity=True,
                                         with_caa=with_caa, caa_kernel_size=caa_kernel_size,
                                         norm_cfg=norm_cfg, act_cfg=act_cfg)
        self.ffn = ConvFFN(hidden_channels, out_channels, ffn_scale, ffn_kernel_size, dropout_rate, add_identity=False,
                           norm_cfg=None, act_cfg=None)
        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else nn.Identity()

        self.layer_scale = layer_scale
        if self.layer_scale:
            self.gamma1 = nn.Parameter(layer_scale * torch.ones(hidden_channels), requires_grad=True)
            self.gamma2 = nn.Parameter(layer_scale * torch.ones(out_channels), requires_grad=True)
        self.add_identity = add_identity and in_channels == out_channels

    def forward(self, x):
        if self.layer_scale:
            if self.add_identity:
                x = x + self.drop_path(self.gamma1.unsqueeze(-1).unsqueeze(-1) * self.block(self.norm1(x)))
                x = x + self.drop_path(self.gamma2.unsqueeze(-1).unsqueeze(-1) * self.ffn(self.norm2(x)))
            else:
                x = self.drop_path(self.gamma1.unsqueeze(-1).unsqueeze(-1) * self.block(self.norm1(x)))
                x = self.drop_path(self.gamma2.unsqueeze(-1).unsqueeze(-1) * self.ffn(self.norm2(x)))
        else:
            if self.add_identity:
                x = x + self.drop_path(self.block(self.norm1(x)))
                x = x + self.drop_path(self.ffn(self.norm2(x)))
            else:
                x = self.drop_path(self.block(self.norm1(x)))
                x = self.drop_path(self.ffn(self.norm2(x)))
        return x


class PKIStage(BaseModule):
    """Poly Kernel Inception Stage"""
    def __init__(
            self,
            in_channels: int,
            out_channels: int,
            num_blocks: int,
            kernel_sizes: Sequence[int] = (3, 5, 7, 9, 11),
            dilations: Sequence[int] = (1, 1, 1, 1, 1),
            expansion: float = 0.5,
            ffn_scale: float = 4.0,
            ffn_kernel_size: int = 3,
            dropout_rate: float = 0.,
            drop_path_rate: Union[float, list] = 0.,
            layer_scale: Optional[float] = 1.0,
            shortcut_with_ffn: bool = True,
            shortcut_ffn_scale: float = 4.0,
            shortcut_ffn_kernel_size: int = 5,
            add_identity: bool = True,
            with_caa: bool = True,
            caa_kernel_size: int = 11,
            norm_cfg: Optional[dict] = dict(type='BN', momentum=0.03, eps=0.001),
            act_cfg: Optional[dict] = dict(type='SiLU'),
            init_cfg: Optional[dict] = None,
    ):
        super().__init__(init_cfg)
        hidden_channels = make_divisible(int(out_channels * expansion), 8)

        self.downsample = DownSamplingLayer(in_channels, out_channels, norm_cfg, act_cfg)

        self.conv1 = ConvModule(out_channels, 2 * hidden_channels, kernel_size=1, stride=1, padding=0, dilation=1,
                                norm_cfg=norm_cfg, act_cfg=act_cfg)
        self.conv2 = ConvModule(2 * hidden_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1,
                                norm_cfg=norm_cfg, act_cfg=act_cfg)
        self.conv3 = ConvModule(out_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1,
                                norm_cfg=norm_cfg, act_cfg=act_cfg)

        self.ffn = ConvFFN(hidden_channels, hidden_channels, shortcut_ffn_scale, shortcut_ffn_kernel_size, 0.,
                           add_identity=True, norm_cfg=None, act_cfg=None) if shortcut_with_ffn else None

        self.blocks = nn.ModuleList([
            PKIBlock(hidden_channels, hidden_channels, kernel_sizes, dilations, with_caa,
                     caa_kernel_size+2*i, 1.0, ffn_scale, ffn_kernel_size, dropout_rate,
                     drop_path_rate[i] if isinstance(drop_path_rate, list) else drop_path_rate,
                     layer_scale, add_identity, norm_cfg, act_cfg) for i in range(num_blocks)
        ])

    def forward(self, x):
        x = self.downsample(x)

        x, y = list(self.conv1(x).chunk(2, 1))
        if self.ffn is not None:
            x = self.ffn(x)

        z = [x]
        t = torch.zeros(y.shape, device=y.device)
        for block in self.blocks:
            t = t + block(y)
        z.append(t)
        z = torch.cat(z, dim=1)
        z = self.conv2(z)
        z = self.conv3(z)

        return z
```

ç›´æ¥å¤åˆ¶ä¸Šè¿°ä»£ç ï¼Œç²˜è´´åˆ°æ–°å»ºçš„PKINet.py 

> å¯èƒ½é‡åˆ°çš„é—®é¢˜ä¸€ï¼šModuleNotFoundError: No module named 'mmcv'
> 
> ![](https://i-blog.csdnimg.cn/blog_migrate/7df235f431d74b3f8db3143e899c4602.png)
> 
> è§£å†³æ–¹æ³•ï¼š
> 
> ï¼ˆ1ï¼‰ç¬¬ä¸€æ­¥ï¼šæ›´æ–°pip
> 
> ```java
> python -m ensurepip --upgrade
> ```
> 
> ï¼ˆ2ï¼‰ç¬¬äºŒæ­¥ï¼šæ›´æ–°setuptools
> 
> ```java
> pip uninstall -y setuptoolsÂ 
> pip install setuptoolsÂ 
> ```
> 
> ï¼ˆ3ï¼‰ç¬¬ä¸‰æ­¥ï¼šæ ¹æ®å®˜æ–¹æ–‡æ¡£é€‰æ‹©å‘½ä»¤å®‰è£…
> 
> ```java
> â€‹â€‹â€‹â€‹â€‹â€‹Installation â€” mmcv 2.1.0 æ–‡æ¡£
> ```
> 
> æ¯”å¦‚ï¼š
> 
> ```java
> pip install -U openmim
> mim install mmcv
> pip install mmcv==2.1.0 -f https://download.openmmlab.com/mmcv/dist/cu121/torch2.1/index.html
> ```

> å¯èƒ½é‡åˆ°çš„é—®é¢˜äºŒï¼šAssertionError: if use autopad, kernel size must be odd![](https://i-blog.csdnimg.cn/blog_migrate/6b95fb9ee8e6ceff2388a929d40fcffe.png) ä¹‹å‰æˆ‘ä»¬ç”¨çš„æ–¹æ³•ä¸€ç›´æ˜¯åœ¨common.pyå°¾éƒ¨åŠ å…¥æ–°çš„ç½‘ç»œç»“æ„ï¼Œç„¶åå†yolo.pyè¡¨ä¸­æ³¨å†Œã€‚ä½†æ˜¯è¿™æ¬¡ç”¨åŒæ ·çš„æ–¹æ³•ä¸€ç¤ºå·ç§¯æ ¸å°ºå¯¸å¿…é¡»ä¸ºå¥‡æ•°ï¼ˆå¯æ˜¯æˆ‘çš„å·ç§¯æ ¸å°ºå¯¸æœ¬æ¥å°±æ˜¯å¥‡æ•°ã€‚ã€‚ã€‚è¿™é‡ŒæŸ¥äº†å¥½å¤šèµ„æ–™éƒ½ä¸çŸ¥é“ä¸ºå•¥ï¼‰ï¼Œæ‰€ä»¥å°±ç”¨æ–°å»ºæ–‡ä»¶çš„æ–¹æ³•äº†ã€‚

#### ç¬¬â‘¡æ­¥ï¼šä¿®æ”¹yolo.pyæ–‡ä»¶ 

åœ¨yolo.pyå¤´éƒ¨åŠ å…¥è¿™å¥ï¼Œå¼•å…¥PKINet

```java
from models.PKINet import PKIBlock
```

#### ç¬¬â‘¢æ­¥ï¼šåˆ›å»ºè‡ªå®šä¹‰çš„yamlæ–‡ä»¶ 

```java
# YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license
# Parameters
nc: 80  # number of classes
depth_multiple: 0.33  # model depth multiple
width_multiple: 0.50  # layer channel multiple
anchors:
  - [10,13, 16,30, 33,23]  # P3/8
  - [30,61, 62,45, 59,119]  # P4/16
  - [116,90, 156,198, 373,326]  # P5/32

# YOLOv5 v6.0 backbone + three Attention modules
backbone:
  # [from, number, module, args]
  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2
   [-1, 1, Conv, [128, 3, 2]],    # 1-P2/4
   [-1, 3, C3, [128]],
   [-1, 1, Conv, [256, 3, 2]],    # 3-P3/8
   [-1, 6, C3, [256]],
   [-1, 1, PKIBlock, [256]],    # ---> argsæ”¹æˆä¸Šä¸€å±‚çš„é€šé“æ•°
   [-1, 1, Conv, [512, 3, 2]],    # 6-P4/16
   [-1, 9, C3, [512]],
   [-1, 1, PKIBlock, [512]],    # ---> argsæ”¹æˆä¸Šä¸€å±‚çš„é€šé“æ•°
   [-1, 1, Conv, [1024, 3, 2]],   # 9-P5/32
   [-1, 3, C3, [1024]],
   [-1, 1, SPPF, [1024, 5]],      # 11
   [-1, 1, PKIBlock, [1024]],    # ---> argsæ”¹æˆä¸Šä¸€å±‚çš„é€šé“æ•°
  ]

# YOLOv5 v6.0 head
head:
  [[-1, 1, Conv, [512, 1, 1]],
   [-1, 1, nn.Upsample, [None, 2, 'nearest']],
   [[-1, 8], 1, Concat, [1]],  # cat backbone P4
   [-1, 3, C3, [512, False]],  # 16

   [-1, 1, Conv, [256, 1, 1]],
   [-1, 1, nn.Upsample, [None, 2, 'nearest']],
   [[-1, 5], 1, Concat, [1]],  # cat backbone P3
   [-1, 3, C3, [256, False]],  # 20 (P3/8-small)

   [-1, 1, Conv, [256, 3, 2]],
   [[-1, 17], 1, Concat, [1]],  # cat head P4
   [-1, 3, C3, [512, False]],  # 23 (P4/16-medium)

   [-1, 1, Conv, [512, 3, 2]],
   [[-1, 13], 1, Concat, [1]],  # cat head P5
   [-1, 3, C3, [1024, False]],  # 26 (P5/32-large)

   [[20, 23, 26], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)
  ]
```

#### ç¬¬â‘£æ­¥ï¼šéªŒè¯æ˜¯å¦åŠ å…¥æˆåŠŸ 

è¿è¡Œyolo.py

![](https://i-blog.csdnimg.cn/blog_migrate/0577b6a934ca7778981d2bb78d6f2564.png)

è¿™æ ·å°±OKå•¦ï¼

## ğŸŒŸæœ¬äººYOLOv5ç³»åˆ—å¯¼èˆª 

![962f7cb1b48f44e29d9beb1d499d0530.gif](https://i-blog.csdnimg.cn/blog_migrate/ac3c5d6bfbcbf982e8e9e3632d7f20d1.gif) ğŸ€[YOLOv5æºç ][YOLOv5 1]è¯¦è§£ç³»åˆ—ï¼š

[YOLOv5æºç é€è¡Œè¶…è¯¦ç»†æ³¨é‡Šä¸è§£è¯»ï¼ˆ1ï¼‰â€”â€”é¡¹ç›®ç›®å½•ç»“æ„è§£æ][YOLOv5_1]

[YOLOv5æºç é€è¡Œè¶…è¯¦ç»†æ³¨é‡Šä¸è§£è¯»ï¼ˆ2ï¼‰â€”â€”æ¨ç†éƒ¨åˆ†detect.py][YOLOv5_2_detect.py]

[YOLOv5æºç é€è¡Œè¶…è¯¦ç»†æ³¨é‡Šä¸è§£è¯»ï¼ˆ3ï¼‰â€”â€”è®­ç»ƒéƒ¨åˆ†train.py][YOLOv5_3_train.py]

[YOLOv5æºç é€è¡Œè¶…è¯¦ç»†æ³¨é‡Šä¸è§£è¯»ï¼ˆ4ï¼‰â€”â€”éªŒè¯éƒ¨åˆ†valï¼ˆtestï¼‰.py][YOLOv5_4_val_test_.py]

[YOLOv5æºç é€è¡Œè¶…è¯¦ç»†æ³¨é‡Šä¸è§£è¯»ï¼ˆ5ï¼‰â€”â€”é…ç½®æ–‡ä»¶yolov5s.yaml][YOLOv5_5_yolov5s.yaml]

[YOLOv5æºç é€è¡Œè¶…è¯¦ç»†æ³¨é‡Šä¸è§£è¯»ï¼ˆ6ï¼‰â€”â€”ç½‘ç»œç»“æ„ï¼ˆ1ï¼‰yolo.py][YOLOv5_6_1_yolo.py]

[YOLOv5æºç é€è¡Œè¶…è¯¦ç»†æ³¨é‡Šä¸è§£è¯»ï¼ˆ7ï¼‰â€”â€”ç½‘ç»œç»“æ„ï¼ˆ2ï¼‰common.py][YOLOv5_7_2_common.py]

![962f7cb1b48f44e29d9beb1d499d0530.gif](https://i-blog.csdnimg.cn/blog_migrate/ac3c5d6bfbcbf982e8e9e3632d7f20d1.gif) ğŸ€[YOLOv5å…¥é—¨å®è·µ][YOLOv5 1]ç³»åˆ—ï¼š

[YOLOv5å…¥é—¨å®è·µï¼ˆ1ï¼‰â€”â€”æ‰‹æŠŠæ‰‹å¸¦ä½ ç¯å¢ƒé…ç½®æ­å»º][YOLOv5_1 1]

[YOLOv5å…¥é—¨å®è·µï¼ˆ2ï¼‰â€”â€”æ‰‹æŠŠæ‰‹æ•™ä½ åˆ©ç”¨labelimgæ ‡æ³¨æ•°æ®é›†][YOLOv5_2_labelimg]

[YOLOv5å…¥é—¨å®è·µï¼ˆ3ï¼‰â€”â€”æ‰‹æŠŠæ‰‹æ•™ä½ åˆ’åˆ†è‡ªå·±çš„æ•°æ®é›†][YOLOv5_3]

[YOLOv5å…¥é—¨å®è·µï¼ˆ4ï¼‰â€”â€”æ‰‹æŠŠæ‰‹æ•™ä½ è®­ç»ƒè‡ªå·±çš„æ•°æ®é›†][YOLOv5_4]

[YOLOv5å…¥é—¨å®è·µï¼ˆ5ï¼‰â€”â€”ä»é›¶å¼€å§‹ï¼Œæ‰‹æŠŠæ‰‹æ•™ä½ è®­ç»ƒè‡ªå·±çš„ç›®æ ‡æ£€æµ‹æ¨¡å‹ï¼ˆåŒ…å«pyqt5ç•Œé¢ï¼‰][YOLOv5_5_pyqt5]

![](https://i-blog.csdnimg.cn/blog_migrate/f029c0617e18347a611e64e684734e78.gif)


[YOLOv5_0]: https://blog.csdn.net/weixin_43334693/article/details/130564848?spm=1001.2014.3001.5501
[YOLOv5_1_SE]: https://blog.csdn.net/weixin_43334693/article/details/130551913?spm=1001.2014.3001.5501
[YOLOv5_2_CBAM]: https://blog.csdn.net/weixin_43334693/article/details/130587102?spm=1001.2014.3001.5501
[YOLOv5_3_CA]: https://blog.csdn.net/weixin_43334693/article/details/130619604?spm=1001.2014.3001.5501
[YOLOv5_4_ECA]: https://blog.csdn.net/weixin_43334693/article/details/130641318?spm=1001.2014.3001.5501
[YOLOv5_5_ MobileNetV3]: https://blog.csdn.net/weixin_43334693/article/details/130832933?spm=1001.2014.3001.5501
[YOLOv5_6_ ShuffleNetV2]: https://blog.csdn.net/weixin_43334693/article/details/131008642?spm=1001.2014.3001.5501
[YOLOv5_7_SimAM]: https://blog.csdn.net/weixin_43334693/article/details/131031541?spm=1001.2014.3001.5501
[YOLOv5_8_SOCA]: https://blog.csdn.net/weixin_43334693/article/details/131053284?spm=1001.2014.3001.5501
[YOLOv5_9_EfficientNetv2]: https://blog.csdn.net/weixin_43334693/article/details/131207097?csdn_share_tail=%7B%22type%22%3A%22blog%22%2C%22rType%22%3A%22article%22%2C%22rId%22%3A%22131207097%22%2C%22source%22%3A%22weixin_43334693%22%7D
[YOLOv5_10_GhostNet]: https://blog.csdn.net/weixin_43334693/article/details/131235113?spm=1001.2014.3001.5501
[YOLOv5_11_EIoU_AlphaIoU_SIoU_WIoU]: https://blog.csdn.net/weixin_43334693/article/details/131350224?spm=1001.2014.3001.5501
[YOLOv5_12_Neck_BiFPN]: https://blog.csdn.net/weixin_43334693/article/details/131461294?spm=1001.2014.3001.5501
[YOLOv5_13_SiLU_ReLU_ELU_Hardswish_Mish_Softplus_AconC]: https://blog.csdn.net/weixin_43334693/article/details/131513850?spm=1001.2014.3001.5502
[YOLOv5_14_NMS_ DIoU-NMS_CIoU-NMS_EIoU-NMS_GIoU-NMS _SIoU-NMS_Soft-NMS]: https://blog.csdn.net/weixin_43334693/article/details/131552028?spm=1001.2014.3001.5501
[YOLOv5_15]: https://blog.csdn.net/weixin_43334693/article/details/131613721?spm=1001.2014.3001.5502
[YOLOv5_16_EMA_ICASSP2023]: https://blog.csdn.net/weixin_43334693/article/details/131973273?spm=1001.2014.3001.5501
[YOLOv5_17_IoU_MPDIoU_ELSEVIER 2023_WIoU_EIoU]: https://blog.csdn.net/weixin_43334693/article/details/131999141?spm=1001.2014.3001.5501
[YOLOv5_18_Neck_AFPN_PAFPN]: https://blog.csdn.net/weixin_43334693/article/details/132070079?spm=1001.2014.3001.5501
[YOLOv5_19_Swin TransformerV1_ViT]: https://blog.csdn.net/weixin_43334693/article/details/132161488?spm=1001.2014.3001.5501
[YOLOv5_20_BiFormer_CVPR2023]: https://blog.csdn.net/weixin_43334693/article/details/132203200?csdn_share_tail=%7B%22type%22%3A%22blog%22%2C%22rType%22%3A%22article%22%2C%22rId%22%3A%22132203200%22%2C%22source%22%3A%22weixin_43334693%22%7D
[YOLOv5_21_RepViT_ ICCV 2023_ViT]: https://blog.csdn.net/weixin_43334693/article/details/132211831?spm=1001.2014.3001.5501
[YOLOv5_22_MobileViTv1_ ViT]: https://blog.csdn.net/weixin_43334693/article/details/132367429?csdn_share_tail=%7B%22type%22%3A%22blog%22%2C%22rType%22%3A%22article%22%2C%22rId%22%3A%22132367429%22%2C%22source%22%3A%22weixin_43334693%22%7D
[YOLOv5_23_MobileViTv2_ Transformer]: https://blog.csdn.net/weixin_43334693/article/details/132428203?spm=1001.2014.3001.5502
[YOLOv5_24_MobileViTv3]: https://blog.csdn.net/weixin_43334693/article/details/133199471?spm=1001.2014.3001.5502
[YOLOv5_25_LSKNet]: https://blog.csdn.net/weixin_43334693/article/details/135510571?spm=1001.2014.3001.5501
[YOLOv5_26_RFAConv]: https://blog.csdn.net/weixin_43334693/article/details/135562865?spm=1001.2014.3001.5501
[YOLOv5_27_SCConv_CVPR 2023]: https://blog.csdn.net/weixin_43334693/article/details/135610505?spm=1001.2014.3001.5502
[YOLOv5_28_DSConv_ICCV 2023]: https://blog.csdn.net/weixin_43334693/article/details/135758781?spm=1001.2014.3001.5502
[YOLOv5_29_DilateFormer_MSDA]: https://blog.csdn.net/weixin_43334693/article/details/135845841?spm=1001.2014.3001.5501
[YOLOv5_30_iRMB_ICCV 2023]: https://blog.csdn.net/weixin_43334693/article/details/139153395?spm=1001.2014.3001.5502
[YOLOv5_31_Dual-ViT_TPAMI 2023_Transformer]: https://blog.csdn.net/weixin_43334693/article/details/139834875?spm=1001.2014.3001.5501
[_PKINet_]: #%F0%9F%9A%80%C2%A0%E4%B8%80%E3%80%81iRMB%E4%BB%8B%E7%BB%8D%C2%A0
[1.1 PKINet]: #1.1%C2%A0PKINet%E7%AE%80%E4%BB%8B
[1.2 PKINet]: #1.2%C2%A0PKINet%E6%96%B9%E6%B3%95
[1.2.1 PKI]: #1.2.1%C2%A0PKI%E6%A8%A1%E5%9D%97
[1.2.1 _CAA]: #1.2.1%20%E4%B8%8A%E4%B8%8B%E6%96%87%E9%94%9A%E7%82%B9%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%88CAA%EF%BC%89
[Link 1]: #%F0%9F%9A%80%E4%BA%8C%E3%80%81%E5%85%B7%E4%BD%93%E6%B7%BB%E5%8A%A0%E6%96%B9%E6%B3%95
[2.1 _]: #2.1%20%E6%B7%BB%E5%8A%A0%E9%A1%BA%E5%BA%8F%C2%A0
[2.2 _]: #2.2%20%E5%85%B7%E4%BD%93%E6%B7%BB%E5%8A%A0%E6%AD%A5%E9%AA%A4%C2%A0%C2%A0
[models_PKINet.py]: #%E7%AC%AC%E2%91%A0%E6%AD%A5%EF%BC%9A%E5%9C%A8common.py%E4%B8%AD%E6%B7%BB%E5%8A%A0LSK%E6%A8%A1%E5%9D%97%C2%A0
[_yolo.py_]: #%C2%A0%E7%AC%AC%E2%91%A1%E6%AD%A5%EF%BC%9A%E4%BF%AE%E6%94%B9yolo.py%E6%96%87%E4%BB%B6%C2%A0
[yaml_]: #%E7%AC%AC%E2%91%A2%E6%AD%A5%EF%BC%9A%E5%88%9B%E5%BB%BA%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84yaml%E6%96%87%E4%BB%B6%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0
[Link 2]: #%C2%A0%E7%AC%AC%E2%91%A3%E6%AD%A5%EF%BC%9A%E9%AA%8C%E8%AF%81%E6%98%AF%E5%90%A6%E5%8A%A0%E5%85%A5%E6%88%90%E5%8A%9F
[YOLOv5]: #%F0%9F%8C%9F%E6%9C%AC%E4%BA%BAYOLOv5%E7%B3%BB%E5%88%97%E5%AF%BC%E8%88%AA
[https_export.arxiv.org_pdf_2403.06258]: https://export.arxiv.org/pdf/2403.06258
[https_github.com_NUST-Machine-Intelligence-Laboratory_PKINet]: https://github.com/NUST-Machine-Intelligence-Laboratory/PKINet
[YOLOv5 1]: https://so.csdn.net/so/search?q=YOLOv5%E6%BA%90%E7%A0%81&spm=1001.2101.3001.7020
[YOLOv5_1]: https://blog.csdn.net/weixin_43334693/article/details/129356033?spm=1001.2014.3001.5501
[YOLOv5_2_detect.py]: https://blog.csdn.net/weixin_43334693/article/details/129349094?spm=1001.2014.3001.5501
[YOLOv5_3_train.py]: https://blog.csdn.net/weixin_43334693/article/details/129460666?spm=1001.2014.3001.5501
[YOLOv5_4_val_test_.py]: https://blog.csdn.net/weixin_43334693/article/details/129649553?spm=1001.2014.3001.5501
[YOLOv5_5_yolov5s.yaml]: https://blog.csdn.net/weixin_43334693/article/details/129697521?spm=1001.2014.3001.5501
[YOLOv5_6_1_yolo.py]: https://blog.csdn.net/weixin_43334693/article/details/129803802?spm=1001.2014.3001.5501
[YOLOv5_7_2_common.py]: https://blog.csdn.net/weixin_43334693/article/details/129854764?spm=1001.2014.3001.5501
[YOLOv5_1 1]: https://blog.csdn.net/weixin_43334693/article/details/129981848?spm=1001.2014.3001.5501
[YOLOv5_2_labelimg]: https://blog.csdn.net/weixin_43334693/article/details/129995604?spm=1001.2014.3001.5501
[YOLOv5_3]: https://blog.csdn.net/weixin_43334693/article/details/130025866?spm=1001.2014.3001.5501
[YOLOv5_4]: https://blog.csdn.net/weixin_43334693/article/details/130043351?spm=1001.2014.3001.5501
[YOLOv5_5_pyqt5]: https://blog.csdn.net/weixin_43334693/article/details/130044342?spm=1001.2014.3001.5501