![](https://i-blog.csdnimg.cn/blog_migrate/9abb5b2338de720ad3812461bcdc7560.gif)

![](https://i-blog.csdnimg.cn/blog_migrate/813905c6e1d7dfc09433e23374a228d0.png)

![962f7cb1b48f44e29d9beb1d499d0530.gif](https://i-blog.csdnimg.cn/blog_migrate/ac3c5d6bfbcbf982e8e9e3632d7f20d1.gif)ã€YOLOv5æ”¹è¿›ç³»åˆ—ã€‘å‰æœŸå›é¡¾ï¼š

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ0ï¼‰â€”â€”é‡è¦æ€§èƒ½æŒ‡æ ‡ä¸è®­ç»ƒç»“æœè¯„ä»·åŠåˆ†æ][YOLOv5_0]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ1ï¼‰â€”â€”æ·»åŠ SEæ³¨æ„åŠ›æœºåˆ¶][YOLOv5_1_SE]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ2ï¼‰â€”â€”æ·»åŠ CBAMæ³¨æ„åŠ›æœºåˆ¶][YOLOv5_2_CBAM]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ3ï¼‰â€”â€”æ·»åŠ CAæ³¨æ„åŠ›æœºåˆ¶][YOLOv5_3_CA]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ4ï¼‰â€”â€”æ·»åŠ ECAæ³¨æ„åŠ›æœºåˆ¶][YOLOv5_4_ECA]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ5ï¼‰â€”â€”æ›¿æ¢ä¸»å¹²ç½‘ç»œä¹‹ MobileNetV3][YOLOv5_5_ MobileNetV3]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ6ï¼‰â€”â€”æ›¿æ¢ä¸»å¹²ç½‘ç»œä¹‹ ShuffleNetV2][YOLOv5_6_ ShuffleNetV2]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ7ï¼‰â€”â€”æ·»åŠ SimAMæ³¨æ„åŠ›æœºåˆ¶][YOLOv5_7_SimAM]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ8ï¼‰â€”â€”æ·»åŠ SOCAæ³¨æ„åŠ›æœºåˆ¶][YOLOv5_8_SOCA]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ9ï¼‰â€”â€”æ›¿æ¢ä¸»å¹²ç½‘ç»œä¹‹EfficientNetv2][YOLOv5_9_EfficientNetv2]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ10ï¼‰â€”â€”æ›¿æ¢ä¸»å¹²ç½‘ç»œä¹‹GhostNet][YOLOv5_10_GhostNet]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ11ï¼‰â€”â€”æ·»åŠ æŸå¤±å‡½æ•°ä¹‹EIoUã€AlphaIoUã€SIoUã€WIoU][YOLOv5_11_EIoU_AlphaIoU_SIoU_WIoU]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ12ï¼‰â€”â€”æ›´æ¢Neckä¹‹BiFPN][YOLOv5_12_Neck_BiFPN]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ13ï¼‰â€”â€”æ›´æ¢æ¿€æ´»å‡½æ•°ä¹‹SiLUï¼ŒReLUï¼ŒELUï¼ŒHardswishï¼ŒMishï¼ŒSoftplusï¼ŒAconCç³»åˆ—ç­‰][YOLOv5_13_SiLU_ReLU_ELU_Hardswish_Mish_Softplus_AconC][YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ14ï¼‰â€”â€”æ›´æ¢NMSï¼ˆéæå¤§æŠ‘åˆ¶ï¼‰ä¹‹ DIoU-NMSã€CIoU-NMSã€EIoU-NMSã€GIoU-NMS ã€SIoU-NMSã€Soft-NMS][YOLOv5_14_NMS_ DIoU-NMS_CIoU-NMS_EIoU-NMS_GIoU-NMS _SIoU-NMS_Soft-NMS][YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ15ï¼‰â€”â€”å¢åŠ å°ç›®æ ‡æ£€æµ‹å±‚][YOLOv5_15]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ16ï¼‰â€”â€”æ·»åŠ EMAæ³¨æ„åŠ›æœºåˆ¶ï¼ˆICASSP2023|å®æµ‹æ¶¨ç‚¹ï¼‰][YOLOv5_16_EMA_ICASSP2023]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ17ï¼‰â€”â€”æ›´æ¢IoUä¹‹MPDIoUï¼ˆELSEVIER 2023|è¶…è¶ŠWIoUã€EIoUç­‰|å®æµ‹æ¶¨ç‚¹ï¼‰][YOLOv5_17_IoU_MPDIoU_ELSEVIER 2023_WIoU_EIoU]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ18ï¼‰â€”â€”æ›´æ¢Neckä¹‹AFPNï¼ˆå…¨æ–°æ¸è¿›ç‰¹å¾é‡‘å­—å¡”|è¶…è¶ŠPAFPN|å®æµ‹æ¶¨ç‚¹ï¼‰][YOLOv5_18_Neck_AFPN_PAFPN]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ19ï¼‰â€”â€”æ›¿æ¢ä¸»å¹²ç½‘ç»œä¹‹Swin TransformerV1ï¼ˆå‚æ•°é‡æ›´å°çš„ViTæ¨¡å‹ï¼‰][YOLOv5_19_Swin TransformerV1_ViT]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ20ï¼‰â€”â€”æ·»åŠ BiFormeræ³¨æ„åŠ›æœºåˆ¶ï¼ˆCVPR2023|å°ç›®æ ‡æ¶¨ç‚¹ç¥å™¨ï¼‰][YOLOv5_20_BiFormer_CVPR2023]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ21ï¼‰â€”â€”æ›¿æ¢ä¸»å¹²ç½‘ç»œä¹‹RepViTï¼ˆæ¸…å ICCV 2023|æœ€æ–°å¼€æºç§»åŠ¨ç«¯ViTï¼‰][YOLOv5_21_RepViT_ ICCV 2023_ViT]

[YOLOv5æ”¹è¿›ç³»åˆ—ï¼ˆ22ï¼‰â€”â€”æ›¿æ¢ä¸»å¹²ç½‘ç»œä¹‹MobileViTv1ï¼ˆä¸€ç§è½»é‡çº§çš„ã€é€šç”¨çš„ç§»åŠ¨è®¾å¤‡ ViTï¼‰][YOLOv5_22_MobileViTv1_ ViT]

![](https://i-blog.csdnimg.cn/blog_migrate/b432bc3c0686ff75fa31365d201e1c46.gif)

ç›®å½•

[ğŸš€ä¸€ã€MobileViT v2ä»‹ç» ][MobileViT v2_]

[1.1 ç®€ä»‹ ][1.1 _]

[1.2 ç½‘ç»œç»“æ„][1.2]

[MHA][]

[Separable self-attention][]

[1.3 å®éªŒ][1.3]

[ğŸš€äºŒã€å…·ä½“æ·»åŠ æ–¹æ³• ][Link 1]

[ç¬¬â‘ æ­¥ï¼šåœ¨common.pyä¸­æ·»åŠ MobileViTv2æ¨¡å—][common.py_MobileViTv2]

[ç¬¬â‘¡æ­¥ï¼šä¿®æ”¹yolo.pyæ–‡ä»¶][yolo.py]

[ç¬¬â‘¢æ­¥ï¼šåˆ›å»ºè‡ªå®šä¹‰çš„yamlæ–‡ä»¶ ][yaml_]

[ç¬¬â‘£æ­¥ éªŒè¯æ˜¯å¦åŠ å…¥æˆåŠŸ][Link 2]

[ğŸŒŸæœ¬äººYOLOv5ç³»åˆ—å¯¼èˆª][YOLOv5]

![](https://i-blog.csdnimg.cn/blog_migrate/4b4a6c296e738e68b3d860911546a180.gif)

## ğŸš€ä¸€ã€MobileViT v2ä»‹ç» 

>  *  è®ºæ–‡é¢˜ç›®ï¼šã€ŠSeparable Self-attention for Mobile Vision Transformers ã€‹
>  *  è®ºæ–‡åœ°å€ï¼š[https://arxiv.org/abs/2110.02178][https_arxiv.org_abs_2110.02178]
>  *  æºç åœ°å€ï¼š[GitHub - apple/ml-cvnets: CVNets: A library for training computer vision networks][GitHub - apple_ml-cvnets_ CVNets_ A library for training computer vision networks]

![](https://i-blog.csdnimg.cn/blog_migrate/00a2a25764bd20b66c601c3065de14e4.png)

### 1.1 ç®€ä»‹ 

ä¸Šä¸€ç¯‡ä»‹ç»çš„MobileViTå¯ä»¥åœ¨å¤šä¸ªç§»åŠ¨è§†è§‰ä»»åŠ¡ä¸­å®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬åˆ†ç±»å’Œæ£€æµ‹ã€‚è™½ç„¶è¿™äº›æ¨¡å‹çš„å‚æ•°è¾ƒå°‘ï¼Œä½†ä¸åŸºäºå·ç§¯ç¥ç»ç½‘ç»œçš„æ¨¡å‹ç›¸æ¯”ï¼Œå®ƒä»¬å…·æœ‰è¾ƒé«˜çš„å»¶è¿Ÿã€‚

å…¶ä¸»è¦æ•ˆç‡ç“¶é¢ˆï¼š

1.  transformerä¸­çš„å¤šå¤´è‡ªæˆ‘æ³¨æ„(MHA)ï¼Œç›¸å¯¹äºtokençš„æ•°é‡kï¼Œå®ƒéœ€è¦O(k^2)çš„æ—¶é—´å¤æ‚åº¦ã€‚
2.  MHAéœ€è¦æ˜‚è´µçš„æ“ä½œ(ä¾‹å¦‚ï¼Œæ‰¹é‡çŸ©é˜µä¹˜æ³•)æ¥è®¡ç®—è‡ªæˆ‘æ³¨æ„ï¼Œå½±å“èµ„æºå—é™è®¾å¤‡çš„å»¶è¿Ÿã€‚ 

æœ¬æ–‡ä»‹ç»äº†ä¸€ç§é’ˆå¯¹MobileViTæ¨¡å‹ä¸­å¤šå¤´è‡ªæ³¨æ„åŠ›(MHA)æ•ˆç‡ç“¶é¢ˆçš„è§£å†³æ–¹æ¡ˆâ€”â€”å¯åˆ†ç¦»è‡ªæ³¨æ„åŠ›MobileViT v2ã€‚è¯¥æ–¹æ³•å…·æœ‰çº¿æ€§å¤æ‚åº¦çš„å¯åˆ†ç¦»è‡ªæ³¨æ„æ–¹æ³•ï¼Œå¹¶ä¸”ä½¿ç”¨åŸºäºå…ƒç´ çš„æ“ä½œæ¥è®¡ç®—è‡ªæ³¨æ„åŠ›ï¼Œä»è€Œä½¿å…¶åœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šçš„æ‰§è¡Œæ•ˆç‡æ›´é«˜ã€‚

MobileViTv2åœ¨å¤šä¸ªç§»åŠ¨è§†è§‰ä»»åŠ¡ä¸Šéƒ½æ˜¯æœ€å…ˆè¿›çš„ï¼ŒåŒ…æ‹¬ImageNetå¯¹è±¡åˆ†ç±»å’ŒMS-COCOå¯¹è±¡æ£€æµ‹ã€‚é€šè¿‡å¤§çº¦300ä¸‡ä¸ªå‚æ•°ï¼ŒMobileViTv2åœ¨ImageNetæ•°æ®é›†ä¸Šè·å¾—äº†75.6%çš„top-1ç²¾åº¦ï¼Œæ¯”MobileViTé«˜å‡ºçº¦1%ï¼ŒåŒæ—¶åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šè¿è¡Œé€Ÿåº¦å¿«3.2å€ã€‚

![](https://i-blog.csdnimg.cn/blog_migrate/9ddf5b16b6724217d1c7a8079bcf96f9.png)

### 1.2 ç½‘ç»œç»“æ„ 

é™ä½å¤šå¤´è‡ªæ³¨æ„æ—¶é—´å¤æ‚åº¦æœ‰ä¸¤ä¸ªæ–¹å‘ï¼š

 *  tokensï¼šåœ¨è‡ªæ³¨æ„å±‚å¼•å…¥sparsityï¼Œåœ¨è¾“å…¥åºåˆ—ä¸­æ¯ä¸ªtokenå¼•å…¥tokensä¸€ä¸ªå­é›†ï¼›ä½¿ç”¨é¢„å®šä¹‰æ¨¡å¼é™åˆ¶tokenè¾“å…¥ï¼ˆä¸æ¥å—æ‰€æœ‰çš„tokensè€Œæ˜¯æ¥å—å­é›†ï¼Œç¼ºç‚¹è®­ç»ƒæ ·æœ¬å°‘æ€§èƒ½ä¸‹é™å¾ˆå¿«ï¼‰æˆ–è€…ä½¿ç”¨å±€éƒ¨æ•æ„Ÿçš„hashåˆ†ç»„tokensï¼ˆå¤§å‹åºåˆ—ä¸Šæ‰èƒ½çœ‹åˆ°æå‡ï¼‰
 *  patchesï¼šé€šè¿‡ä½ç§©çŸ©é˜µä¼°è®¡å¾—åˆ°è¿‘ä¼¼è‡ªæ³¨æ„çŸ©é˜µï¼Œç”±çº¿æ€§è¿æ¥å°†è‡ªæ³¨æ„æ“ä½œåˆ†è§£æˆå¤šä¸ªæ›´å°çš„è‡ªæ³¨æ„æ“ä½œï¼ˆLinformerä½¿ç”¨batch-wiseçŸ©é˜µä¹˜æ³•ï¼‰

æœ¬æ–‡ä¸»è¦æ˜¯ä¸ºäº†è§£å†³v1ç‰ˆæœ¬çš„é«˜å»¶è¿Ÿé—®é¢˜ï¼š

1.  ç”¨åˆ†ç¦»è‡ªæ³¨æ„ä»£æ›¿å¤šå¤´è‡ªæ³¨æ„æé«˜æ•ˆç‡
2.  ä½¿ç”¨element-wiseæ“ä½œæ›¿ä»£batch-wiseçŸ©é˜µä¹˜æ³•

![](https://i-blog.csdnimg.cn/blog_migrate/f3ea315428848a61e598f0aa5c47e25c.png)

 *  (a)æ˜¯ä¸€ç§æ ‡å‡†çš„å¤šå¤´è‡ªæ³¨æ„(MHA)å˜å‹å™¨ã€‚
 *  (b)åœ¨(a)ä¸­é€šè¿‡å¼•å…¥tokenæŠ•å½±å±‚æ‰©å±•MHAï¼Œå°†kä¸ªtokenæŠ•å½±åˆ°é¢„å®šä¹‰æ•°é‡çš„token pï¼Œä»è€Œå°†å¤æ‚åº¦ä»O(k2)é™ä½åˆ°O(k)ã€‚ç„¶è€Œï¼Œå®ƒä»ç„¶ä½¿ç”¨æ˜‚è´µçš„æ“ä½œ(ä¾‹å¦‚ï¼Œæ‰¹é‡çŸ©é˜µä¹˜æ³•)æ¥è®¡ç®—è‡ªæˆ‘æ³¨æ„ï¼Œå½±å“èµ„æºå—é™è®¾å¤‡ä¸Šçš„å»¶è¿Ÿã€‚
 *  (c)æ˜¯æå‡ºçš„å¯åˆ†ç¦»çš„è‡ªæˆ‘æ³¨æ„å±‚ï¼Œå…¶å¤æ‚æ€§æ˜¯çº¿æ€§çš„å³O(k)ï¼Œå¹¶ä½¿ç”¨å…ƒç´ æ“ä½œæ¥æ›´å¿«çš„æ¨æ–­ã€‚  
    

#### MHA 

dh=d/h,æœ€åè¾“å‡ºkä¸ªdç»´tokensï¼Œè¿™ä¸ªè¾“å‡ºä¼šåœ¨åšä¸€æ¬¡çŸ©é˜µä¹˜æ³•å˜æˆk\*dç»´å‘é‡ï¼Œä½œä¸ºæœ€åçš„è¾“å‡ºã€‚

![](https://i-blog.csdnimg.cn/blog_migrate/e69f15280594aa41ce44e18fb6f69426.png)

#### Separable self-attention 

è®ºæ–‡ä¸­æåˆ°çš„è§£å†³æ–¹æ¡ˆçš„å…³é”®æ˜¯å°†è‡ªæˆ‘å…³æ³¨è®¡ç®—åˆ†æˆä¸¤ä¸ªçº¿æ€§è®¡ç®—ã€‚

å…·ä½“æ¥è¯´ï¼Œè¯¥æ–¹æ³•ä½¿ç”¨ä¸€ä¸ªæ½œåœ¨æ ‡è®°æ¥è®¡ç®—ä¸Šä¸‹æ–‡åˆ†æ•°ï¼Œç„¶åä½¿ç”¨è¿™äº›åˆ†æ•°æ¥é‡æ–°åŠ æƒè¾“å…¥æ ‡è®°ï¼Œç”Ÿæˆä¸€ä¸ªä¸Šä¸‹æ–‡å‘é‡ã€‚

ç”±äºè‡ªæˆ‘å…³æ³¨è®¡ç®—ä¸æ½œåœ¨æ ‡è®°çš„è®¡ç®—ç›¸å…³ï¼Œå› æ­¤è¿™ç§æ–¹æ³•å¯ä»¥å°†è‡ªæˆ‘å…³æ³¨è®¡ç®—çš„å¤æ‚æ€§ä» O(k^2) é™ä½åˆ° O(k)ï¼Œå…¶ä¸­ k æ˜¯æ ‡è®°çš„æ•°é‡ã€‚

![](https://i-blog.csdnimg.cn/blog_migrate/6a89285051a6028d7018e54c36694c9b.png)

 *  åˆ†æ”¯Lï¼šç”¨çŸ©é˜µ(b)Lå°†xä¸­æ¯ä¸ªdç»´å‘é‡æ˜ å°„åˆ°æ ‡é‡ï¼Œè®¡ç®—(b)Lä¸xçš„è·ç¦»å¾—åˆ°ä¸€ä¸ªkç»´å‘é‡ï¼Œè¿™ä¸ªkç»´å‘é‡softmaxåå°±æ˜¯ä¸Šä¸‹æ–‡å¾—åˆ†csï¼›
 *  åˆ†æ”¯Kï¼šç›´æ¥çŸ©é˜µç›¸ä¹˜å¾—åˆ°è¾“å‡ºXk,ä¸csç›¸ä¹˜å¹¶ç›¸åŠ kå±‚ï¼Œå¾—åˆ°cvï¼Œcvç±»ä¼¼äºMHAçš„açŸ©é˜µï¼Œä¹Ÿç¼–ç äº†æ‰€æœ‰xçš„è¾“å…¥ï¼›
 *  åˆ†æ”¯Vï¼šçº¿æ€§æ˜ å°„å¹¶ç”±ReLUæ¿€æ´»å¾—åˆ°Xvï¼Œç„¶åä¸cv element-wiseç›¸ä¹˜ï¼Œæœ€åé€šè¿‡çº¿æ€§å±‚å¾—åˆ°æœ€åçš„è¾“å‡ºã€‚

### 1.3 å®éªŒ 

æ•°æ®é›†

è®ºæ–‡ä¸­ä½¿ç”¨äº†å¤šä¸ªæ•°æ®é›†è¿›è¡Œå®éªŒï¼ŒåŒ…æ‹¬ImageNet-1kã€ImageNet-21k-På’ŒMS-COCOã€‚

 *  åœ¨ImageNet-1kæ•°æ®é›†ä¸Šï¼Œä½œè€…ä½¿ç”¨äº†ä¸€ä¸ªè®­ç»ƒé›†å’Œä¸€ä¸ªéªŒè¯é›†ï¼Œè®­ç»ƒé›†åŒ…å«128ä¸‡å¼ å›¾åƒï¼ŒéªŒè¯é›†åŒ…å«5ä¸‡å¼ å›¾åƒã€‚
 *  åœ¨ImageNet-21k-Pæ•°æ®é›†ä¸Šï¼Œä½œè€…ä½¿ç”¨äº†ä¸€ä¸ªè®­ç»ƒé›†å’Œä¸€ä¸ªéªŒè¯é›†ï¼Œè®­ç»ƒé›†åŒ…å«1100ä¸‡å¼ å›¾åƒï¼ŒéªŒè¯é›†åŒ…å«52ä¸‡å¼ å›¾åƒã€‚
 *  åœ¨MS-COCOæ•°æ®é›†ä¸Šï¼Œä½œè€…ä½¿ç”¨äº†ä¸€ä¸ªè®­ç»ƒé›†å’Œä¸€ä¸ªéªŒè¯é›†ï¼Œè®­ç»ƒé›†åŒ…å«8ä¸‡å¼ å›¾åƒï¼ŒéªŒè¯é›†åŒ…å«4åƒå¼ å›¾åƒã€‚

è®­ç»ƒæ–¹æ³•

 *  åœ¨ImageNet-1kæ•°æ®é›†ä¸Šï¼Œä½œè€…ä½¿ç”¨äº†AdamWç®—æ³•è¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨äº†ä¸€ä¸ªæœ‰æ•ˆçš„æ‰¹æ¬¡å¤§å°ä¸º1024çš„å›¾åƒï¼ˆ128ä¸ªå›¾åƒæ¯ä¸ªGPUÃ—8ä¸ªGPUï¼‰ï¼Œè®­ç»ƒ300ä¸ªæ—¶æœŸã€‚
 *  åœ¨ImageNet-21k-Pæ•°æ®é›†ä¸Šï¼Œä½œè€…ä½¿ç”¨äº†ä¸ImageNet-1kæ•°æ®é›†ç›¸åŒçš„è®­ç»ƒæ–¹æ³•ï¼Œä½†æ˜¯ä½¿ç”¨äº†ä¸€ä¸ªæœ‰æ•ˆçš„æ‰¹æ¬¡å¤§å°ä¸º4096çš„å›¾åƒï¼ˆ64ä¸ªå›¾åƒæ¯ä¸ªGPUÃ—64ä¸ªGPUï¼‰ï¼Œè®­ç»ƒ80ä¸ªæ—¶æœŸã€‚
 *  åœ¨MS-COCOæ•°æ®é›†ä¸Šï¼Œä½œè€…ä½¿ç”¨äº†ä¸ImageNet-1kæ•°æ®é›†ç›¸åŒçš„è®­ç»ƒæ–¹æ³•ï¼Œä½†æ˜¯ä½¿ç”¨äº†ä¸€ä¸ªæœ‰æ•ˆçš„æ‰¹æ¬¡å¤§å°ä¸º128çš„å›¾åƒã€‚

ï¼ˆ1ï¼‰ä¸è‡ªæˆ‘æ³¨æ„æ–¹æ³•çš„æ¯”è¾ƒ

![](https://i-blog.csdnimg.cn/blog_migrate/710f97d09376559fd0f89ec72bcbecbe.png)

 ï¼ˆ2ï¼‰ImageNet-1kéªŒè¯é›†ä¸Šçš„åˆ†ç±»æ€§èƒ½

![](https://i-blog.csdnimg.cn/blog_migrate/14621052da6636b028a9b4f4d844c898.png)

ï¼ˆ3ï¼‰ADE20k å’Œ PASCAL VOC 2012 æ•°æ®é›†ä¸Šçš„è¯­ä¹‰åˆ†å‰²ç»“æœ

![](https://i-blog.csdnimg.cn/blog_migrate/a7c3438549c956bca027c0a5728c6316.png)

ï¼ˆ4ï¼‰åœ¨MS-COCOæ•°æ®é›†ä¸Šä½¿ç”¨SSDLiteè¿›è¡Œå¯¹è±¡æ£€æµ‹

![](https://i-blog.csdnimg.cn/blog_migrate/35c0dde68ee176991cdf0673bfdf85fc.png)

ï¼ˆ5ï¼‰MobileViTv2 æ¨¡å‹ä¸åŒè¾“å‡ºæ­¥å¹… (OS) çš„ä¸Šä¸‹æ–‡åˆ†æ•°å›¾

![](https://i-blog.csdnimg.cn/blog_migrate/72ee528bff1a6b7b048441350263f94b.png)

## ğŸš€äºŒã€å…·ä½“æ·»åŠ æ–¹æ³• 

#### ç¬¬â‘ æ­¥ï¼šåœ¨common.pyä¸­æ·»åŠ MobileViTv2æ¨¡å— 

é¦–å…ˆï¼Œå®šä¹‰å·ç§¯å±‚ã€‚

åˆ†ä¸ºæ™®é€šå·ç§¯å±‚å’Œæ·±åº¦å¯åˆ†ç¦»å·ç§¯å±‚

```java
def autopad(k, p=None):  # kernel, padding
    if p is None:
        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad
    return p

# æ™®é€šå·ç§¯å±‚
class Conv(nn.Module):
    # Standard convolution
    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups
        super().__init__()
        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False)
        self.bn = nn.BatchNorm2d(c2)
        self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity())

    def forward(self, x):
        return self.act(self.bn(self.conv(x)))

    def forward_fuse(self, x):
        return self.act(self.conv(x))

# æ·±åº¦å¯åˆ†ç¦»å·ç§¯
class DWConv(Conv):
    # Depth-wise convolution class
    def __init__(self, c1, c2, k=1, s=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups
        super().__init__(c1, c2, k, s, g=math.gcd(c1, c2), act=act)

# å¸¦bnçš„1Ã—1å·ç§¯åˆ†æ”¯
def conv_1x1_bn(inp, oup):
    return nn.Sequential(
        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),
        nn.BatchNorm2d(oup),
        nn.SiLU()
    )
```

æ¥ç€ï¼Œæ„é€ ViTæ¨¡å—ã€‚

Transformer Encoderæ¨¡å—ä¸­ç¼–ç 

```java
#---ViTéƒ¨åˆ†---#

# è§„èŒƒåŒ–å±‚çš„ç±»å°è£…
class PreNorm(nn.Module):
    def __init__(self, dim, fn):
 '''
  dim: è¾“å…¥å’Œè¾“å‡ºç»´åº¦
  fn: å‰é¦ˆç½‘ç»œå±‚ï¼Œé€‰æ‹©Multi-Head Attnå’ŒMLPäºŒè€…ä¹‹ä¸€
 '''
        super().__init__()
        # LayerNorm: ( a - mean(last 2 dim) ) / sqrt( var(last 2 dim) )
        # æ•°æ®å½’ä¸€åŒ–çš„è¾“å…¥ç»´åº¦è®¾å®šï¼Œä»¥åŠä¿å­˜å‰é¦ˆå±‚

        self.norm = nn.LayerNorm(dim)
        self.fn = fn

    def forward(self, x, **kwargs):
        return self.fn(self.norm(x), **kwargs)

# FFN
class FeedForward(nn.Module):
    def __init__(self, dim, hidden_dim, dropout=0.):
'''
 dimï¼š è¾“å…¥å’Œè¾“å‡ºç»´åº¦
 hidden_dimï¼š  ä¸­é—´å±‚çš„ç»´åº¦
 dropoutï¼š  dropoutæ“ä½œçš„æ¦‚ç‡å‚æ•°p
'''
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim, hidden_dim),
            nn.SiLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, dim),
            nn.Dropout(dropout)
        )

    def forward(self, x):
        return self.net(x)

# Attention
class Attention(nn.Module):
    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):
        super().__init__()
        inner_dim = heads * dim_head
        project_out = not (heads == 1 and dim_head == dim)
 
        self.heads = heads
        # è¡¨ç¤º1/(sqrt(dim_head))ç”¨äºæ¶ˆé™¤è¯¯å·®ï¼Œä¿è¯æ–¹å·®ä¸º1ï¼Œé¿å…å‘é‡å†…ç§¯è¿‡å¤§å¯¼è‡´çš„softmaxå°†è®¸å¤šè¾“å‡ºç½®0çš„æƒ…å†µ
        # å¯ä»¥çœ‹åŸæ–‡ã€Šattention is all you needã€‹ä¸­å…³äºScale Dot-Product Attentionå¦‚ä½•æŠ‘åˆ¶å†…ç§¯è¿‡å¤§
        self.scale = dim_head ** -0.5
        # dim =  > 0 æ—¶ï¼Œè¡¨ç¤ºmaskç¬¬dç»´åº¦ï¼Œå¯¹ç›¸åŒçš„ç¬¬dç»´åº¦ï¼Œè¿›è¡Œsoftmax
        # dim =  < 0 æ—¶ï¼Œè¡¨ç¤ºmaskå€’æ•°ç¬¬dç»´åº¦ï¼Œå¯¹ç›¸åŒçš„å€’æ•°ç¬¬dç»´åº¦ï¼Œè¿›è¡Œsoftmax
        self.attend = nn.Softmax(dim = -1)
        # ç”ŸæˆqkvçŸ©é˜µï¼Œä¸‰ä¸ªçŸ©é˜µè¢«æ”¾åœ¨ä¸€èµ·ï¼Œåç»­ä¼šè¢«åˆ†å¼€
        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)
        # å¦‚æœæ˜¯å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶åˆ™éœ€è¦è¿›è¡Œå…¨è¿æ¥å’Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œå¦åˆ™è¾“å‡ºä¸åšæ›´æ”¹
        self.to_out = nn.Sequential(
            nn.Linear(inner_dim, dim),
            nn.Dropout(dropout)
        ) if project_out else nn.Identity()
 
    def forward(self, x):
        # åˆ†å‰²æˆqã€kã€vä¸‰ä¸ªçŸ©é˜µ
        # qkvä¸º inner_dim * 3ï¼Œå…¶ä¸­inner_dim = heads * dim_head
        qkv = self.to_qkv(x).chunk(3, dim = -1)
        # qkvçš„ç»´åº¦æ˜¯(3, inner_dim = heads * dim_head)
        # 'b n (h d) -> b h n d' é‡æ–°æŒ‰æ€è·¯åˆ†ç¦»å‡º8ä¸ªå¤´ï¼Œä¸€å…±8ç»„q,k,vçŸ©é˜µ
        # rearrangeåç»´åº¦å˜æˆ (3, heads, dim, dim_head)
        # ç»è¿‡mapåï¼Œqã€kã€vç»´åº¦å˜æˆ(1, heads, dim, dim_head)
        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)
        # query * key å¾—åˆ°å¯¹valueçš„æ³¨æ„åŠ›é¢„æµ‹ï¼Œå¹¶é€šè¿‡å‘é‡å†…ç§¯ç¼©æ”¾é˜²æ­¢softmaxæ— æ•ˆåŒ–éƒ¨åˆ†å‚æ•°
        # heads * dim * dim
        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale
        # å¯¹æœ€åä¸€ä¸ªç»´åº¦è¿›è¡Œsoftmaxåå¾—åˆ°é¢„æµ‹çš„æ¦‚ç‡å€¼
        attn = self.attend(dots)
        # ä¹˜ç§¯å¾—åˆ°é¢„æµ‹ç»“æœ
        # out -> heads * dim * dim_head
        out = torch.matmul(attn, v)
        # é‡ç»„å¼ é‡ï¼Œå°†headsç»´åº¦é‡æ–°è¿˜åŸ
        out = rearrange(out, 'b h n d -> b n (h d)')
        return self.to_out(out)
# Transformeræ¨¡å—ç¼–ç 
class Transformer(nn.Module):
    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):
        super().__init__()
        self.layers = nn.ModuleList([])
        for _ in range(depth):
            self.layers.append(nn.ModuleList([
                PreNorm(dim, Attention(dim, heads, dim_head, dropout)),
                PreNorm(dim, FeedForward(dim, mlp_dim, dropout))
            ]))

    def forward(self, x):
        for attn, ff in self.layers:
            x = attn(x) + x
            x = ff(x) + x
        return x
```

 ç„¶åï¼ŒMV2æ¨¡å—

![](https://i-blog.csdnimg.cn/blog_migrate/0bb18c624fc9aa944e73c57a2fc5b41c.png)

åˆ†æ–‡stride=1å’Œstride=2ä¸¤ç§ã€‚

```java
# MV2æ¨¡å—
class MV2Block(nn.Module):
    def __init__(self, inp, oup, stride=1, expansion=4):
        super().__init__()
        self.stride = stride
        assert stride in [1, 2]

        hidden_dim = int(inp * expansion)
        self.use_res_connect = self.stride == 1 and inp == oup

        if expansion == 1: # æ‰©å¼ ç‡
            self.conv = nn.Sequential(
                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),# 3Ã—3çš„å·ç§¯å±‚
                nn.BatchNorm2d(hidden_dim), # BNå±‚
                nn.SiLU(), # SiLUå‡½æ•°
                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False), # 1Ã—1çš„å·ç§¯å±‚
                nn.BatchNorm2d(oup), # BNå±‚
            )
        else:
            self.conv = nn.Sequential(
                # pw
                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False), # 1Ã—1çš„å·ç§¯å±‚
                nn.BatchNorm2d(hidden_dim), # BNå±‚ 
                nn.SiLU(), # SiLUå‡½æ•°
                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False), # 1Ã—1çš„å·ç§¯å±‚
                nn.BatchNorm2d(hidden_dim),# BNå±‚ 
                nn.SiLU(), # SiLUå‡½æ•°
                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False), # 1Ã—1çš„å·ç§¯å±‚
                nn.BatchNorm2d(oup), # BNå±‚
            )

    def forward(self, x):
        if self.use_res_connect:
            return x + self.conv(x)
        else:
            return self.conv(x)
```

æœ€åï¼Œæ ¸å¿ƒæ¨¡å— MobileViT\_Block

![](https://i-blog.csdnimg.cn/blog_migrate/a8c0d0c46fabfd711e7ddc956b99037a.png)

åˆ†ä¸ºå±€éƒ¨è¡¨å¾æ¨¡å—å’Œå…¨å±€è¡¨å¾æ¨¡å—ã€‚

```java
# MobileViTv2_Blockæ¨¡å—ï¼ˆæ ¸å¿ƒéƒ¨åˆ†ï¼‰
class MobileViTv2_Block(nn.Module):
    def __init__(self, sim_channel, dim=64, depth=2, kernel_size=3, patch_size=(2, 2), mlp_dim=int(64 * 2), dropout=0.):
        super().__init__()
        self.ph, self.pw = patch_size # è·å–hå’Œw
        self.dwc = DWConv(sim_channel, sim_channel, kernel_size) # 3Ã—3å¯åˆ†ç¦»å·ç§¯
        self.conv2 = conv_1x1_bn(sim_channel, dim) # 1Ã—1çš„å·ç§¯å±‚
        self.transformer = Transformer(dim, depth, 4, 8, mlp_dim, dropout) # Transformerè¿›è¡Œç¼–ç æ“ä½œ
        self.conv3 = conv_1x1_bn(dim, sim_channel) # 1Ã—1çš„å·ç§¯å±‚
        self.mv2 = MV2Block(sim_channel, sim_channel) # MV2æ¨¡å—

    def forward(self, x):
        # Local representations #mg
        x = self.dwc(x)
        x = self.conv2(x)
        # Global representations #mg
        _, _, h, w = x.shape
        x = rearrange(x, 'b d (h ph) (w pw) -> b (ph pw) (h w) d', ph=self.ph, pw=self.pw)
        x = self.transformer(x)
        x = rearrange(x, 'b (ph pw) (h w) d -> b d (h ph) (w pw)', h=h // self.ph, w=w // self.pw, ph=self.ph,
                      pw=self.pw)
        x = self.conv3(x)
        x = self.mv2(x)
        return x
```

ä»¥ä¸‹æ˜¯å®Œæ•´ä»£ç ï¼š

å°†ä»¥ä¸‹ä»£ç å¤åˆ¶ç²˜è´´åˆ°common.pyæ–‡ä»¶çš„æœ«å°¾

```java
# MobileViTv2
from einops import rearrange
import math


def autopad(k, p=None):  # kernel, padding
    if p is None:
        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad
    return p

# æ™®é€šå·ç§¯å±‚
class Conv(nn.Module):
    # Standard convolution
    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups
        super().__init__()
        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False)
        self.bn = nn.BatchNorm2d(c2)
        self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity())

    def forward(self, x):
        return self.act(self.bn(self.conv(x)))

    def forward_fuse(self, x):
        return self.act(self.conv(x))

# æ·±åº¦å¯åˆ†ç¦»å·ç§¯
class DWConv(Conv):
    # Depth-wise convolution class
    def __init__(self, c1, c2, k=1, s=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups
        super().__init__(c1, c2, k, s, g=math.gcd(c1, c2), act=act)

# å¸¦bnçš„1Ã—1å·ç§¯åˆ†æ”¯
def conv_1x1_bn(inp, oup):
    return nn.Sequential(
        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),
        nn.BatchNorm2d(oup),
        nn.SiLU()
    )
#---ViTéƒ¨åˆ†---#

# è§„èŒƒåŒ–å±‚çš„ç±»å°è£…
class PreNorm(nn.Module):
    def __init__(self, dim, fn):
 '''
  dim: è¾“å…¥å’Œè¾“å‡ºç»´åº¦
  fn: å‰é¦ˆç½‘ç»œå±‚ï¼Œé€‰æ‹©Multi-Head Attnå’ŒMLPäºŒè€…ä¹‹ä¸€
 '''
        super().__init__()
        # LayerNorm: ( a - mean(last 2 dim) ) / sqrt( var(last 2 dim) )
        # æ•°æ®å½’ä¸€åŒ–çš„è¾“å…¥ç»´åº¦è®¾å®šï¼Œä»¥åŠä¿å­˜å‰é¦ˆå±‚

        self.norm = nn.LayerNorm(dim)
        self.fn = fn

    def forward(self, x, **kwargs):
        return self.fn(self.norm(x), **kwargs)

# FFN
class FeedForward(nn.Module):
    def __init__(self, dim, hidden_dim, dropout=0.):
'''
 dimï¼š è¾“å…¥å’Œè¾“å‡ºç»´åº¦
 hidden_dimï¼š  ä¸­é—´å±‚çš„ç»´åº¦
 dropoutï¼š  dropoutæ“ä½œçš„æ¦‚ç‡å‚æ•°p
'''
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim, hidden_dim),
            nn.SiLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, dim),
            nn.Dropout(dropout)
        )

    def forward(self, x):
        return self.net(x)

# Attention
class Attention(nn.Module):
    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):
        super().__init__()
        inner_dim = heads * dim_head
        project_out = not (heads == 1 and dim_head == dim)
 
        self.heads = heads
        # è¡¨ç¤º1/(sqrt(dim_head))ç”¨äºæ¶ˆé™¤è¯¯å·®ï¼Œä¿è¯æ–¹å·®ä¸º1ï¼Œé¿å…å‘é‡å†…ç§¯è¿‡å¤§å¯¼è‡´çš„softmaxå°†è®¸å¤šè¾“å‡ºç½®0çš„æƒ…å†µ
        # å¯ä»¥çœ‹åŸæ–‡ã€Šattention is all you needã€‹ä¸­å…³äºScale Dot-Product Attentionå¦‚ä½•æŠ‘åˆ¶å†…ç§¯è¿‡å¤§
        self.scale = dim_head ** -0.5
        # dim =  > 0 æ—¶ï¼Œè¡¨ç¤ºmaskç¬¬dç»´åº¦ï¼Œå¯¹ç›¸åŒçš„ç¬¬dç»´åº¦ï¼Œè¿›è¡Œsoftmax
        # dim =  < 0 æ—¶ï¼Œè¡¨ç¤ºmaskå€’æ•°ç¬¬dç»´åº¦ï¼Œå¯¹ç›¸åŒçš„å€’æ•°ç¬¬dç»´åº¦ï¼Œè¿›è¡Œsoftmax
        self.attend = nn.Softmax(dim = -1)
        # ç”ŸæˆqkvçŸ©é˜µï¼Œä¸‰ä¸ªçŸ©é˜µè¢«æ”¾åœ¨ä¸€èµ·ï¼Œåç»­ä¼šè¢«åˆ†å¼€
        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)
        # å¦‚æœæ˜¯å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶åˆ™éœ€è¦è¿›è¡Œå…¨è¿æ¥å’Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œå¦åˆ™è¾“å‡ºä¸åšæ›´æ”¹
        self.to_out = nn.Sequential(
            nn.Linear(inner_dim, dim),
            nn.Dropout(dropout)
        ) if project_out else nn.Identity()
 
    def forward(self, x):
        # åˆ†å‰²æˆqã€kã€vä¸‰ä¸ªçŸ©é˜µ
        # qkvä¸º inner_dim * 3ï¼Œå…¶ä¸­inner_dim = heads * dim_head
        qkv = self.to_qkv(x).chunk(3, dim = -1)
        # qkvçš„ç»´åº¦æ˜¯(3, inner_dim = heads * dim_head)
        # 'b n (h d) -> b h n d' é‡æ–°æŒ‰æ€è·¯åˆ†ç¦»å‡º8ä¸ªå¤´ï¼Œä¸€å…±8ç»„q,k,vçŸ©é˜µ
        # rearrangeåç»´åº¦å˜æˆ (3, heads, dim, dim_head)
        # ç»è¿‡mapåï¼Œqã€kã€vç»´åº¦å˜æˆ(1, heads, dim, dim_head)
        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)
        # query * key å¾—åˆ°å¯¹valueçš„æ³¨æ„åŠ›é¢„æµ‹ï¼Œå¹¶é€šè¿‡å‘é‡å†…ç§¯ç¼©æ”¾é˜²æ­¢softmaxæ— æ•ˆåŒ–éƒ¨åˆ†å‚æ•°
        # heads * dim * dim
        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale
        # å¯¹æœ€åä¸€ä¸ªç»´åº¦è¿›è¡Œsoftmaxåå¾—åˆ°é¢„æµ‹çš„æ¦‚ç‡å€¼
        attn = self.attend(dots)
        # ä¹˜ç§¯å¾—åˆ°é¢„æµ‹ç»“æœ
        # out -> heads * dim * dim_head
        out = torch.matmul(attn, v)
        # é‡ç»„å¼ é‡ï¼Œå°†headsç»´åº¦é‡æ–°è¿˜åŸ
        out = rearrange(out, 'b h n d -> b n (h d)')
        return self.to_out(out)

# Transformeræ¨¡å—ç¼–ç 
class Transformer(nn.Module):
    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):
        super().__init__()
        self.layers = nn.ModuleList([])
        for _ in range(depth):
            self.layers.append(nn.ModuleList([
                PreNorm(dim, Attention(dim, heads, dim_head, dropout)),
                PreNorm(dim, FeedForward(dim, mlp_dim, dropout))
            ]))

    def forward(self, x):
        for attn, ff in self.layers:
            x = attn(x) + x
            x = ff(x) + x
        return x

# ---MobileViTv2éƒ¨åˆ†--- #
# MV2æ¨¡å—
class MV2Block(nn.Module):
    def __init__(self, inp, oup, stride=1, expansion=4):
        super().__init__()
        self.stride = stride
        assert stride in [1, 2]

        hidden_dim = int(inp * expansion)
        self.use_res_connect = self.stride == 1 and inp == oup

        if expansion == 1: # æ‰©å¼ ç‡
            self.conv = nn.Sequential(
                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),# 3Ã—3çš„å·ç§¯å±‚
                nn.BatchNorm2d(hidden_dim), # BNå±‚
                nn.SiLU(), # SiLUå‡½æ•°
                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False), # 1Ã—1çš„å·ç§¯å±‚
                nn.BatchNorm2d(oup), # BNå±‚
            )
        else:
            self.conv = nn.Sequential(
                # pw
                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False), # 1Ã—1çš„å·ç§¯å±‚
                nn.BatchNorm2d(hidden_dim), # BNå±‚ 
                nn.SiLU(), # SiLUå‡½æ•°
                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False), # 1Ã—1çš„å·ç§¯å±‚
                nn.BatchNorm2d(hidden_dim),# BNå±‚ 
                nn.SiLU(), # SiLUå‡½æ•°
                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False), # 1Ã—1çš„å·ç§¯å±‚
                nn.BatchNorm2d(oup), # BNå±‚
            )

    def forward(self, x):
        if self.use_res_connect:
            return x + self.conv(x)
        else:
            return self.conv(x)

# MobileViTv2_Blockæ¨¡å—ï¼ˆæ ¸å¿ƒéƒ¨åˆ†ï¼‰
class MobileViTv2_Block(nn.Module):
    def __init__(self, sim_channel, dim=64, depth=2, kernel_size=3, patch_size=(2, 2), mlp_dim=int(64 * 2), dropout=0.):
        super().__init__()
        self.ph, self.pw = patch_size # è·å–hå’Œw
        self.dwc = DWConv(sim_channel, sim_channel, kernel_size) # 3Ã—3å¯åˆ†ç¦»å·ç§¯
        self.conv2 = conv_1x1_bn(sim_channel, dim) # 1Ã—1çš„å·ç§¯å±‚
        self.transformer = Transformer(dim, depth, 4, 8, mlp_dim, dropout) # Transformerè¿›è¡Œç¼–ç æ“ä½œ
        self.conv3 = conv_1x1_bn(dim, sim_channel) # 1Ã—1çš„å·ç§¯å±‚
        self.mv2 = MV2Block(sim_channel, sim_channel) # MV2æ¨¡å—

    def forward(self, x):
        # Local representations #mg
        x = self.dwc(x)
        x = self.conv2(x)
        # Global representations #mg
        _, _, h, w = x.shape
        x = rearrange(x, 'b d (h ph) (w pw) -> b (ph pw) (h w) d', ph=self.ph, pw=self.pw)
        x = self.transformer(x)
        x = rearrange(x, 'b (ph pw) (h w) d -> b d (h ph) (w pw)', h=h // self.ph, w=w // self.pw, ph=self.ph,
                      pw=self.pw)
        x = self.conv3(x)
        x = self.mv2(x)
        return x
```

å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![](https://i-blog.csdnimg.cn/blog_migrate/e8a4667c6758e5693f16ad92ae54486d.png)

#### ç¬¬â‘¡æ­¥ï¼šä¿®æ”¹yolo.pyæ–‡ä»¶ 

å†æ¥ä¿®æ”¹yolo.pyï¼Œåœ¨parse\_modelå‡½æ•°ä¸­æ‰¾åˆ° elif m is Concat: è¯­å¥ï¼Œåœ¨å…¶åé¢åŠ ä¸Šä¸‹é¢ä»£ç ï¼š

```java
# mobilevit v2
        elif m in [MobileViTv2_Block]:
            c1, c2 = ch[f], args[0]
            if c2 != no:
                c2 = make_divisible(c2 * gw, 8)
            args = [c1, c2]
            if m in [MobileViTv2_Block]:
                args.insert(2, n)
                n = 1
```

å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![](https://i-blog.csdnimg.cn/blog_migrate/75288093ab35e5cefe3de496711138ab.png)

#### ç¬¬â‘¢æ­¥ï¼šåˆ›å»ºè‡ªå®šä¹‰çš„yamlæ–‡ä»¶ 

yamlæ–‡ä»¶é…ç½®å®Œæ•´ä»£ç å¦‚ä¸‹ï¼š

```java
# YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license

# Parameters
nc: 80  # number of classes
depth_multiple: 0.33  # model depth multiple
width_multiple: 0.50  # layer channel multiple
anchors:
  - [10,13, 16,30, 33,23]  # P3/8
  - [30,61, 62,45, 59,119]  # P4/16
  - [116,90, 156,198, 373,326]  # P5/32

# YOLOv5 v6.0 backbone
backbone:
  # [from, number, module, args]
  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2
   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4
   [-1, 3, C3, [128]],
   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8
   [-1, 6, C3, [256]],
   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16
   [-1, 9, C3, [512]],
   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32
   [-1, 3, C3, [1024]],
   [-1, 1, SPPF, [1024, 5]],  # 9

  ]

# YOLOv5 v6.0 head
head:
  [[-1, 1, Conv, [512, 1, 1]],
   [-1, 1, nn.Upsample, [None, 2, 'nearest']],
   [[-1, 6], 1, Concat, [1]],  # cat backbone P4
   [-1, 3, C3, [512, False]],  # 13

   [-1, 1, Conv, [256, 1, 1]],
   [-1, 1, nn.Upsample, [None, 2, 'nearest']],
   [[-1, 4], 1, Concat, [1]],  # cat backbone P3
   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)

   [-1, 1, Conv, [256, 3, 2]],
   [[-1, 14], 1, Concat, [1]],  # cat head P4
   [-1, 3, MobileViTv2_Block, [512, False]],  # 20 (P4/16-medium)

   [-1, 1, Conv, [512, 3, 2]],
   [[-1, 10], 1, Concat, [1]],  # cat head P5
   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)


   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)
  ]
```

#### ç¬¬â‘£æ­¥ éªŒè¯æ˜¯å¦åŠ å…¥æˆåŠŸ 

è¿è¡Œyolo.py

![](https://i-blog.csdnimg.cn/blog_migrate/6e8e2e4c6f9cf0320963218bb1ca4a28.png)

è¿™æ ·å°±OKå•¦~

> ä»£ç å‚è€ƒï¼š
> 
> [YOLOv5ã€YOLOv8æ”¹è¿›MobileViTv2ä¸»å¹²ç³»åˆ—ï¼šå…¨ç½‘é¦–å‘æœ€æ–°è‹¹æœç»­ä½œåŠ å¼ºç‰ˆ MobileViTv2ç»“æ„ï¼ˆäºŒï¼‰ï¼Œæå‡ºç§»åŠ¨è§†è§‰ Transformer çš„å¯åˆ†ç¦»è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œé«˜æ•ˆæ¶¨ç‚¹æé€Ÿ\_mobilevitv2åŸè®ºæ–‡\_èŠ’æœæ±æ²¡æœ‰èŠ’æœçš„åšå®¢-CSDNåšå®¢][YOLOv5_YOLOv8_MobileViTv2_ MobileViTv2_ Transformer _mobilevitv2_-CSDN]

## ğŸŒŸæœ¬äººYOLOv5ç³»åˆ—å¯¼èˆª 

![962f7cb1b48f44e29d9beb1d499d0530.gif](https://i-blog.csdnimg.cn/blog_migrate/ac3c5d6bfbcbf982e8e9e3632d7f20d1.gif) ğŸ€[YOLOv5æºç ][YOLOv5 1]è¯¦è§£ç³»åˆ—ï¼š

[YOLOv5æºç é€è¡Œè¶…è¯¦ç»†æ³¨é‡Šä¸è§£è¯»ï¼ˆ1ï¼‰â€”â€”é¡¹ç›®ç›®å½•ç»“æ„è§£æ][YOLOv5_1]

[YOLOv5æºç é€è¡Œè¶…è¯¦ç»†æ³¨é‡Šä¸è§£è¯»ï¼ˆ2ï¼‰â€”â€”æ¨ç†éƒ¨åˆ†detect.py][YOLOv5_2_detect.py]

[YOLOv5æºç é€è¡Œè¶…è¯¦ç»†æ³¨é‡Šä¸è§£è¯»ï¼ˆ3ï¼‰â€”â€”è®­ç»ƒéƒ¨åˆ†train.py][YOLOv5_3_train.py]

[YOLOv5æºç é€è¡Œè¶…è¯¦ç»†æ³¨é‡Šä¸è§£è¯»ï¼ˆ4ï¼‰â€”â€”éªŒè¯éƒ¨åˆ†valï¼ˆtestï¼‰.py][YOLOv5_4_val_test_.py]

[YOLOv5æºç é€è¡Œè¶…è¯¦ç»†æ³¨é‡Šä¸è§£è¯»ï¼ˆ5ï¼‰â€”â€”é…ç½®æ–‡ä»¶yolov5s.yaml][YOLOv5_5_yolov5s.yaml]

[YOLOv5æºç é€è¡Œè¶…è¯¦ç»†æ³¨é‡Šä¸è§£è¯»ï¼ˆ6ï¼‰â€”â€”ç½‘ç»œç»“æ„ï¼ˆ1ï¼‰yolo.py][YOLOv5_6_1_yolo.py]

[YOLOv5æºç é€è¡Œè¶…è¯¦ç»†æ³¨é‡Šä¸è§£è¯»ï¼ˆ7ï¼‰â€”â€”ç½‘ç»œç»“æ„ï¼ˆ2ï¼‰common.py][YOLOv5_7_2_common.py]

![962f7cb1b48f44e29d9beb1d499d0530.gif](https://i-blog.csdnimg.cn/blog_migrate/ac3c5d6bfbcbf982e8e9e3632d7f20d1.gif) ğŸ€[YOLOv5å…¥é—¨å®è·µ][YOLOv5 1]ç³»åˆ—ï¼š

[YOLOv5å…¥é—¨å®è·µï¼ˆ1ï¼‰â€”â€”æ‰‹æŠŠæ‰‹å¸¦ä½ ç¯å¢ƒé…ç½®æ­å»º][YOLOv5_1 1]

[YOLOv5å…¥é—¨å®è·µï¼ˆ2ï¼‰â€”â€”æ‰‹æŠŠæ‰‹æ•™ä½ åˆ©ç”¨labelimgæ ‡æ³¨æ•°æ®é›†][YOLOv5_2_labelimg]

[YOLOv5å…¥é—¨å®è·µï¼ˆ3ï¼‰â€”â€”æ‰‹æŠŠæ‰‹æ•™ä½ åˆ’åˆ†è‡ªå·±çš„æ•°æ®é›†][YOLOv5_3]

[YOLOv5å…¥é—¨å®è·µï¼ˆ4ï¼‰â€”â€”æ‰‹æŠŠæ‰‹æ•™ä½ è®­ç»ƒè‡ªå·±çš„æ•°æ®é›†][YOLOv5_4]

[YOLOv5å…¥é—¨å®è·µï¼ˆ5ï¼‰â€”â€”ä»é›¶å¼€å§‹ï¼Œæ‰‹æŠŠæ‰‹æ•™ä½ è®­ç»ƒè‡ªå·±çš„ç›®æ ‡æ£€æµ‹æ¨¡å‹ï¼ˆåŒ…å«pyqt5ç•Œé¢ï¼‰][YOLOv5_5_pyqt5]

![](https://i-blog.csdnimg.cn/blog_migrate/25caddd9f389306104be0d196cefa0b6.gif)


[YOLOv5_0]: https://blog.csdn.net/weixin_43334693/article/details/130564848?spm=1001.2014.3001.5501
[YOLOv5_1_SE]: https://blog.csdn.net/weixin_43334693/article/details/130551913?spm=1001.2014.3001.5501
[YOLOv5_2_CBAM]: https://blog.csdn.net/weixin_43334693/article/details/130587102?spm=1001.2014.3001.5501
[YOLOv5_3_CA]: https://blog.csdn.net/weixin_43334693/article/details/130619604?spm=1001.2014.3001.5501
[YOLOv5_4_ECA]: https://blog.csdn.net/weixin_43334693/article/details/130641318?spm=1001.2014.3001.5501
[YOLOv5_5_ MobileNetV3]: https://blog.csdn.net/weixin_43334693/article/details/130832933?spm=1001.2014.3001.5501
[YOLOv5_6_ ShuffleNetV2]: https://blog.csdn.net/weixin_43334693/article/details/131008642?spm=1001.2014.3001.5501
[YOLOv5_7_SimAM]: https://blog.csdn.net/weixin_43334693/article/details/131031541?spm=1001.2014.3001.5501
[YOLOv5_8_SOCA]: https://blog.csdn.net/weixin_43334693/article/details/131053284?spm=1001.2014.3001.5501
[YOLOv5_9_EfficientNetv2]: https://blog.csdn.net/weixin_43334693/article/details/131207097?csdn_share_tail=%7B%22type%22%3A%22blog%22%2C%22rType%22%3A%22article%22%2C%22rId%22%3A%22131207097%22%2C%22source%22%3A%22weixin_43334693%22%7D
[YOLOv5_10_GhostNet]: https://blog.csdn.net/weixin_43334693/article/details/131235113?spm=1001.2014.3001.5501
[YOLOv5_11_EIoU_AlphaIoU_SIoU_WIoU]: https://blog.csdn.net/weixin_43334693/article/details/131350224?spm=1001.2014.3001.5501
[YOLOv5_12_Neck_BiFPN]: https://blog.csdn.net/weixin_43334693/article/details/131461294?spm=1001.2014.3001.5501
[YOLOv5_13_SiLU_ReLU_ELU_Hardswish_Mish_Softplus_AconC]: https://blog.csdn.net/weixin_43334693/article/details/131513850?spm=1001.2014.3001.5502
[YOLOv5_14_NMS_ DIoU-NMS_CIoU-NMS_EIoU-NMS_GIoU-NMS _SIoU-NMS_Soft-NMS]: https://blog.csdn.net/weixin_43334693/article/details/131552028?spm=1001.2014.3001.5501
[YOLOv5_15]: https://blog.csdn.net/weixin_43334693/article/details/131613721?spm=1001.2014.3001.5502
[YOLOv5_16_EMA_ICASSP2023]: https://blog.csdn.net/weixin_43334693/article/details/131973273?spm=1001.2014.3001.5501
[YOLOv5_17_IoU_MPDIoU_ELSEVIER 2023_WIoU_EIoU]: https://blog.csdn.net/weixin_43334693/article/details/131999141?spm=1001.2014.3001.5501
[YOLOv5_18_Neck_AFPN_PAFPN]: https://blog.csdn.net/weixin_43334693/article/details/132070079?spm=1001.2014.3001.5501
[YOLOv5_19_Swin TransformerV1_ViT]: https://blog.csdn.net/weixin_43334693/article/details/132161488?spm=1001.2014.3001.5501
[YOLOv5_20_BiFormer_CVPR2023]: https://blog.csdn.net/weixin_43334693/article/details/132203200?spm=1001.2014.3001.5502
[YOLOv5_21_RepViT_ ICCV 2023_ViT]: https://blog.csdn.net/weixin_43334693/article/details/132211831?spm=1001.2014.3001.5501
[YOLOv5_22_MobileViTv1_ ViT]: https://blog.csdn.net/weixin_43334693/article/details/132367429?csdn_share_tail=%7B%22type%22%3A%22blog%22%2C%22rType%22%3A%22article%22%2C%22rId%22%3A%22132367429%22%2C%22source%22%3A%22weixin_43334693%22%7D
[MobileViT v2_]: #%F0%9F%9A%80%E4%B8%80%E3%80%81MobileViT%20v2%E4%BB%8B%E7%BB%8D%C2%A0%C2%A0%C2%A0
[1.1 _]: #1.1%20%E7%AE%80%E4%BB%8B%C2%A0
[1.2]: #1.2%20%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84
[MHA]: #MHA
[Separable self-attention]: #Separable%20self-attention
[1.3]: #1.3%20%E5%AE%9E%E9%AA%8C
[Link 1]: #%F0%9F%9A%80%E4%BA%8C%E3%80%81%E5%85%B7%E4%BD%93%E6%B7%BB%E5%8A%A0%E6%96%B9%E6%B3%95%C2%A0
[common.py_MobileViTv2]: #%E7%AC%AC%E2%91%A0%E6%AD%A5%EF%BC%9A%E5%9C%A8common.py%E4%B8%AD%E6%B7%BB%E5%8A%A0SE%E6%A8%A1%E5%9D%97
[yolo.py]: #%C2%A0%E7%AC%AC%E2%91%A1%E6%AD%A5%EF%BC%9A%E5%9C%A8yolo.py%E6%96%87%E4%BB%B6%E9%87%8C%E7%9A%84parse_model%E5%87%BD%E6%95%B0%E5%8A%A0%E5%85%A5%E7%B1%BB%E5%90%8D
[yaml_]: #%C2%A0%E7%AC%AC%E2%91%A2%E6%AD%A5%EF%BC%9A%E5%88%9B%E5%BB%BA%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84yaml%E6%96%87%E4%BB%B6%C2%A0%C2%A0
[Link 2]: #%E7%AC%AC%E2%91%A3%E6%AD%A5%20%E9%AA%8C%E8%AF%81%E6%98%AF%E5%90%A6%E5%8A%A0%E5%85%A5%E6%88%90%E5%8A%9F
[YOLOv5]: #%F0%9F%8C%9F%E6%9C%AC%E4%BA%BAYOLOv5%E7%B3%BB%E5%88%97%E5%AF%BC%E8%88%AA
[https_arxiv.org_abs_2110.02178]: https://arxiv.org/abs/2110.02178
[GitHub - apple_ml-cvnets_ CVNets_ A library for training computer vision networks]: https://github.com/apple/ml-cvnets
[YOLOv5_YOLOv8_MobileViTv2_ MobileViTv2_ Transformer _mobilevitv2_-CSDN]: https://yoloair.blog.csdn.net/article/details/127107723
[YOLOv5 1]: https://so.csdn.net/so/search?q=YOLOv5%E6%BA%90%E7%A0%81&spm=1001.2101.3001.7020
[YOLOv5_1]: https://blog.csdn.net/weixin_43334693/article/details/129356033?spm=1001.2014.3001.5501
[YOLOv5_2_detect.py]: https://blog.csdn.net/weixin_43334693/article/details/129349094?spm=1001.2014.3001.5501
[YOLOv5_3_train.py]: https://blog.csdn.net/weixin_43334693/article/details/129460666?spm=1001.2014.3001.5501
[YOLOv5_4_val_test_.py]: https://blog.csdn.net/weixin_43334693/article/details/129649553?spm=1001.2014.3001.5501
[YOLOv5_5_yolov5s.yaml]: https://blog.csdn.net/weixin_43334693/article/details/129697521?spm=1001.2014.3001.5501
[YOLOv5_6_1_yolo.py]: https://blog.csdn.net/weixin_43334693/article/details/129803802?spm=1001.2014.3001.5501
[YOLOv5_7_2_common.py]: https://blog.csdn.net/weixin_43334693/article/details/129854764?spm=1001.2014.3001.5501
[YOLOv5_1 1]: https://blog.csdn.net/weixin_43334693/article/details/129981848?spm=1001.2014.3001.5501
[YOLOv5_2_labelimg]: https://blog.csdn.net/weixin_43334693/article/details/129995604?spm=1001.2014.3001.5501
[YOLOv5_3]: https://blog.csdn.net/weixin_43334693/article/details/130025866?spm=1001.2014.3001.5501
[YOLOv5_4]: https://blog.csdn.net/weixin_43334693/article/details/130043351?spm=1001.2014.3001.5501
[YOLOv5_5_pyqt5]: https://blog.csdn.net/weixin_43334693/article/details/130044342?spm=1001.2014.3001.5501